code	desc
protected final void bind indexed ( configuration property name name , bindable < ? > target , aggregate element binder element binder , resolvable type aggregate type , resolvable type element type , indexed collection supplier result ) { for ( configuration property source source : get context ( ) . get sources ( ) ) { bind indexed ( source , name , target , element binder , result , aggregate type , element type ) ; if ( result . was supplied ( ) && result . get ( ) != null ) { return ; } } }	Bind indexed elements to the supplied collection.
public void set servlet names ( collection < string > servlet names ) { assert . not null ( servlet names , str ) ; this . servlet names = new linked hash set < > ( servlet names ) ; }	Set servlet names that the filter will be registered against.
public void add servlet names ( string ... servlet names ) { assert . not null ( servlet names , str ) ; this . servlet names . add all ( arrays . as list ( servlet names ) ) ; }	Add servlet names for the filter.
public void set url patterns ( collection < string > url patterns ) { assert . not null ( url patterns , str ) ; this . url patterns = new linked hash set < > ( url patterns ) ; }	Set the URL patterns that the filter will be registered against.
public void add url patterns ( string ... url patterns ) { assert . not null ( url patterns , str ) ; collections . add all ( this . url patterns , url patterns ) ; }	Add URL patterns, as defined in the Servlet specification, that the filter will beregistered against.
public file get dir ( string sub dir ) { file dir = new file ( get dir ( ) , sub dir ) ; dir . mkdirs ( ) ; return dir ; }	Return a sub-directory of the application temp.
public file get dir ( ) { if ( this . dir == null ) { synchronized ( this ) { byte [ ] hash = generate hash ( this . source class ) ; this . dir = new file ( get temp directory ( ) , to hex string ( hash ) ) ; this . dir . mkdirs ( ) ; assert . state ( this . dir . exists ( ) , ( ) -> str + this . dir ) ; } } return this . dir ; }	Return the directory to be used for application specific temp files.
protected map < string , object > generate content ( ) { map < string , object > content = extract content ( to property source ( ) ) ; post process content ( content ) ; return content ; }	Extract the content to contribute to the info endpoint.
@ suppress warnings ( str ) protected map < string , object > get nested map ( map < string , object > map , string key ) { object value = map . get ( key ) ; if ( value == null ) { return collections . empty map ( ) ; } return ( map < string , object > ) value ; }	Return the nested map with the specified key or empty map if the specified mapcontains no mapping for the key.
public static < c , a > callback < c , a > callback ( class < c > callback type , c callback instance , a argument , object ... additional arguments ) { assert . not null ( callback type , str ) ; assert . not null ( callback instance , str ) ; return new callback < > ( callback type , callback instance , argument , additional arguments ) ; }	Start a call to a single callback instance, dealing with common generic typeconcerns and exceptions.
public static < c , a > callbacks < c , a > callbacks ( class < c > callback type , collection < ? extends c > callback instances , a argument , object ... additional arguments ) { assert . not null ( callback type , str ) ; assert . not null ( callback instances , str ) ; return new callbacks < > ( callback type , callback instances , argument , additional arguments ) ; }	Start a call to callback instances, dealing with common generic type concerns andexceptions.
public < r > stream < r > invoke and ( function < c , r > invoker ) { function < c , invocation result < r > > mapper = ( callback instance ) -> invoke ( callback instance , ( ) -> invoker . apply ( callback instance ) ) ; return this . callback instances . stream ( ) . map ( mapper ) . filter ( invocation result :: has result ) . map ( invocation result :: get ) ; }	Invoke the callback instances where the callback method returns a result.
public int start ( ) throws io { synchronized ( this . monitor ) { assert . state ( ! is started ( ) , str ) ; logger . debug ( str + this . port ) ; this . server socket = new server socket ( this . port ) ; int local port = this . server socket . get local port ( ) ; this . listen thread = this . thread factory . new thread ( this :: accept connections ) ; this . listen thread . set daemon ( bool ) ; this . listen thread . set name ( str ) ; this . listen thread . start ( ) ; return local port ; } }	Start the livereload server and accept incoming connections.
public void stop ( ) throws io { synchronized ( this . monitor ) { if ( this . listen thread != null ) { close all connections ( ) ; try { this . executor . shutdown ( ) ; this . executor . await termination ( num , time unit . minutes ) ; } catch ( interrupted exception ex ) { thread . current thread ( ) . interrupt ( ) ; } this . server socket . close ( ) ; try { this . listen thread . join ( ) ; } catch ( interrupted exception ex ) { thread . current thread ( ) . interrupt ( ) ; } this . listen thread = null ; this . server socket = null ; } } }	Gracefully stop the livereload server.
public void trigger reload ( ) { synchronized ( this . monitor ) { synchronized ( this . connections ) { for ( connection connection : this . connections ) { try { connection . trigger reload ( ) ; } catch ( exception ex ) { logger . debug ( str , ex ) ; } } } } }	Trigger livereload of all connected clients.
public spring application builder child ( class < ? > ... sources ) { spring application builder child = new spring application builder ( ) ; child . sources ( sources ) ;	Create a child application with the provided sources.
public spring application builder parent ( class < ? > ... sources ) { if ( this . parent == null ) { this . parent = new spring application builder ( sources ) . web ( web application type . none ) . properties ( this . default properties ) . environment ( this . environment ) ; } else { this . parent . sources ( sources ) ; } return this . parent ; }	Add a parent application with the provided sources.
public spring application builder parent ( configurable application context parent ) { this . parent = new spring application builder ( ) ; this . parent . context = parent ; this . parent . running . set ( bool ) ; return this ; }	Add an already running parent context to an existing application.
public spring application builder properties ( map < string , object > defaults ) { this . default properties . put all ( defaults ) ; this . application . set default properties ( this . default properties ) ; if ( this . parent != null ) { this . parent . properties ( this . default properties ) ; this . parent . environment ( this . environment ) ; } return this ; }	Default properties for the environment.
public static void add application converters ( converter registry registry ) { add delimited string converters ( registry ) ; registry . add converter ( new string to duration converter ( ) ) ; registry . add converter ( new duration to string converter ( ) ) ; registry . add converter ( new number to duration converter ( ) ) ; registry . add converter ( new duration to number converter ( ) ) ; registry . add converter ( new string to data size converter ( ) ) ; registry . add converter ( new number to data size converter ( ) ) ; registry . add converter factory ( new string to enum ignoring case converter factory ( ) ) ; }	Add converters useful for most Spring Boot applications.
public static void add delimited string converters ( converter registry registry ) { conversion service service = ( conversion service ) registry ; registry . add converter ( new array to delimited string converter ( service ) ) ; registry . add converter ( new collection to delimited string converter ( service ) ) ; registry . add converter ( new delimited string to array converter ( service ) ) ; registry . add converter ( new delimited string to collection converter ( service ) ) ; }	Add converters to support delimited strings.
public static void add application formatters ( formatter registry registry ) { registry . add formatter ( new char array formatter ( ) ) ; registry . add formatter ( new inet address formatter ( ) ) ; registry . add formatter ( new iso offset formatter ( ) ) ; }	Add formatters useful for most Spring Boot applications.
public string determine driver class name ( ) { if ( string utils . has text ( this . driver class name ) ) { assert . state ( driver class is loadable ( ) , ( ) -> str + this . driver class name ) ; return this . driver class name ; } string driver class name = null ; if ( string utils . has text ( this . url ) ) { driver class name = database driver . from jdbc url ( this . url ) . get driver class name ( ) ; } if ( ! string utils . has text ( driver class name ) ) { driver class name = this . embedded database connection . get driver class name ( ) ; } if ( ! string utils . has text ( driver class name ) ) { throw new data source bean creation exception ( str , this , this . embedded database connection ) ; } return driver class name ; }	Determine the driver to use based on this configuration and the environment.
public string determine url ( ) { if ( string utils . has text ( this . url ) ) { return this . url ; } string database name = determine database name ( ) ; string url = ( database name != null ) ? this . embedded database connection . get url ( database name ) : null ; if ( ! string utils . has text ( url ) ) { throw new data source bean creation exception ( str , this , this . embedded database connection ) ; } return url ; }	Determine the url to use based on this configuration and the environment.
public string determine database name ( ) { if ( this . generate unique name ) { if ( this . unique name == null ) { this . unique name = uuid . random uuid ( ) . to string ( ) ; } return this . unique name ; } if ( string utils . has length ( this . name ) ) { return this . name ; } if ( this . embedded database connection != embedded database connection . none ) { return str ; } return null ; }	Determine the name to used based on this configuration.
public string determine username ( ) { if ( string utils . has text ( this . username ) ) { return this . username ; } if ( embedded database connection . is embedded ( determine driver class name ( ) ) ) { return str ; } return null ; }	Determine the username to use based on this configuration and the environment.
public string determine password ( ) { if ( string utils . has text ( this . password ) ) { return this . password ; } if ( embedded database connection . is embedded ( determine driver class name ( ) ) ) { return str ; } return null ; }	Determine the password to use based on this configuration and the environment.
private closeable http response execute initializr metadata retrieval ( string url ) { http get request = new http get ( url ) ; request . set header ( new basic header ( http headers . accept , accept meta data ) ) ; return execute ( request , url , str ) ; }	Retrieves the meta-data of the service at the specified URL.
public void compile and run ( ) throws exception { synchronized ( this . monitor ) { try { stop ( ) ; class < ? > [ ] compiled sources = compile ( ) ; monitor for changes ( ) ;	Compile and run the application.
protected final boolean any matches ( condition context context , annotated type metadata metadata , condition ... conditions ) { for ( condition condition : conditions ) { if ( matches ( context , metadata , condition ) ) { return bool ; } } return bool ; }	Return true if any of the specified conditions match.
protected final boolean matches ( condition context context , annotated type metadata metadata , condition condition ) { if ( condition instanceof spring boot condition ) { return ( ( spring boot condition ) condition ) . get match outcome ( context , metadata ) . is match ( ) ; } return condition . matches ( context , metadata ) ; }	Return true if any of the specified condition matches.
private boolean is log configuration message ( throwable ex ) { if ( ex instanceof invocation target exception ) { return is log configuration message ( ex . get cause ( ) ) ; } string message = ex . get message ( ) ; if ( message != null ) { for ( string candidate : log configuration messages ) { if ( message . contains ( candidate ) ) { return bool ; } } } return bool ; }	Check if the exception is a log configuration message, i.e.
public task scheduler builder await termination period ( duration await termination period ) { return new task scheduler builder ( this . pool size , this . await termination , await termination period , this . thread name prefix , this . customizers ) ; }	Set the maximum time the executor is supposed to block on shutdown.
public task scheduler builder thread name prefix ( string thread name prefix ) { return new task scheduler builder ( this . pool size , this . await termination , this . await termination period , thread name prefix , this . customizers ) ; }	Set the prefix to use for the names of newly created threads.
public static logging system get ( class loader class loader ) { string logging system = system . get property ( system property ) ; if ( string utils . has length ( logging system ) ) { if ( none . equals ( logging system ) ) { return new no op logging system ( ) ; } return get ( class loader , logging system ) ; } return systems . entry set ( ) . stream ( ) . filter ( ( entry ) -> class utils . is present ( entry . get key ( ) , class loader ) ) . map ( ( entry ) -> get ( class loader , entry . get value ( ) ) ) . find first ( ) . or else throw ( ( ) -> new illegal state exception ( str ) ) ; }	Detect and return the logging system in use.
public void start ( ) { synchronized ( this . monitor ) { save initial snapshots ( ) ; if ( this . watch thread == null ) { map < file , folder snapshot > local folders = new hash map < > ( ) ; local folders . put all ( this . folders ) ; this . watch thread = new thread ( new watcher ( this . remaining scans , new array list < > ( this . listeners ) , this . trigger filter , this . poll interval , this . quiet period , local folders ) ) ; this . watch thread . set name ( str ) ; this . watch thread . set daemon ( this . daemon ) ; this . watch thread . start ( ) ; } } }	Start monitoring the source folder for changes.
void stop after ( int remaining scans ) { thread thread ; synchronized ( this . monitor ) { thread = this . watch thread ; if ( thread != null ) { this . remaining scans . set ( remaining scans ) ; if ( remaining scans <= num ) { thread . interrupt ( ) ; } } this . watch thread = null ; } if ( thread != null && thread . current thread ( ) != thread ) { try { thread . join ( ) ; } catch ( interrupted exception ex ) { thread . current thread ( ) . interrupt ( ) ; } } }	Stop monitoring the source folders.
protected map < string , object > get error attributes ( server request request , boolean include stack trace ) { return this . error attributes . get error attributes ( request , include stack trace ) ; }	Extract the error attributes from the current request, to be used to populate errorviews or JSON payloads.
protected boolean is trace enabled ( server request request ) { string parameter = request . query param ( str ) . or else ( str ) ; return ! str . equals ignore case ( parameter ) ; }	Check whether the trace attribute has been set on the given request.
public void stop ( ) throws mojo execution exception , io , instance not found exception { try { this . connection . invoke ( this . object name , str , null , null ) ; } catch ( reflection exception ex ) { throw new mojo execution exception ( str , ex . get cause ( ) ) ; } catch ( m ex ) { throw new mojo execution exception ( str , ex ) ; } }	Stop the application managed by this instance.
public string generate ( string url ) throws io { object content = this . initializr service . load service capabilities ( url ) ; if ( content instanceof initializr service metadata ) { return generate help ( url , ( initializr service metadata ) content ) ; } return content . to string ( ) ; }	Generate a report for the specified service.
protected mono < server response > render error view ( server request request ) { boolean include stack trace = is include stack trace ( request , media type . text html ) ; map < string , object > error = get error attributes ( request , include stack trace ) ; http status error status = get http status ( error ) ; server response . body builder response body = server response . status ( error status ) . content type ( media type . text html ) ; return flux . just ( str + error status . value ( ) , str + series views . get ( error status . series ( ) ) , str ) . flat map ( ( view name ) -> render error view ( view name , response body , error ) ) . switch if empty ( this . error properties . get whitelabel ( ) . is enabled ( ) ? render default error view ( response body , error ) : mono . error ( get error ( request ) ) ) . next ( ) ; }	Render the error information as an HTML view.
protected mono < server response > render error response ( server request request ) { boolean include stack trace = is include stack trace ( request , media type . all ) ; map < string , object > error = get error attributes ( request , include stack trace ) ; return server response . status ( get http status ( error ) ) . content type ( media type . application json ut ) . body ( body inserters . from object ( error ) ) ; }	Render the error information as a JSON payload.
protected http status get http status ( map < string , object > error attributes ) { int status code = ( int ) error attributes . get ( str ) ; return http status . value of ( status code ) ; }	Get the HTTP error status information from the error map.
private boolean should extract ( project generation request request , project generation response response ) { if ( request . is extract ( ) ) { return bool ; }	Detect if the project should be extracted.
@ safe varargs public final set < class < ? > > scan ( class < ? extends annotation > ... annotation types ) throws class not found exception { list < string > packages = get packages ( ) ; if ( packages . is empty ( ) ) { return collections . empty set ( ) ; } class path scanning candidate component provider scanner = new class path scanning candidate component provider ( bool ) ; scanner . set environment ( this . context . get environment ( ) ) ; scanner . set resource loader ( this . context ) ; for ( class < ? extends annotation > annotation type : annotation types ) { scanner . add include filter ( new annotation type filter ( annotation type ) ) ; } set < class < ? > > entity set = new hash set < > ( ) ; for ( string base package : packages ) { if ( string utils . has text ( base package ) ) { for ( bean definition candidate : scanner . find candidate components ( base package ) ) { entity set . add ( class utils . for name ( candidate . get bean class name ( ) , this . context . get class loader ( ) ) ) ; } } } return entity set ; }	Scan for entities with the specified annotations.
private void resolve name ( configuration metadata item item ) { item . set name ( item . get id ( ) ) ;	Resolve the name of an item against this instance.
protected void restart ( set < url > urls , class loader files files ) { restarter restarter = restarter . get instance ( ) ; restarter . add urls ( urls ) ; restarter . add class loader files ( files ) ; restarter . restart ( ) ; }	Called to restart the application.
public final void load ( class < ? > relative class , string ... resource names ) { resource [ ] resources = new resource [ resource names . length ] ; for ( int i = num ; i < resource names . length ; i ++ ) { resources [ i ] = new class path resource ( resource names [ i ] , relative class ) ; } this . reader . load bean definitions ( resources ) ; }	Load bean definitions from the given XML resources.
public void if bound ( consumer < ? super t > consumer ) { assert . not null ( consumer , str ) ; if ( this . value != null ) { consumer . accept ( this . value ) ; } }	Invoke the specified consumer with the bound value, or do nothing if no value hasbeen bound.
public < u > bind result < u > map ( function < ? super t , ? extends u > mapper ) { assert . not null ( mapper , str ) ; return of ( ( this . value != null ) ? mapper . apply ( this . value ) : null ) ; }	Apply the provided mapping function to the bound value, or return an updatedunbound result if no value has been bound.
public t or else create ( class < ? extends t > type ) { assert . not null ( type , str ) ; return ( this . value != null ) ? this . value : bean utils . instantiate class ( type ) ; }	Return the object that was bound, or a new instance of the specified class if novalue has been bound.
protected class loader create class loader ( list < archive > archives ) throws exception { list < url > urls = new array list < > ( archives . size ( ) ) ; for ( archive archive : archives ) { urls . add ( archive . get url ( ) ) ; } return create class loader ( urls . to array ( new url [ num ] ) ) ; }	Create a classloader for the specified archives.
public static string to dashed form ( string name , int start ) { string builder result = new string builder ( ) ; string replaced = name . replace ( str , str ) ; for ( int i = start ; i < replaced . length ( ) ; i ++ ) { char ch = replaced . char at ( i ) ; if ( character . is upper case ( ch ) && result . length ( ) > num && result . char at ( result . length ( ) - num ) != str ) { result . append ( str ) ; } result . append ( character . to lower case ( ch ) ) ; } return result . to string ( ) ; }	Return the specified Java Bean property name in dashed form.
public void replay to ( log destination ) { synchronized ( this . lines ) { for ( line line : this . lines ) { log to ( destination , line . get level ( ) , line . get message ( ) , line . get throwable ( ) ) ; } this . lines . clear ( ) ; } }	Replay deferred logging to the specified destination.
public static color converter new instance ( configuration config , string [ ] options ) { if ( options . length < num ) { logger . error ( str + str , options . length ) ; return null ; } if ( options [ num ] == null ) { logger . error ( str ) ; return null ; } pattern parser parser = pattern layout . create pattern parser ( config ) ; list < pattern formatter > formatters = parser . parse ( options [ num ] ) ; ansi element element = ( options . length != num ) ? elements . get ( options [ num ] ) : null ; return new color converter ( formatters , element ) ; }	Creates a new instance of the class.
public void run ( ) throws exception { if ( this . header . contains ( str ) && this . header . contains ( str ) ) { run web socket ( ) ; } if ( this . header . contains ( str ) ) { this . output stream . write http ( get class ( ) . get resource as stream ( str ) , str ) ; } }	Run the connection.
@ suppress warnings ( str ) public final object bind ( configuration property name name , bindable < ? > target , aggregate element binder element binder ) { object result = bind aggregate ( name , target , element binder ) ; supplier < ? > value = target . get value ( ) ; if ( result == null || value == null ) { return result ; } return merge ( ( supplier < t > ) value , ( t ) result ) ; }	Perform binding for the aggregate.
protected void add property sources ( configurable environment environment , resource loader resource loader ) { random value property source . add to environment ( environment ) ; new loader ( environment , resource loader ) . load ( ) ; }	Add config file property sources to the specified environment.
public void set status mapping ( map < string , integer > status mapping ) { assert . not null ( status mapping , str ) ; this . status mapping = new hash map < > ( status mapping ) ; }	Set specific status mappings.
public void add status mapping ( map < string , integer > status mapping ) { assert . not null ( status mapping , str ) ; this . status mapping . put all ( status mapping ) ; }	Add specific status mappings to the existing set.
public string get relative name ( ) { file folder = this . source folder . get absolute file ( ) ; file file = this . file . get absolute file ( ) ; string folder name = string utils . clean path ( folder . get path ( ) ) ; string file name = string utils . clean path ( file . get path ( ) ) ; assert . state ( file name . starts with ( folder name ) , ( ) -> str + file name + str + folder name ) ; return file name . substring ( folder name . length ( ) + num ) ; }	Return the name of the file relative to the source folder.
ansi string append ( string text , code ... codes ) { if ( codes . length == num || ! is ansi supported ( ) ) { this . value . append ( text ) ; return this ; } ansi ansi = ansi . ansi ( ) ; for ( code code : codes ) { ansi = apply code ( ansi , code ) ; } this . value . append ( ansi . a ( text ) . reset ( ) . to string ( ) ) ; return this ; }	Append text with the given ANSI codes.
protected void log disabled fork ( ) { if ( get log ( ) . is warn enabled ( ) ) { if ( has agent ( ) ) { get log ( ) . warn ( str ) ; } if ( has jvm args ( ) ) { run arguments run arguments = resolve jvm arguments ( ) ; get log ( ) . warn ( str + arrays . stream ( run arguments . as array ( ) ) . collect ( collectors . joining ( str ) ) + str ) ; } if ( has working directory set ( ) ) { get log ( ) . warn ( str ) ; } } }	Log a warning indicating that fork mode has been explicitly disabled while someconditions are present that require to enable it.
protected run arguments resolve jvm arguments ( ) { string builder string builder = new string builder ( ) ; if ( this . system property variables != null ) { string builder . append ( this . system property variables . entry set ( ) . stream ( ) . map ( ( e ) -> system property formatter . format ( e . get key ( ) , e . get value ( ) ) ) . collect ( collectors . joining ( str ) ) ) ; } if ( this . jvm arguments != null ) { string builder . append ( str ) . append ( this . jvm arguments ) ; } return new run arguments ( string builder . to string ( ) ) ; }	Resolve the JVM arguments to use.
protected condition outcome get resource outcome ( condition context context , annotated type metadata metadata ) { list < string > found = new array list < > ( ) ; for ( string location : this . resource locations ) { resource resource = context . get resource loader ( ) . get resource ( location ) ; if ( resource != null && resource . exists ( ) ) { found . add ( location ) ; } } if ( found . is empty ( ) ) { condition message message = start condition message ( ) . did not find ( str , str ) . items ( style . quote , arrays . as list ( this . resource locations ) ) ; return condition outcome . no match ( message ) ; } condition message message = start condition message ( ) . found ( str , str ) . items ( style . quote , found ) ; return condition outcome . match ( message ) ; }	Check if one of the default resource locations actually exists.
public static list < string > get ( bean factory bean factory ) { try { return bean factory . get bean ( bean , base packages . class ) . get ( ) ; } catch ( no such bean definition exception ex ) { throw new illegal state exception ( str ) ; } }	Return the auto-configuration base packages for the given bean factory.
public void clear cache ( ) { for ( url url : get ur ( ) ) { try { url connection = url . open connection ( ) ; if ( connection instanceof url ) { clear cache ( connection ) ; } } catch ( io ex ) {	Clear URL caches.
public int start ( ) throws io { synchronized ( this . monitor ) { assert . state ( this . server thread == null , str ) ; server socket channel server socket channel = server socket channel . open ( ) ; server socket channel . socket ( ) . bind ( new inet socket address ( this . listen port ) ) ; int port = server socket channel . socket ( ) . get local port ( ) ; logger . trace ( str + port ) ; this . server thread = new server thread ( server socket channel ) ; this . server thread . start ( ) ; return port ; } }	Start the client and accept incoming connections.
public void stop ( ) throws io { synchronized ( this . monitor ) { if ( this . server thread != null ) { this . server thread . close ( ) ; try { this . server thread . join ( num ) ; } catch ( interrupted exception ex ) { thread . current thread ( ) . interrupt ( ) ; } this . server thread = null ; } } }	Stop the client, disconnecting any servers.
protected final file create temp dir ( string prefix ) { try { file temp dir = file . create temp file ( prefix + str , str + get port ( ) ) ; temp dir . delete ( ) ; temp dir . mkdir ( ) ; temp dir . delete on exit ( ) ; return temp dir ; } catch ( io ex ) { throw new web server exception ( str + system . get property ( str ) , ex ) ; } }	Return the absolute temp dir for given web server.
private scope peek ( ) throws json { if ( this . stack . is empty ( ) ) { throw new json ( str ) ; } return this . stack . get ( this . stack . size ( ) - num ) ; }	Returns the value on the top of the stack.
protected map < string , object > aggregate details ( map < string , health > healths ) { return new linked hash map < > ( healths ) ; }	Return the map of 'aggregate' details that should be used from the specifiedhealths.
public void set listener ( t listener ) { assert . not null ( listener , str ) ; assert . is true ( is supported type ( listener ) , str ) ; this . listener = listener ; }	Set the listener that will be registered.
public int determine port ( ) { if ( collection utils . is empty ( this . parsed addresses ) ) { return get port ( ) ; } address address = this . parsed addresses . get ( num ) ; return address . port ; }	Returns the port from the first address, or the configured port if no addresseshave been set.
public void set status order ( status ... status order ) { string [ ] order = new string [ status order . length ] ; for ( int i = num ; i < status order . length ; i ++ ) { order [ i ] = status order [ i ] . get code ( ) ; } set status order ( arrays . as list ( order ) ) ; }	Set the ordering of the status.
protected boolean should register jsp servlet ( ) { return this . jsp != null && this . jsp . get registered ( ) && class utils . is present ( this . jsp . get class name ( ) , get class ( ) . get class loader ( ) ) ; }	Returns whether or not the JSP servlet should be registered with the web server.
protected void log startup profile info ( configurable application context context ) { log log = get application log ( ) ; if ( log . is info enabled ( ) ) { string [ ] active profiles = context . get environment ( ) . get active profiles ( ) ; if ( object utils . is empty ( active profiles ) ) { string [ ] default profiles = context . get environment ( ) . get default profiles ( ) ; log . info ( str + string utils . array to comma delimited string ( default profiles ) ) ; } else { log . info ( str + string utils . array to comma delimited string ( active profiles ) ) ; } } }	Called to log active profile information.
protected void load ( application context context , object [ ] sources ) { if ( logger . is debug enabled ( ) ) { logger . debug ( str + string utils . array to comma delimited string ( sources ) ) ; } bean definition loader loader = create bean definition loader ( get bean definition registry ( context ) , sources ) ; if ( this . bean name generator != null ) { loader . set bean name generator ( this . bean name generator ) ; } if ( this . resource loader != null ) { loader . set resource loader ( this . resource loader ) ; } if ( this . environment != null ) { loader . set environment ( this . environment ) ; } loader . load ( ) ; }	Load beans into the application context.
private bean definition registry get bean definition registry ( application context context ) { if ( context instanceof bean definition registry ) { return ( bean definition registry ) context ; } if ( context instanceof abstract application context ) { return ( bean definition registry ) ( ( abstract application context ) context ) . get bean factory ( ) ; } throw new illegal state exception ( str ) ; }	Get the bean definition registry.
protected void register logged exception ( throwable exception ) { spring boot exception handler handler = get spring boot exception handler ( ) ; if ( handler != null ) { handler . register logged exception ( exception ) ; } }	Register that the given exception has been logged.
public string add ( string extension , string mime type ) { mapping previous = this . map . put ( extension , new mapping ( extension , mime type ) ) ; return ( previous != null ) ? previous . get mime type ( ) : null ; }	Add a new mime mapping.
public string get ( string extension ) { mapping mapping = this . map . get ( extension ) ; return ( mapping != null ) ? mapping . get mime type ( ) : null ; }	Get a mime mapping for the given extension.
public string remove ( string extension ) { mapping previous = this . map . remove ( extension ) ; return ( previous != null ) ? previous . get mime type ( ) : null ; }	Remove an existing mapping.
public void set bean name generator ( bean name generator bean name generator ) { this . annotated reader . set bean name generator ( bean name generator ) ; this . xml reader . set bean name generator ( bean name generator ) ; this . scanner . set bean name generator ( bean name generator ) ; }	Set the bean name generator to be used by the underlying readers and scanner.
public void set resource loader ( resource loader resource loader ) { this . resource loader = resource loader ; this . xml reader . set resource loader ( resource loader ) ; this . scanner . set resource loader ( resource loader ) ; }	Set the resource loader to be used by the underlying readers and scanner.
public void set environment ( configurable environment environment ) { this . annotated reader . set environment ( environment ) ; this . xml reader . set environment ( environment ) ; this . scanner . set environment ( environment ) ; }	Set the environment to be used by the underlying readers and scanner.
public void put all ( map < ? , ? > map ) { assert . not null ( map , str ) ; assert not read only system attributes map ( map ) ; map . for each ( this :: put ) ; }	Add all entries from the specified map.
public void put ( object name , object value ) { this . source . put ( ( name != null ) ? name . to string ( ) : null , value ) ; }	Add an individual entry.
protected server thread get server thread ( ) throws io { synchronized ( this ) { if ( this . server thread == null ) { byte channel channel = this . server connection . open ( this . long poll timeout ) ; this . server thread = new server thread ( channel ) ; this . server thread . start ( ) ; } return this . server thread ; } }	Returns the active server thread, creating and starting it if necessary.
public mono < access level > get access level ( string token , string application id ) throws cloud foundry authorization exception { string uri = get permissions uri ( application id ) ; return this . web client . get ( ) . uri ( uri ) . header ( str , str + token ) . retrieve ( ) . body to mono ( map . class ) . map ( this :: get access level ) . on error map ( this :: map error ) ; }	Return a Mono of the access level that should be granted to the given token.
public mono < string > get uaa url ( ) { this . uaa url = this . web client . get ( ) . uri ( this . cloud controller url + str ) . retrieve ( ) . body to mono ( map . class ) . map ( ( response ) -> ( string ) response . get ( str ) ) . cache ( ) . on error map ( ( ex ) -> new cloud foundry authorization exception ( reason . service unavailable , str ) ) ; return this . uaa url ; }	Return a Mono of URL of the UAA.
protected final filter artifacts get filters ( artifacts filter ... additional filters ) { filter artifacts filters = new filter artifacts ( ) ; for ( artifacts filter additional filter : additional filters ) { filters . add filter ( additional filter ) ; } filters . add filter ( new matching group id filter ( clean filter config ( this . exclude group ids ) ) ) ; if ( this . includes != null && ! this . includes . is empty ( ) ) { filters . add filter ( new include filter ( this . includes ) ) ; } if ( this . excludes != null && ! this . excludes . is empty ( ) ) { filters . add filter ( new exclude filter ( this . excludes ) ) ; } return filters ; }	Return artifact filters configured for this MOJO.
public int size ( ) { int size = num ; for ( source folder source folder : this . source folders . values ( ) ) { size += source folder . get files ( ) . size ( ) ; } return size ; }	Return the size of the collection.
public int get exit code ( ) { int exit code = num ; for ( exit code generator generator : this . generators ) { try { int value = generator . get exit code ( ) ; if ( value > num && value > exit code || value < num && value < exit code ) { exit code = value ; } } catch ( exception ex ) { exit code = ( exit code != num ) ? exit code : num ; ex . print stack trace ( ) ; } } return exit code ; }	Get the final exit code that should be returned based on all contained generators.
public void handle ( server http request request , server http response response ) throws io { try { assert . state ( request . get headers ( ) . get content length ( ) > num , str ) ; object input stream object input stream = new object input stream ( request . get body ( ) ) ; class loader files files = ( class loader files ) object input stream . read object ( ) ; object input stream . close ( ) ; this . server . update and restart ( files ) ; response . set status code ( http status . ok ) ; } catch ( exception ex ) { logger . warn ( str , ex ) ; response . set status code ( http status . internal server error ) ; } }	Handle a server request.
protected file get port file ( application context application context ) { string namespace = get server namespace ( application context ) ; if ( string utils . is empty ( namespace ) ) { return this . file ; } string name = this . file . get name ( ) ; string extension = string utils . get filename extension ( this . file . get name ( ) ) ; name = name . substring ( num , name . length ( ) - extension . length ( ) - num ) ; if ( is upper case ( name ) ) { name = name + str + namespace . to upper case ( locale . english ) ; } else { name = name + str + namespace . to lower case ( locale . english ) ; } if ( string utils . has length ( extension ) ) { name = name + str + extension ; } return new file ( this . file . get parent file ( ) , name ) ; }	Return the actual port file that should be written for the given applicationcontext.
public static operation invoker apply ( operation invoker invoker , long time to live ) { if ( time to live > num ) { return new caching operation invoker ( invoker , time to live ) ; } return invoker ; }	Apply caching configuration when appropriate to the given invoker.
public void write to ( writable byte channel channel ) throws io { assert . not null ( channel , str ) ; while ( this . data . has remaining ( ) ) { channel . write ( this . data ) ; } }	Write the content of this payload to the given target channel.
public string to hex string ( ) { byte [ ] bytes = this . data . array ( ) ; char [ ] hex = new char [ this . data . remaining ( ) * num ] ; for ( int i = this . data . position ( ) ; i < this . data . remaining ( ) ; i ++ ) { int b = bytes [ i ] & num ; hex [ i * num ] = hex chars [ b > > > num ] ; hex [ i * num + num ] = hex chars [ b & num ] ; } return new string ( hex ) ; }	Return the payload as a hexadecimal string.
protected boolean is main ( thread thread ) { return thread . get name ( ) . equals ( str ) && thread . get context class loader ( ) . get class ( ) . get name ( ) . contains ( str ) ; }	Returns if the thread is for a main invocation.
public access level get access level ( string token , string application id ) throws cloud foundry authorization exception { try { uri uri = get permissions uri ( application id ) ; request entity < ? > request = request entity . get ( uri ) . header ( str , str + token ) . build ( ) ; map < ? , ? > body = this . rest template . exchange ( request , map . class ) . get body ( ) ; if ( boolean . true . equals ( body . get ( str ) ) ) { return access level . full ; } return access level . restricted ; } catch ( http client error exception ex ) { if ( ex . get status code ( ) . equals ( http status . forbidden ) ) { throw new cloud foundry authorization exception ( reason . access denied , str ) ; } throw new cloud foundry authorization exception ( reason . invalid token , str , ex ) ; } catch ( http server error exception ex ) { throw new cloud foundry authorization exception ( reason . service unavailable , str ) ; } }	Return the access level that should be granted to the given token.
public map < string , string > fetch token keys ( ) { try { return extract token keys ( this . rest template . get for object ( get uaa url ( ) + str , map . class ) ) ; } catch ( http status code exception ex ) { throw new cloud foundry authorization exception ( reason . service unavailable , str ) ; } }	Return all token keys known by the UAA.
public string get uaa url ( ) { if ( this . uaa url == null ) { try { map < ? , ? > response = this . rest template . get for object ( this . cloud controller url + str , map . class ) ; this . uaa url = ( string ) response . get ( str ) ; } catch ( http status code exception ex ) { throw new cloud foundry authorization exception ( reason . service unavailable , str ) ; } } return this . uaa url ; }	Return the URL of the UAA.
public void add url mappings ( string ... url mappings ) { assert . not null ( url mappings , str ) ; this . url mappings . add all ( arrays . as list ( url mappings ) ) ; }	Add URL mappings, as defined in the Servlet specification, for the servlet.
private configuration get error page configuration ( ) { return new abstract configuration ( ) { @ override public void configure ( web app context context ) throws exception { error handler error handler = context . get error handler ( ) ; context . set error handler ( new jetty embedded error handler ( error handler ) ) ; add jetty error pages ( error handler , get error pages ( ) ) ; } } ; }	Create a configuration object that adds error handlers.
private configuration get mime type configuration ( ) { return new abstract configuration ( ) { @ override public void configure ( web app context context ) throws exception { mime types mime types = context . get mime types ( ) ; for ( mime mappings . mapping mapping : get mime mappings ( ) ) { mime types . add mime mapping ( mapping . get extension ( ) , mapping . get mime type ( ) ) ; } } } ; }	Create a configuration object that adds mime type mappings.
public object next value ( ) throws json { int c = next clean internal ( ) ; switch ( c ) { case - num : throw syntax error ( str ) ; case str : return read object ( ) ; case str : return read array ( ) ; case str : case str : return next string ( ( char ) c ) ; default : this . pos -- ; return read literal ( ) ; } }	Returns the next value from the input.
public static list < repository configuration > create default repository configuration ( ) { maven settings maven settings = new maven settings reader ( ) . read settings ( ) ; list < repository configuration > repository configuration = new array list < > ( ) ; repository configuration . add ( maven central ) ; if ( ! boolean . get boolean ( str ) ) { repository configuration . add ( spring milestone ) ; repository configuration . add ( spring snapshot ) ; } add default cache as repository ( maven settings . get local repository ( ) , repository configuration ) ; add active profile repositories ( maven settings . get active profiles ( ) , repository configuration ) ; return repository configuration ; }	Create a new default repository configuration.
protected void handle invalid excludes ( list < string > invalid excludes ) { string builder message = new string builder ( ) ; for ( string exclude : invalid excludes ) { message . append ( str ) . append ( exclude ) . append ( string . format ( str ) ) ; } throw new illegal state exception ( string . format ( str + str , message ) ) ; }	Handle any invalid excludes that have been specified.
protected set < string > get exclusions ( annotation metadata metadata , annotation attributes attributes ) { set < string > excluded = new linked hash set < > ( ) ; excluded . add all ( as list ( attributes , str ) ) ; excluded . add all ( arrays . as list ( attributes . get string array ( str ) ) ) ; excluded . add all ( get exclude auto configurations property ( ) ) ; return excluded ; }	Return any exclusions that limit the candidate configurations.
protected configurations merge ( configurations other ) { set < class < ? > > merged classes = new linked hash set < > ( get classes ( ) ) ; merged classes . add all ( other . get classes ( ) ) ; return merge ( merged classes ) ; }	Merge configurations from another source of the same type.
public static class < ? > [ ] get classes ( collection < configurations > configurations ) { list < configurations > ordered = new array list < > ( configurations ) ; ordered . sort ( comparator ) ; list < configurations > collated = collate ( ordered ) ; linked hash set < class < ? > > classes = collated . stream ( ) . flat map ( configurations :: stream classes ) . collect ( collectors . to collection ( linked hash set :: new ) ) ; return class utils . to class array ( classes ) ; }	Return the classes from all the specified configurations in the order that theywould be registered.
private static string coerce to epoch ( string s ) { long epoch = parse epoch second ( s ) ; if ( epoch != null ) { return string . value of ( epoch ) ; } simple date format format = new simple date format ( str ) ; try { return string . value of ( format . parse ( s ) . get time ( ) ) ; } catch ( parse exception ex ) { return s ; } }	Attempt to convert the specified value to epoch time.
public void write ( file file ) throws io { assert . state ( this . pid != null , str ) ; create parent folder ( file ) ; if ( file . exists ( ) ) { assert can overwrite ( file ) ; } try ( file writer writer = new file writer ( file ) ) { writer . append ( this . pid ) ; } }	Write the PID to the specified file.
private void pre initialize leaky classes ( ) { try { class < ? > reader class = class name reader . class ; field field = reader class . get declared field ( str ) ; field . set accessible ( bool ) ; ( ( throwable ) field . get ( null ) ) . fill in stack trace ( ) ; } catch ( exception ex ) { this . logger . warn ( str , ex ) ; } }	CGLIB has a private exception field which needs to initialized early to ensure thatthe stacktrace doesn't retain a reference to the RestartClassLoader.
public void add urls ( collection < url > urls ) { assert . not null ( urls , str ) ; this . urls . add all ( urls ) ; }	Add additional URLs to be includes in the next restart.
public void restart ( failure handler failure handler ) { if ( ! this . enabled ) { this . logger . debug ( str ) ; return ; } this . logger . debug ( str ) ; get leak safe thread ( ) . call ( ( ) -> { restarter . this . stop ( ) ; restarter . this . start ( failure handler ) ; return null ; } ) ; }	Restart the running application.
protected void start ( failure handler failure handler ) throws exception { do { throwable error = do start ( ) ; if ( error == null ) { return ; } if ( failure handler . handle ( error ) == outcome . abort ) { return ; } } while ( bool ) ; }	Start the application.
protected throwable relaunch ( class loader class loader ) throws exception { restart launcher launcher = new restart launcher ( class loader , this . main class name , this . args , this . exception handler ) ; launcher . start ( ) ; launcher . join ( ) ; return launcher . get error ( ) ; }	Relaunch the application using the specified classloader.
protected void stop ( ) throws exception { this . logger . debug ( str ) ; this . stop lock . lock ( ) ; try { for ( configurable application context context : this . root contexts ) { context . close ( ) ; this . root contexts . remove ( context ) ; } cleanup caches ( ) ; if ( this . force reference cleanup ) { force reference cleanup ( ) ; } } finally { this . stop lock . unlock ( ) ; } system . gc ( ) ; system . run finalization ( ) ; }	Stop the application.
public string get last element ( form form ) { int size = get number of elements ( ) ; return ( size != num ) ? get element ( size - num , form ) : empty string ; }	Return the last element in the name in the given form.
public string get element ( int element index , form form ) { char sequence element = this . elements . get ( element index ) ; element type type = this . elements . get type ( element index ) ; if ( type . is indexed ( ) ) { return element . to string ( ) ; } if ( form == form . original ) { if ( type != element type . non uniform ) { return element . to string ( ) ; } return convert to original form ( element ) . to string ( ) ; } if ( form == form . dashed ) { if ( type == element type . uniform || type == element type . dashed ) { return element . to string ( ) ; } return convert to dashed element ( element ) . to string ( ) ; } char sequence uniform element = this . uniform elements [ element index ] ; if ( uniform element == null ) { uniform element = ( type != element type . uniform ) ? convert to uniform element ( element ) : element ; this . uniform elements [ element index ] = uniform element . to string ( ) ; } return uniform element . to string ( ) ; }	Return an element in the name in the given form.
public void add commands ( iterable < command > commands ) { assert . not null ( commands , str ) ; for ( command command : commands ) { add command ( command ) ; } }	Add the specified commands.
public command find command ( string name ) { for ( command candidate : this . commands ) { string candidate name = candidate . get name ( ) ; if ( candidate name . equals ( name ) || ( is option command ( candidate ) && ( str + candidate name ) . equals ( name ) ) ) { return candidate ; } } return null ; }	Find a command by name.
public int run and handle errors ( string ... args ) { string [ ] args without debug flags = remove debug flags ( args ) ; boolean debug = args without debug flags . length != args . length ; if ( debug ) { system . set property ( str , str ) ; } try { exit status result = run ( args without debug flags ) ;	Run the appropriate and handle and errors.
protected exit status run ( string ... args ) throws exception { if ( args . length == num ) { throw new no arguments exception ( ) ; } string command name = args [ num ] ; string [ ] command arguments = arrays . copy of range ( args , num , args . length ) ; command command = find command ( command name ) ; if ( command == null ) { throw new no such command exception ( command name ) ; } before run ( command ) ; try { return command . run ( command arguments ) ; } finally { after run ( command ) ; } }	Parse the arguments and run a suitable command.
private void apply serialization modifier ( object mapper mapper ) { serializer factory factory = bean serializer factory . instance . with serializer modifier ( new generic serializer modifier ( ) ) ; mapper . set serializer factory ( factory ) ; }	Ensure only bindable and non-cyclic bean properties are reported.
@ suppress warnings ( str ) private map < string , object > sanitize ( string prefix , map < string , object > map ) { map . for each ( ( key , value ) -> { string qualified key = ( prefix . is empty ( ) ? prefix : prefix + str ) + key ; if ( value instanceof map ) { map . put ( key , sanitize ( qualified key , ( map < string , object > ) value ) ) ; } else if ( value instanceof list ) { map . put ( key , sanitize ( qualified key , ( list < object > ) value ) ) ; } else { value = this . sanitizer . sanitize ( key , value ) ; value = this . sanitizer . sanitize ( qualified key , value ) ; map . put ( key , value ) ; } } ) ; return map ; }	Sanitize all unwanted configuration properties to avoid leaking of sensitiveinformation.
public void set keys to sanitize ( string ... keys to sanitize ) { assert . not null ( keys to sanitize , str ) ; this . keys to sanitize = new pattern [ keys to sanitize . length ] ; for ( int i = num ; i < keys to sanitize . length ; i ++ ) { this . keys to sanitize [ i ] = get pattern ( keys to sanitize [ i ] ) ; } }	Keys that should be sanitized.
public object sanitize ( string key , object value ) { if ( value == null ) { return null ; } for ( pattern pattern : this . keys to sanitize ) { if ( pattern . matcher ( key ) . matches ( ) ) { return str ; } } return value ; }	Sanitize the given value if necessary.
public static string number to string ( number number ) throws json { if ( number == null ) { throw new json ( str ) ; } double double value = number . double value ( ) ; json . check double ( double value ) ;	Encodes the number as a JSON string.
public void configure ( default jms listener container factory factory , connection factory connection factory ) { assert . not null ( factory , str ) ; assert . not null ( connection factory , str ) ; factory . set connection factory ( connection factory ) ; factory . set pub sub domain ( this . jms properties . is pub sub domain ( ) ) ; if ( this . transaction manager != null ) { factory . set transaction manager ( this . transaction manager ) ; } else { factory . set session transacted ( bool ) ; } if ( this . destination resolver != null ) { factory . set destination resolver ( this . destination resolver ) ; } if ( this . message converter != null ) { factory . set message converter ( this . message converter ) ; } jms properties . listener listener = this . jms properties . get listener ( ) ; factory . set auto startup ( listener . is auto startup ( ) ) ; if ( listener . get acknowledge mode ( ) != null ) { factory . set session acknowledge mode ( listener . get acknowledge mode ( ) . get mode ( ) ) ; } string concurrency = listener . format concurrency ( ) ; if ( concurrency != null ) { factory . set concurrency ( concurrency ) ; } }	Configure the specified jms listener container factory.
public void include ( configuration metadata repository repository ) { for ( configuration metadata group group : repository . get all groups ( ) . values ( ) ) { configuration metadata group existing group = this . all groups . get ( group . get id ( ) ) ; if ( existing group == null ) { this . all groups . put ( group . get id ( ) , group ) ; } else {	Merge the content of the specified repository to this repository.
protected void configure ssl ( ssl context factory factory , ssl ssl , ssl store provider ssl store provider ) { factory . set protocol ( ssl . get protocol ( ) ) ; configure ssl client auth ( factory , ssl ) ; configure ssl passwords ( factory , ssl ) ; factory . set cert alias ( ssl . get key alias ( ) ) ; if ( ! object utils . is empty ( ssl . get ciphers ( ) ) ) { factory . set include cipher suites ( ssl . get ciphers ( ) ) ; factory . set exclude cipher suites ( ) ; } if ( ssl . get enabled protocols ( ) != null ) { factory . set include protocols ( ssl . get enabled protocols ( ) ) ; } if ( ssl store provider != null ) { try { factory . set key store ( ssl store provider . get key store ( ) ) ; factory . set trust store ( ssl store provider . get trust store ( ) ) ; } catch ( exception ex ) { throw new illegal state exception ( str , ex ) ; } } else { configure ssl key store ( factory , ssl ) ; configure ssl trust store ( factory , ssl ) ; } }	Configure the SSL connection.
public void configure ( concurrent kafka listener container factory < object , object > listener factory , consumer factory < object , object > consumer factory ) { listener factory . set consumer factory ( consumer factory ) ; configure listener factory ( listener factory ) ; configure container ( listener factory . get container properties ( ) ) ; }	Configure the specified Kafka listener container factory.
public resource resolve config location ( ) { if ( this . config == null ) { return null ; } assert . is true ( this . config . exists ( ) , ( ) -> str + str + this . config . get description ( ) + str ) ; return this . config ; }	Resolve the config location if set.
public < t > t execute ( long wait , int max attempts , callable < t > callback ) throws exception { get log ( ) . debug ( str ) ; for ( int i = num ; i < max attempts ; i ++ ) { t result = callback . call ( ) ; if ( result != null ) { return result ; } string message = str + wait + str + ( i + num ) + str ; get log ( ) . debug ( message ) ; synchronized ( this . lock ) { try { this . lock . wait ( wait ) ; } catch ( interrupted exception ex ) { thread . current thread ( ) . interrupt ( ) ; throw new illegal state exception ( str ) ; } } } throw new mojo execution exception ( str + str + ( wait * max attempts ) + str ) ; }	Execute a task, retrying it on failure.
public void set init parameters ( map < string , string > init parameters ) { assert . not null ( init parameters , str ) ; this . init parameters = new linked hash map < > ( init parameters ) ; }	Set init-parameters for this registration.
public void add init parameter ( string name , string value ) { assert . not null ( name , str ) ; this . init parameters . put ( name , value ) ; }	Add a single init-parameter, replacing any existing parameter with the same name.
protected final string get or deduce name ( object value ) { return ( this . name != null ) ? this . name : conventions . get variable name ( value ) ; }	Deduces the name for this registration.
@ suppress warnings ( str ) public < a extends annotation > a get annotation ( class < a > type ) { for ( annotation annotation : this . annotations ) { if ( type . is instance ( annotation ) ) { return ( a ) annotation ; } } return null ; }	Return a single associated annotations that could affect binding.
@ suppress warnings ( str ) protected class < ? extends t > get cause type ( ) { return ( class < ? extends t > ) resolvable type . for class ( abstract failure analyzer . class , get class ( ) ) . resolve generic ( ) ; }	Return the cause type being handled by the analyzer.
private map < string , object > flatten ( map < string , object > map ) { map < string , object > result = new linked hash map < > ( ) ; flatten ( null , result , map ) ; return result ; }	Flatten the map keys using period separator.
public void record condition evaluation ( string source , condition condition , condition outcome outcome ) { assert . not null ( source , str ) ; assert . not null ( condition , str ) ; assert . not null ( outcome , str ) ; this . unconditional classes . remove ( source ) ; if ( ! this . outcomes . contains key ( source ) ) { this . outcomes . put ( source , new condition and outcomes ( ) ) ; } this . outcomes . get ( source ) . add ( condition , outcome ) ; this . added ancestor outcomes = bool ; }	Record the occurrence of condition evaluation.
public void record exclusions ( collection < string > exclusions ) { assert . not null ( exclusions , str ) ; this . exclusions . add all ( exclusions ) ; }	Records the names of the classes that have been excluded from condition evaluation.
public void record evaluation candidates ( list < string > evaluation candidates ) { assert . not null ( evaluation candidates , str ) ; this . unconditional classes . add all ( evaluation candidates ) ; }	Records the names of the classes that are candidates for condition evaluation.
public map < string , condition and outcomes > get condition and outcomes by source ( ) { if ( ! this . added ancestor outcomes ) { this . outcomes . for each ( ( source , source outcomes ) -> { if ( ! source outcomes . is full match ( ) ) { add no match outcome to ancestors ( source ) ; } } ) ; this . added ancestor outcomes = bool ; } return collections . unmodifiable map ( this . outcomes ) ; }	Returns condition outcomes from this report, grouped by the source.
public set < string > get unconditional classes ( ) { set < string > filtered = new hash set < > ( this . unconditional classes ) ; filtered . remove all ( this . exclusions ) ; return collections . unmodifiable set ( filtered ) ; }	Returns the names of the classes that were evaluated but were not conditional.
public boolean create schema ( ) { list < resource > scripts = get scripts ( str , this . properties . get schema ( ) , str ) ; if ( ! scripts . is empty ( ) ) { if ( ! is enabled ( ) ) { logger . debug ( str ) ; return bool ; } string username = this . properties . get schema username ( ) ; string password = this . properties . get schema password ( ) ; run scripts ( scripts , username , password ) ; } return ! scripts . is empty ( ) ; }	Create the schema if necessary.
public void init schema ( ) { list < resource > scripts = get scripts ( str , this . properties . get data ( ) , str ) ; if ( ! scripts . is empty ( ) ) { if ( ! is enabled ( ) ) { logger . debug ( str ) ; return ; } string username = this . properties . get data username ( ) ; string password = this . properties . get data password ( ) ; run scripts ( scripts , username , password ) ; } }	Initialize the schema if necessary.
public void set tld skip patterns ( collection < string > patterns ) { assert . not null ( patterns , str ) ; this . tld skip patterns = new linked hash set < > ( patterns ) ; }	Set the patterns that match jars to ignore for TLD scanning.
public void add tld skip patterns ( string ... patterns ) { assert . not null ( patterns , str ) ; this . tld skip patterns . add all ( arrays . as list ( patterns ) ) ; }	Add patterns that match jars to ignore for TLD scanning.
public static boolean is terminal ( array list < configuration > beam ) { for ( configuration configuration : beam ) if ( ! configuration . state . is terminal state ( ) ) return bool ; return bool ; }	Shows true if all of the configurations in the beam are in the terminal state.
public string [ ] [ ] to word tag ner array ( ner tag set ) { list < string [ ] > tuple list = utility . convert sentence to ner ( this , tag set ) ; string [ ] [ ] result = new string [ num ] [ tuple list . size ( ) ] ; iterator < string [ ] > iterator = tuple list . iterator ( ) ; for ( int i = num ; i < result [ num ] . length ; i ++ ) { string [ ] tuple = iterator . next ( ) ; for ( int j = num ; j < num ; ++ j ) { result [ j ] [ i ] = tuple [ j ] ; } } return result ; }	word pos ner.
protected int add word to vocab ( string word ) { vocab [ vocab size ] = new vocab word ( word ) ; vocab size ++ ;	Adds a word to the vocabulary.
int search vocab ( string word ) { if ( word == null ) return - num ; integer pos = vocab index map . get ( word ) ; return pos == null ? - num : pos . int value ( ) ; }	Returns position of a word in the vocabulary; if the word is not found, returns -1.
void sort vocab ( ) { arrays . sort ( vocab , num , vocab size ) ;	Sorts the vocabulary by frequency using word counts.
void create binary tree ( ) { int [ ] point = new int [ vocab word . max code length ] ; char [ ] code = new char [ vocab word . max code length ] ; int [ ] count = new int [ vocab size * num + num ] ; char [ ] binary = new char [ vocab size * num + num ] ; int [ ] parent node = new int [ vocab size * num + num ] ; for ( int i = num ; i < vocab size ; i ++ ) count [ i ] = vocab [ i ] . cn ; for ( int i = vocab size ; i < vocab size * num ; i ++ ) count [ i ] = integer . max value ; int pos1 = vocab size - num ; int pos2 = vocab size ;	Create binary Huffman tree using the word counts.Frequent words will have short uniqe binary codes.
@ override public int [ ] to id list ( int code point ) { int count ; if ( code point < num ) count = num ; else if ( code point < num ) count = num ; else if ( code point < num ) count = num ; else if ( code point < num ) count = num ; else if ( code point < num ) count = num ; else if ( code point <= num ) count = num ; else return emptylist ; int [ ] r = new int [ count ] ; switch ( count ) { case num : r [ num ] = ( char ) ( num | ( code point & num ) ) ; code point = code point > > num ; code point |= num ; case num : r [ num ] = ( char ) ( num | ( code point & num ) ) ; code point = code point > > num ; code point |= num ; case num : r [ num ] = ( char ) ( num | ( code point & num ) ) ; code point = code point > > num ; code point |= num ; case num : r [ num ] = ( char ) ( num | ( code point & num ) ) ; code point = code point > > num ; code point |= num ; case num : r [ num ] = ( char ) ( num | ( code point & num ) ) ; code point = code point > > num ; code point |= num ; case num : r [ num ] = ( char ) code point ; } return r ; }	codes ported from iconv lib in utf8.h utf8_codepointtomb.
void normalize ( ) { double nrm = norm ( ) ; for ( map . entry < integer , double > d : entry set ( ) ) { d . set value ( d . get value ( ) / nrm ) ; } }	Normalize a vector.
void multiply constant ( double x ) { for ( map . entry < integer , double > entry : entry set ( ) ) { entry . set value ( entry . get value ( ) * x ) ; } }	Multiply each value of avector by a constant value.
void add vector ( sparse vector vec ) { for ( map . entry < integer , double > entry : vec . entry set ( ) ) { double v = get ( entry . get key ( ) ) ; if ( v == null ) v = num ; put ( entry . get key ( ) , v + entry . get value ( ) ) ; } }	Add other vector.
static double inner product ( sparse vector vec1 , sparse vector vec2 ) { iterator < map . entry < integer , double > > it ; sparse vector other ; if ( vec1 . size ( ) < vec2 . size ( ) ) { it = vec1 . entry set ( ) . iterator ( ) ; other = vec2 ; } else { it = vec2 . entry set ( ) . iterator ( ) ; other = vec1 ; } double prod = num ; while ( it . has next ( ) ) { map . entry < integer , double > entry = it . next ( ) ; prod += entry . get value ( ) * other . get ( entry . get key ( ) ) ; } return prod ; }	Calculate the inner product value between vectors.
double cosine ( sparse vector vec1 , sparse vector vec2 ) { double norm1 = vec1 . norm ( ) ; double norm2 = vec2 . norm ( ) ; double result = num ; if ( norm1 == num && norm2 == num ) { return result ; } else { double prod = inner product ( vec1 , vec2 ) ; result = prod / ( norm1 * norm2 ) ; return double . is na n ( result ) ? num : result ; } }	Calculate the cosine value between vectors.
void reduce vocab ( ) { table = new int [ vocab size ] ; int j = num ; for ( int i = num ; i < vocab size ; i ++ ) { if ( vocab [ i ] . cn > min reduce ) { vocab [ j ] . cn = vocab [ i ] . cn ; vocab [ j ] . word = vocab [ i ] . word ; table [ vocab index map . get ( vocab [ j ] . word ) ] = j ; j ++ ; } else { table [ vocab index map . get ( vocab [ j ] . word ) ] = - num ; } }	Reduces the vocabulary by removing infrequent tokens.
string read word ( buffered reader raf ) throws io { while ( bool ) {	Reads a single word from a file, assuming space + tab + EOL to be word boundaries.
void set composite vector ( ) { composite . clear ( ) ; for ( document < k > document : documents ) { composite . add vector ( document . feature ( ) ) ; } }	Add the vectors of all documents to a composite vector.
sparse vector centroid vector ( ) { if ( documents . size ( ) > num && composite . size ( ) == num ) set composite vector ( ) ; centroid = ( sparse vector ) composite vector ( ) . clone ( ) ; centroid . normalize ( ) ; return centroid ; }	Get the pointer of a centroid vector.
void refresh ( ) { list iterator < document < k > > list iterator = documents . list iterator ( ) ; while ( list iterator . has next ( ) ) { if ( list iterator . next ( ) == null ) list iterator . remove ( ) ; } }	Delete removed documents from the internal container.
void set sectioned gain ( ) { double gain = num ; if ( sectioned gain == num && sectioned clusters . size ( ) > num ) { for ( cluster < k > cluster : sectioned clusters ) { gain += cluster . composite vector ( ) . norm ( ) ; } gain -= composite . norm ( ) ; } sectioned gain = gain ; }	Set a gain when the cluster sectioned.
public static list < string > parse or exit ( object target , string [ ] args ) { try { return parse ( target , args ) ; } catch ( illegal argument exception e ) { system . err . println ( e . get message ( ) ) ; args . usage ( target ) ; system . exit ( num ) ; throw e ; } }	A convenience method for parsing and automatically producing error messages.
public static matrix construct with copy ( double [ ] [ ] a ) { int m = a . length ; int n = a [ num ] . length ; matrix x = new matrix ( m , n ) ; double [ ] [ ] c = x . get array ( ) ; for ( int i = num ; i < m ; i ++ ) { if ( a [ i ] . length != n ) { throw new illegal argument exception ( str ) ; } for ( int j = num ; j < n ; j ++ ) { c [ i ] [ j ] = a [ i ] [ j ] ; } } return x ; }	Construct a matrix from a copy of a 2-D array.
public matrix copy ( ) { matrix x = new matrix ( m , n ) ; double [ ] [ ] c = x . get array ( ) ; for ( int i = num ; i < m ; i ++ ) { for ( int j = num ; j < n ; j ++ ) { c [ i ] [ j ] = a [ i ] [ j ] ; } } return x ; }	Make a deep copy of a matrix.
public double [ ] [ ] get array copy ( ) { double [ ] [ ] c = new double [ m ] [ n ] ; for ( int i = num ; i < m ; i ++ ) { for ( int j = num ; j < n ; j ++ ) { c [ i ] [ j ] = a [ i ] [ j ] ; } } return c ; }	Copy the internal two-dimensional array.
public matrix plus ( matrix b ) { check matrix dimensions ( b ) ; matrix x = new matrix ( m , n ) ; double [ ] [ ] c = x . get array ( ) ; for ( int i = num ; i < m ; i ++ ) { for ( int j = num ; j < n ; j ++ ) { c [ i ] [ j ] = a [ i ] [ j ] + b . a [ i ] [ j ] ; } } return x ; }	C = A + B.
public matrix plus equals ( matrix b ) { check matrix dimensions ( b ) ; for ( int i = num ; i < m ; i ++ ) { for ( int j = num ; j < n ; j ++ ) { a [ i ] [ j ] = a [ i ] [ j ] + b . a [ i ] [ j ] ; } } return this ; }	A = A + B.
public double trace ( ) { double t = num ; for ( int i = num ; i < math . min ( m , n ) ; i ++ ) { t += a [ i ] [ i ] ; } return t ; }	Matrix trace.
public static matrix random ( int m , int n ) { matrix a = new matrix ( m , n ) ; double [ ] [ ] x = a . get array ( ) ; for ( int i = num ; i < m ; i ++ ) { for ( int j = num ; j < n ; j ++ ) { x [ i ] [ j ] = math . random ( ) ; } } return a ; }	Generate matrix with random elements.
public static matrix identity ( int m , int n ) { matrix a = new matrix ( m , n ) ; double [ ] [ ] x = a . get array ( ) ; for ( int i = num ; i < m ; i ++ ) { for ( int j = num ; j < n ; j ++ ) { x [ i ] [ j ] = ( i == j ? num : num ) ; } } return a ; }	Generate identity matrix.
public void print ( int w , int d ) { print ( new print writer ( system . out , bool ) , w , d ) ; }	Print the matrix to stdout.
public void print ( print writer output , int w , int d ) { decimal format format = new decimal format ( ) ; format . set decimal format symbols ( new decimal format symbols ( locale . us ) ) ; format . set minimum integer digits ( num ) ; format . set maximum fraction digits ( d ) ; format . set minimum fraction digits ( d ) ; format . set grouping used ( bool ) ; print ( output , format , w + num ) ; }	Print the matrix to the output stream.
public void print ( number format format , int width ) { print ( new print writer ( system . out , bool ) , format , width ) ; }	Print the matrix to stdout.
public final void add strings ( collection < string > str collection ) { if ( source node != null ) { string previous string = str ;	Adds a Collection of Strings to the MDAG.
public void add string ( string str ) { if ( source node != null ) { add string internal ( str ) ; replace or register ( source node , str ) ; } else { un simplify ( ) ; add string ( str ) ; } }	Adds a string to the MDAG.
private int calculate sole transition path length ( string str ) { stack < mdag > transition path node stack = source node . get transition path nodes ( str ) ; transition path node stack . pop ( ) ;	Calculates the length of the the sub-path in a _transition path, that is used only by a given string.
public void remove string ( string str ) { if ( source node != null ) {	Removes a String from the MDAG.
private string determine longest prefix in mdag ( string str ) { mdag current node = source node ; int number of chars = str . length ( ) ; int one past prefix end index = num ;	Determines the longest prefix of a given String that isthe prefix of another String previously added to the MDAG.
private int create simple mdag ( mdag node , mdag [ ] mdag data array , int one past last created transition set index ) { int pivot index = one past last created transition set index ;	Creates a SimpleMDAGNode version of an MDAGNode's outgoing _transition set in mdagDataArray.
private static boolean is accept node ( object node obj ) { if ( node obj != null ) { class node obj class = node obj . get class ( ) ; if ( node obj class . equals ( mdag . class ) ) return ( ( mdag ) node obj ) . is accept node ( ) ; else if ( node obj class . equals ( mdag . class ) ) return ( ( mdag ) node obj ) . is accept node ( ) ; } throw new illegal argument exception ( str ) ; }	Determines if a child node object is accepting.
public mdag transition ( mdag [ ] mdag data array , char letter ) { mdag target node = null ; int offset = binary search ( mdag data array , letter ) ; if ( offset >= num ) { target node = mdag data array [ offset ] ; }	Follows an outgoing _transition from this node.
public static mdag traverse mdag ( mdag [ ] mdag data array , mdag source node , string str ) {	Follows a _transition path starting from the source node of a MDAG.
public boolean is nonprojective ( ) { for ( int dep1 : gold dependencies . key set ( ) ) { int head1 = gold dependencies . get ( dep1 ) . head index ; for ( int dep2 : gold dependencies . key set ( ) ) { int head2 = gold dependencies . get ( dep2 ) . head index ; if ( head1 < num || head2 < num ) continue ; if ( dep1 > head1 && head1 != head2 ) if ( ( dep1 > head2 && dep1 < dep2 && head1 < head2 ) || ( dep1 < head2 && dep1 > dep2 && head1 < dep2 ) ) return bool ; if ( dep1 < head1 && head1 != head2 ) if ( ( head1 > head2 && head1 < dep2 && dep1 < head2 ) || ( head1 < head2 && head1 > dep2 && dep1 < dep2 ) ) return bool ; } } return bool ; }	Shows whether the tree to train is projective or not.
public void save ( output stream stream ) throws io { data output stream out = null ; try { out = new data output stream ( new buffered output stream ( stream ) ) ; for ( int i = num ; i < array . length ; ++ i ) { out . write int ( array [ i ] ) ; } } finally { if ( out != null ) { out . close ( ) ; } } }	Saves the trie data into a stream.
public int exact match search ( byte [ ] key ) { int unit = array [ num ] ; int node pos = num ; for ( byte b : key ) {	Returns the corresponding value if the key is found.
public list < pair < integer , integer > > common prefix search ( byte [ ] key , int offset , int max results ) { array list < pair < integer , integer > > result = new array list < pair < integer , integer > > ( ) ; int unit = array [ num ] ; int node pos = num ;	Returns the keys that begins with the given key and its corresponding values.The first of the returned pair represents the length of the found key.
private void write array header ( byte buf allocator allocator , array header redis message msg , list < object > out ) { write array header ( allocator , msg . is null ( ) , msg . length ( ) , out ) ; }	Write array header only without body.
private void write array message ( byte buf allocator allocator , array redis message msg , list < object > out ) { if ( msg . is null ( ) ) { write array header ( allocator , msg . is null ( ) , redis constants . null value , out ) ; } else { write array header ( allocator , msg . is null ( ) , msg . children ( ) . size ( ) , out ) ; for ( redis message child : msg . children ( ) ) { write redis message ( allocator , child , out ) ; } } }	Write full constructed array message.
public static byte buf wrapped buffer ( byte buffer buffer ) { if ( ! buffer . has remaining ( ) ) { return empty buffer ; } if ( ! buffer . is direct ( ) && buffer . has array ( ) ) { return wrapped buffer ( buffer . array ( ) , buffer . array offset ( ) + buffer . position ( ) , buffer . remaining ( ) ) . order ( buffer . order ( ) ) ; } else if ( platform dependent . has unsafe ( ) ) { if ( buffer . is read only ( ) ) { if ( buffer . is direct ( ) ) { return new read only unsafe direct byte buf ( alloc , buffer ) ; } else { return new read only byte buffer buf ( alloc , buffer ) ; } } else { return new unpooled unsafe direct byte buf ( alloc , buffer , buffer . remaining ( ) ) ; } } else { if ( buffer . is read only ( ) ) { return new read only byte buffer buf ( alloc , buffer ) ; } else { return new unpooled direct byte buf ( alloc , buffer , buffer . remaining ( ) ) ; } } }	Creates a new buffer which wraps the specified NIO buffer's currentslice.
public static byte buf wrapped buffer ( byte buf buffer ) { if ( buffer . is readable ( ) ) { return buffer . slice ( ) ; } else { buffer . release ( ) ; return empty buffer ; } }	Creates a new buffer which wraps the specified buffer's readable bytes.A modification on the specified buffer's content will be visible to thereturned buffer.
public static byte buf wrapped buffer ( int max num components , byte buf ... buffers ) { switch ( buffers . length ) { case num : break ; case num : byte buf buffer = buffers [ num ] ; if ( buffer . is readable ( ) ) { return wrapped buffer ( buffer . order ( big endian ) ) ; } else { buffer . release ( ) ; } break ; default : for ( int i = num ; i < buffers . length ; i ++ ) { byte buf buf = buffers [ i ] ; if ( buf . is readable ( ) ) { return new composite byte buf ( alloc , bool , max num components , buffers , i ) ; } buf . release ( ) ; } break ; } return empty buffer ; }	Creates a new big-endian composite buffer which wraps the readable bytes of thespecified buffers without copying them.
public static byte buf wrapped buffer ( int max num components , byte buffer ... buffers ) { return wrapped buffer ( max num components , composite byte buf . byte buffer wrapper , buffers ) ; }	Creates a new big-endian composite buffer which wraps the slices of the specifiedNIO buffers without copying them.
public static byte buf copy int ( int value ) { byte buf buf = buffer ( num ) ; buf . write int ( value ) ; return buf ; }	Creates a new 4-byte big-endian buffer that holds the specified 32-bit integer.
public static byte buf copy int ( int ... values ) { if ( values == null || values . length == num ) { return empty buffer ; } byte buf buffer = buffer ( values . length * num ) ; for ( int v : values ) { buffer . write int ( v ) ; } return buffer ; }	Create a big-endian buffer that holds a sequence of the specified 32-bit integers.
public static byte buf copy short ( int value ) { byte buf buf = buffer ( num ) ; buf . write short ( value ) ; return buf ; }	Creates a new 2-byte big-endian buffer that holds the specified 16-bit integer.
public static byte buf copy short ( short ... values ) { if ( values == null || values . length == num ) { return empty buffer ; } byte buf buffer = buffer ( values . length * num ) ; for ( int v : values ) { buffer . write short ( v ) ; } return buffer ; }	Create a new big-endian buffer that holds a sequence of the specified 16-bit integers.
public static byte buf copy medium ( int value ) { byte buf buf = buffer ( num ) ; buf . write medium ( value ) ; return buf ; }	Creates a new 3-byte big-endian buffer that holds the specified 24-bit integer.
public static byte buf copy medium ( int ... values ) { if ( values == null || values . length == num ) { return empty buffer ; } byte buf buffer = buffer ( values . length * num ) ; for ( int v : values ) { buffer . write medium ( v ) ; } return buffer ; }	Create a new big-endian buffer that holds a sequence of the specified 24-bit integers.
public static byte buf copy long ( long value ) { byte buf buf = buffer ( num ) ; buf . write long ( value ) ; return buf ; }	Creates a new 8-byte big-endian buffer that holds the specified 64-bit integer.
public static byte buf copy long ( long ... values ) { if ( values == null || values . length == num ) { return empty buffer ; } byte buf buffer = buffer ( values . length * num ) ; for ( long v : values ) { buffer . write long ( v ) ; } return buffer ; }	Create a new big-endian buffer that holds a sequence of the specified 64-bit integers.
public static byte buf copy boolean ( boolean value ) { byte buf buf = buffer ( num ) ; buf . write boolean ( value ) ; return buf ; }	Creates a new single-byte big-endian buffer that holds the specified boolean value.
public static byte buf copy boolean ( boolean ... values ) { if ( values == null || values . length == num ) { return empty buffer ; } byte buf buffer = buffer ( values . length ) ; for ( boolean v : values ) { buffer . write boolean ( v ) ; } return buffer ; }	Create a new big-endian buffer that holds a sequence of the specified boolean values.
public static byte buf copy float ( float value ) { byte buf buf = buffer ( num ) ; buf . write float ( value ) ; return buf ; }	Creates a new 4-byte big-endian buffer that holds the specified 32-bit floating point number.
public static byte buf copy float ( float ... values ) { if ( values == null || values . length == num ) { return empty buffer ; } byte buf buffer = buffer ( values . length * num ) ; for ( float v : values ) { buffer . write float ( v ) ; } return buffer ; }	Create a new big-endian buffer that holds a sequence of the specified 32-bit floating point numbers.
public static byte buf copy double ( double value ) { byte buf buf = buffer ( num ) ; buf . write double ( value ) ; return buf ; }	Creates a new 8-byte big-endian buffer that holds the specified 64-bit floating point number.
public static byte buf copy double ( double ... values ) { if ( values == null || values . length == num ) { return empty buffer ; } byte buf buffer = buffer ( values . length * num ) ; for ( double v : values ) { buffer . write double ( v ) ; } return buffer ; }	Create a new big-endian buffer that holds a sequence of the specified 64-bit floating point numbers.
private static void encode extras ( byte buf buf , byte buf extras ) { if ( extras == null || ! extras . is readable ( ) ) { return ; } buf . write bytes ( extras ) ; }	Encode the extras.
public web socket server handshaker new handshaker ( http request req ) { char sequence version = req . headers ( ) . get ( http header names . sec websocket version ) ; if ( version != null ) { if ( version . equals ( web socket version . v13 . to http header value ( ) ) ) {	Instances a new handshaker.
public static channel future send unsupported version response ( channel channel , channel promise promise ) { http response res = new default full http response ( http version . http 1 1 , http response status . upgrade required ) ; res . headers ( ) . set ( http header names . sec websocket version , web socket version . v13 . to http header value ( ) ) ; http util . set content length ( res , num ) ; return channel . write and flush ( res , promise ) ; }	Return that we need cannot not support the web socket version.
public static read only http2 headers server headers ( boolean validate headers , ascii string status , ascii string ... other headers ) { return new read only http2 headers ( validate headers , new ascii string [ ] { pseudo header name . status . value ( ) , status } , other headers ) ; }	Create a new read only representation of headers used by servers.
public int bwt ( ) { final int [ ] sa = this . sa ; final byte [ ] t = this . t ; final int n = this . n ; final int [ ] bucket a = new int [ bucket a size ] ; final int [ ] bucket b = new int [ bucket b size ] ; if ( n == num ) { return num ; } if ( n == num ) { sa [ num ] = t [ num ] ; return num ; } int m = sort type bstar ( bucket a , bucket b ) ; if ( num < m ) { return construct bwt ( bucket a , bucket b ) ; } return num ; }	Performs a Burrows Wheeler Transform on the input array.
private composite byte buf add components ( boolean increase index , int c index , iterable < byte buf > buffers ) { if ( buffers instanceof byte buf ) {	but we do in the most common case that the Iterable is a Collection).
private void consolidate if needed ( ) {	This should only be called as last operation from a method as this may adjust the underlyingarray of components and so affect the index etc.
public ascii string decode ( byte buf buf , int length ) throws http2 exception { processor . reset ( ) ; buf . for each byte ( buf . reader index ( ) , length , processor ) ; buf . skip bytes ( length ) ; return processor . end ( ) ; }	Decompresses the given Huffman coded string literal.
public static class resolver weak caching resolver ( class loader class loader ) { return new caching class resolver ( new class loader class resolver ( default class loader ( class loader ) ) , new weak reference map < string , class < ? > > ( new hash map < string , reference < class < ? > > > ( ) ) ) ; }	non-aggressive non-concurrent cachegood for non-shared default cache.
public static class resolver soft caching resolver ( class loader class loader ) { return new caching class resolver ( new class loader class resolver ( default class loader ( class loader ) ) , new soft reference map < string , class < ? > > ( new hash map < string , reference < class < ? > > > ( ) ) ) ; }	aggressive non-concurrent cachegood for non-shared cache, when we're not worried about class unloading.
public static class resolver weak caching concurrent resolver ( class loader class loader ) { return new caching class resolver ( new class loader class resolver ( default class loader ( class loader ) ) , new weak reference map < string , class < ? > > ( platform dependent . < string , reference < class < ? > > > new concurrent hash map ( ) ) ) ; }	non-aggressive concurrent cachegood for shared cache, when we're worried about class unloading.
public static class resolver soft caching concurrent resolver ( class loader class loader ) { return new caching class resolver ( new class loader class resolver ( default class loader ( class loader ) ) , new soft reference map < string , class < ? > > ( platform dependent . < string , reference < class < ? > > > new concurrent hash map ( ) ) ) ; }	aggressive concurrent cachegood for shared cache, when we're not worried about class unloading.
public static boolean is multipart ( http request request ) { if ( request . headers ( ) . contains ( http header names . content type ) ) { return get multipart data boundary ( request . headers ( ) . get ( http header names . content type ) ) != null ; } else { return bool ; } }	Check if the given request is a multipart request.
protected static string [ ] get multipart data boundary ( string content type ) {	Check from the request ContentType if this request is a Multipart request.
protected ssl handler new handler ( byte buf allocator alloc , boolean start tls , executor executor ) { return new ssl handler ( new engine ( alloc ) , start tls , executor ) ; }	Create a new SslHandler.
private static string format simple ( channel handler context ctx , string event name , object msg ) { string ch str = ctx . channel ( ) . to string ( ) ; string msg str = string . value of ( msg ) ; string builder buf = new string builder ( ch str . length ( ) + num + event name . length ( ) + num + msg str . length ( ) ) ; return buf . append ( ch str ) . append ( str ) . append ( event name ) . append ( str ) . append ( msg str ) . to string ( ) ; }	Generates the default log message of the specified event whose argument is an arbitrary object.
final void clear ( ) { while ( ! resolve cache . is empty ( ) ) { for ( iterator < entry < string , entries > > i = resolve cache . entry set ( ) . iterator ( ) ; i . has next ( ) ; ) { map . entry < string , entries > e = i . next ( ) ; i . remove ( ) ; e . get value ( ) . clear and cancel ( ) ; } } }	Remove everything from the cache.
final list < ? extends e > get ( string hostname ) { entries entries = resolve cache . get ( hostname ) ; return entries == null ? null : entries . get ( ) ; }	Returns all caches entries for the given hostname.
final void cache ( string hostname , e value , int ttl , event loop loop ) { entries entries = resolve cache . get ( hostname ) ; if ( entries == null ) { entries = new entries ( hostname ) ; entries old entries = resolve cache . put if absent ( hostname , entries ) ; if ( old entries != null ) { entries = old entries ; } } entries . add ( value , ttl , loop ) ; }	Cache a value for the given hostname that will automatically expire once the TTL is reached.
@ suppress warnings ( str ) public final v get ( ) { internal thread local map thread local map = internal thread local map . get ( ) ; object v = thread local map . indexed variable ( index ) ; if ( v != internal thread local map . unset ) { return ( v ) v ; } return initialize ( thread local map ) ; }	Returns the current value for the current thread.
public final void set ( v value ) { if ( value != internal thread local map . unset ) { internal thread local map thread local map = internal thread local map . get ( ) ; set known not unset ( thread local map , value ) ; } else { remove ( ) ; } }	Set the value for the current thread.
@ unstable api public void set ocsp response ( byte [ ] response ) { if ( ! enable ocsp ) { throw new illegal state exception ( str ) ; } if ( client mode ) { throw new illegal state exception ( str ) ; } synchronized ( this ) { ssl . set ocsp response ( ssl , response ) ; } }	Sets the OCSP response.
public final synchronized void shutdown ( ) { if ( destroyed updater . compare and set ( this , num , num ) ) { engine map . remove ( ssl ) ; ssl . free ssl ( ssl ) ; ssl = network bio = num ; is inbound done = outbound closed = bool ; }	Destroys this engine.
private int write plaintext data ( final byte buffer src , int len ) { final int pos = src . position ( ) ; final int limit = src . limit ( ) ; final int ssl wrote ; if ( src . is direct ( ) ) { ssl wrote = ssl . write to ssl ( ssl , buffer address ( src ) + pos , len ) ; if ( ssl wrote > num ) { src . position ( pos + ssl wrote ) ; } } else { byte buf buf = alloc . direct buffer ( len ) ; try { src . limit ( pos + len ) ; buf . set bytes ( num , src ) ; src . limit ( limit ) ; ssl wrote = ssl . write to ssl ( ssl , memory address ( buf ) , len ) ; if ( ssl wrote > num ) { src . position ( pos + ssl wrote ) ; } else { src . position ( pos ) ; } } finally { buf . release ( ) ; } } return ssl wrote ; }	Write plaintext data to the OpenSSL internal BIOCalling this function with src.remaining == 0 is undefined.
private byte buf write encrypted data ( final byte buffer src , int len ) { final int pos = src . position ( ) ; if ( src . is direct ( ) ) { ssl . bio set byte buffer ( network bio , buffer address ( src ) + pos , len , bool ) ; } else { final byte buf buf = alloc . direct buffer ( len ) ; try { final int limit = src . limit ( ) ; src . limit ( pos + len ) ; buf . write bytes ( src ) ;	Write encrypted data to the OpenSSL network BIO.
private int read plaintext data ( final byte buffer dst ) { final int ssl read ; final int pos = dst . position ( ) ; if ( dst . is direct ( ) ) { ssl read = ssl . read from ssl ( ssl , buffer address ( dst ) + pos , dst . limit ( ) - pos ) ; if ( ssl read > num ) { dst . position ( pos + ssl read ) ; } } else { final int limit = dst . limit ( ) ; final int len = min ( max encrypted packet length0 ( ) , limit - pos ) ; final byte buf buf = alloc . direct buffer ( len ) ; try { ssl read = ssl . read from ssl ( ssl , memory address ( buf ) , len ) ; if ( ssl read > num ) { dst . limit ( pos + ssl read ) ; buf . get bytes ( buf . reader index ( ) , dst ) ; dst . limit ( limit ) ; } } finally { buf . release ( ) ; } } return ssl read ; }	Read plaintext data from the OpenSSL internal BIO.
private ssl shutdown with error ( string operations , int ssl error ) { return shutdown with error ( operations , ssl error , ssl . get last error number ( ) ) ; }	Log the error, shutdown the engine and throw an exception.
private string to java cipher suite ( string open ssl cipher suite ) { if ( open ssl cipher suite == null ) { return null ; } string version = ssl . get version ( ssl ) ; string prefix = to java cipher suite prefix ( version ) ; return cipher suite converter . to java ( open ssl cipher suite , prefix ) ; }	Converts the specified OpenSSL cipher suite to the Java cipher suite.
static byte buf do encode ( byte buf allocator byte buf allocator , mqtt message message ) { switch ( message . fixed header ( ) . message type ( ) ) { case connect : return encode connect message ( byte buf allocator , ( mqtt connect message ) message ) ; case connack : return encode conn ack message ( byte buf allocator , ( mqtt conn ack message ) message ) ; case publish : return encode publish message ( byte buf allocator , ( mqtt publish message ) message ) ; case subscribe : return encode subscribe message ( byte buf allocator , ( mqtt subscribe message ) message ) ; case unsubscribe : return encode unsubscribe message ( byte buf allocator , ( mqtt unsubscribe message ) message ) ; case suback : return encode sub ack message ( byte buf allocator , ( mqtt sub ack message ) message ) ; case unsuback : case puback : case pubrec : case pubrel : case pubcomp : return encode message with only single byte fixed header and message id ( byte buf allocator , message ) ; case pingreq : case pingresp : case disconnect : return encode message with only single byte fixed header ( byte buf allocator , message ) ; default : throw new illegal argument exception ( str + message . fixed header ( ) . message type ( ) . value ( ) ) ; } }	This is the main encoding method.It's only visible for testing.
private http2 settings decode settings ( channel handler context ctx , byte buf frame ) throws http2 exception { try { final http2 settings decoded settings = new http2 settings ( ) ; frame reader . read frame ( ctx , frame , new http2 frame adapter ( ) { @ override public void on settings read ( channel handler context ctx , http2 settings settings ) { decoded settings . copy from ( settings ) ; } } ) ; return decoded settings ; } finally { frame . release ( ) ; } }	Decodes the settings frame and returns the settings.
private static byte buf create settings frame ( channel handler context ctx , byte buf payload ) { byte buf frame = ctx . alloc ( ) . buffer ( frame header length + payload . readable bytes ( ) ) ; write frame header ( frame , payload . readable bytes ( ) , settings , new http2 flags ( ) , num ) ; frame . write bytes ( payload ) ; payload . release ( ) ; return frame ; }	Creates an HTTP2-Settings header with the given payload.
@ override public void close stream local ( http2 stream stream , channel future future ) { switch ( stream . state ( ) ) { case half closed local : case open : stream . close local side ( ) ; break ; default : close stream ( stream , future ) ; break ; } }	Closes the local side of the given stream.
@ override public void close stream remote ( http2 stream stream , channel future future ) { switch ( stream . state ( ) ) { case half closed remote : case open : stream . close remote side ( ) ; break ; default : close stream ( stream , future ) ; break ; } }	Closes the remote side of the given stream.
protected void on connection error ( channel handler context ctx , boolean outbound , throwable cause , http2 exception http2 ex ) { if ( http2 ex == null ) { http2 ex = new http2 exception ( internal error , cause . get message ( ) , cause ) ; } channel promise promise = ctx . new promise ( ) ; channel future future = go away ( ctx , http2 ex , ctx . new promise ( ) ) ; if ( http2 ex . shutdown hint ( ) == http2 exception . shutdown hint . graceful shutdown ) { do graceful shutdown ( ctx , future , promise ) ; } else { future . add listener ( new closing channel future listener ( ctx , promise ) ) ; } }	Handler for a connection error.
protected void handle server header decode size error ( channel handler context ctx , http2 stream stream ) { encoder ( ) . write headers ( ctx , stream . id ( ) , headers too large headers , num , bool , ctx . new promise ( ) ) ; }	Notifies client that this server has received headers that are larger than what it iswilling to accept.
private void check close connection ( channel future future ) {	Closes the connection if the graceful shutdown process has completed.
private char sequence get settings header value ( channel handler context ctx ) { byte buf buf = null ; byte buf encoded buf = null ; try {	Converts the current settings for the handler to the Base64-encoded representation used inthe HTTP2-Settings upgrade header.
public static charset get charset ( http message message , charset default charset ) { char sequence content type value = message . headers ( ) . get ( http header names . content type ) ; if ( content type value != null ) { return get charset ( content type value , default charset ) ; } else { return default charset ; } }	Fetch charset from message's Content-Type header.
public static char sequence get charset as sequence ( http message message ) { char sequence content type value = message . headers ( ) . get ( http header names . content type ) ; if ( content type value != null ) { return get charset as sequence ( content type value ) ; } else { return null ; } }	Fetch charset from message's Content-Type header as a char sequence.A lot of sites/possibly clients have charset="CHARSET", for example charset="utf-8".
public static char sequence get charset as sequence ( char sequence content type value ) { if ( content type value == null ) { throw new null pointer exception ( str ) ; } int index of charset = ascii string . index of ignore case ascii ( content type value , charset equals , num ) ; if ( index of charset == ascii string . index not found ) { return null ; } int index of encoding = index of charset + charset equals . length ( ) ; if ( index of encoding < content type value . length ( ) ) { char sequence charset candidate = content type value . sub sequence ( index of encoding , content type value . length ( ) ) ; int index of semicolon = ascii string . index of ignore case ascii ( charset candidate , semicolon , num ) ; if ( index of semicolon == ascii string . index not found ) { return charset candidate ; } return charset candidate . sub sequence ( num , index of semicolon ) ; } return null ; }	Fetch charset from Content-Type header value as a char sequence.A lot of sites/possibly clients have charset="CHARSET", for example charset="utf-8".
public static string format hostname for http ( inet socket address addr ) { string host string = net util . get hostname ( addr ) ; if ( net util . is valid ip v6 address ( host string ) ) { if ( ! addr . is unresolved ( ) ) { host string = net util . to address string ( addr . get address ( ) ) ; } return str + host string + str ; } return host string ; }	Formats the host string of an address so it can be used for computing an HTTP componentsuch as an URL or a Host header.
private static void generate huffman code lengths ( final int alphabet size , final int [ ] symbol frequencies , final int [ ] code lengths ) { final int [ ] merged frequencies and indices = new int [ alphabet size ] ; final int [ ] sorted frequencies = new int [ alphabet size ] ;	Generate a Huffman code length table for a given list of symbol frequencies.
private void assign huffman code symbols ( ) { final int [ ] [ ] huffman merged code symbols = this . huffman merged code symbols ; final int [ ] [ ] huffman code lengths = this . huffman code lengths ; final int mtf alphabet size = this . mtf alphabet size ; final int total tables = huffman code lengths . length ; for ( int i = num ; i < total tables ; i ++ ) { final int [ ] table lengths = huffman code lengths [ i ] ; int minimum length = num ; int maximum length = num ; for ( int j = num ; j < mtf alphabet size ; j ++ ) { final int length = table lengths [ j ] ; if ( length > maximum length ) { maximum length = length ; } if ( length < minimum length ) { minimum length = length ; } } int code = num ; for ( int j = minimum length ; j <= maximum length ; j ++ ) { for ( int k = num ; k < mtf alphabet size ; k ++ ) { if ( ( huffman code lengths [ i ] [ k ] & num ) == j ) { huffman merged code symbols [ i ] [ k ] = ( j << num ) | code ; code ++ ; } } code <<= num ; } } }	Assigns Canonical Huffman codes based on the calculated lengths.
private void write selectors and huffman tables ( byte buf out ) { final bzip2 bit writer writer = this . writer ; final byte [ ] selectors = this . selectors ; final int total selectors = selectors . length ; final int [ ] [ ] huffman code lengths = this . huffman code lengths ; final int total tables = huffman code lengths . length ; final int mtf alphabet size = this . mtf alphabet size ; writer . write bits ( out , num , total tables ) ; writer . write bits ( out , num , total selectors ) ;	Write out the selector list and Huffman tables.
private void write block data ( byte buf out ) { final bzip2 bit writer writer = this . writer ; final int [ ] [ ] huffman merged code symbols = this . huffman merged code symbols ; final byte [ ] selectors = this . selectors ; final char [ ] mtf = mtf block ; final int mtf length = this . mtf length ; int selector index = num ; for ( int mtf index = num ; mtf index < mtf length ; ) { final int group end = math . min ( mtf index + huffman group run length , mtf length ) - num ; final int [ ] table merged code symbols = huffman merged code symbols [ selectors [ selector index ++ ] ] ; while ( mtf index <= group end ) { final int merged code symbol = table merged code symbols [ mtf [ mtf index ++ ] ] ; writer . write bits ( out , merged code symbol > > > num , merged code symbol ) ; } } }	Writes out the encoded block data.
void encode ( byte buf out ) {	Encodes and writes the block data.
public void resume transfer ( ) { final channel handler context ctx = this . ctx ; if ( ctx == null ) { return ; } if ( ctx . executor ( ) . in event loop ( ) ) { resume transfer0 ( ctx ) ; } else {	Continues to fetch the chunks from the input.
private void initialise inverse bwt ( ) { final int bwt start pointer = this . bwt start pointer ; final byte [ ] bwt block = this . bwt block ; final int [ ] bwt merged pointers = new int [ bwt block length ] ; final int [ ] character base = new int [ num ] ; if ( bwt start pointer < num || bwt start pointer >= bwt block length ) { throw new decompression exception ( str ) ; }	Set up the Inverse Burrows-Wheeler Transform merged pointer array.
public int read ( ) { while ( rle repeat < num ) { if ( bwt bytes decoded == bwt block length ) { return - num ; } int next byte = decode next bwt ( ) ; if ( next byte != rle last decoded byte ) {	Decodes a byte from the final Run-Length Encoding stage, pulling a new byte from theBurrows-Wheeler Transform stage when required.
private int decode next bwt ( ) { int merged pointer = bwt current merged pointer ; int next decoded byte = merged pointer & num ; bwt current merged pointer = bwt merged pointers [ merged pointer > > > num ] ; if ( block randomised ) { if ( -- random count == num ) { next decoded byte ^= num ; random index = ( random index + num ) % num ; random count = bzip2 rand . r nums ( random index ) ; } } bwt bytes decoded ++ ; return next decoded byte ; }	Decodes a byte from the Burrows-Wheeler Transform stage.
public void encode ( byte buf out , char sequence data ) { object util . check not null ( out , str ) ; if ( data instanceof ascii string ) { ascii string string = ( ascii string ) data ; try { encode processor . out = out ; string . for each byte ( encode processor ) ; } catch ( exception e ) { platform dependent . throw exception ( e ) ; } finally { encode processor . end ( ) ; } } else { encode slow path ( out , data ) ; } }	Compresses the input string literal using the Huffman coding.
int get encoded length ( char sequence data ) { if ( data instanceof ascii string ) { ascii string string = ( ascii string ) data ; try { encoded length processor . reset ( ) ; string . for each byte ( encoded length processor ) ; return encoded length processor . length ( ) ; } catch ( exception e ) { platform dependent . throw exception ( e ) ; return - num ; } } else { return get encoded length slow path ( data ) ; } }	Returns the number of bytes required to Huffman encode the input string literal.
static boolean patch shaded library id ( input stream in , output stream out , string original name , string name ) throws io { byte [ ] buffer = new byte [ num ] ; int length ;	Package-private for testing.
private static boolean patch shaded library id ( byte [ ] bytes , string original name , string name ) {	Try to patch shaded library to ensure it uses a unique ID.
private m invalid message ( exception cause ) { state = state . bad message ; m message = build invalid message ( ) ; message . set decoder result ( decoder result . failure ( cause ) ) ; return message ; }	Helper method to create a message indicating a invalid decoding result.
private memcache content invalid chunk ( exception cause ) { state = state . bad message ; memcache content chunk = new default last memcache content ( unpooled . empty buffer ) ; chunk . set decoder result ( decoder result . failure ( cause ) ) ; return chunk ; }	Helper method to create a content chunk indicating a invalid decoding result.
private t retain0 ( t instance , final int increment , final int raw increment ) { int old ref = updater ( ) . get and add ( instance , raw increment ) ; if ( old ref != num && old ref != num && ( old ref & num ) != num ) { throw new illegal reference count exception ( num , increment ) ; }	rawIncrement == increment << 1.
@ override public final void channel read complete ( channel handler context ctx ) throws exception { try { on channel read complete ( ctx ) ; } finally { parent read in progress = bool ; tail = head = null ;	Notifies any child streams of the read completion.
public static byte [ ] best available mac ( ) {	Obtains the best MAC address found on local network interfaces.Generally speaking, an active network interface used on publicnetworks is better than a local network interface.
protected b max reserved streams ( int max reserved streams ) { enforce constraint ( str , str , connection ) ; enforce constraint ( str , str , decoder ) ; enforce constraint ( str , str , encoder ) ; this . max reserved streams = check positive or zero ( max reserved streams , str ) ; return self ( ) ; }	Set the maximum number of streams which can be in the reserved state at any given time.
private static boolean is upgrade request ( http object msg ) { return msg instanceof http request && ( ( http request ) msg ) . headers ( ) . get ( http header names . upgrade ) != null ; }	Determines whether or not the message is an HTTP upgrade request.
private static full http response create upgrade response ( char sequence upgrade protocol ) { default full http response res = new default full http response ( http 1 1 , switching protocols , unpooled . empty buffer , bool ) ; res . headers ( ) . add ( http header names . connection , http header values . upgrade ) ; res . headers ( ) . add ( http header names . upgrade , upgrade protocol ) ; return res ; }	Creates the 101 Switching Protocols response message.
private static list < char sequence > split header ( char sequence header ) { final string builder builder = new string builder ( header . length ( ) ) ; final list < char sequence > protocols = new array list < char sequence > ( num ) ; for ( int i = num ; i < header . length ( ) ; ++ i ) { char c = header . char at ( i ) ; if ( character . is whitespace ( c ) ) {	Splits a comma-separated header value.
void modify ( abstract epoll channel ch ) throws io { assert in event loop ( ) ; native . epoll ctl mod ( epoll fd . int value ( ) , ch . socket . int value ( ) , ch . flags ) ; }	The flags of the given epoll was modified so update the registration.
void handle loop exception ( throwable t ) { logger . warn ( str , t ) ;	Visible only for testing!.
public static string decode name ( byte buf in ) { int position = - num ; int checked = num ; final int end = in . writer index ( ) ; final int readable = in . readable bytes ( ) ;	Retrieves a domain name given a buffer containing a DNS packet.
void create global traffic counter ( scheduled executor service executor ) { if ( executor == null ) { throw new null pointer exception ( str ) ; } traffic counter tc = new traffic counter ( this , executor , str , check interval ) ; set traffic counter ( tc ) ; tc . start ( ) ; }	Create the global TrafficCounter.
@ override public void write ( channel handler context ctx , object msg , channel promise promise ) { string command = ( string ) msg ; if ( command . starts with ( str ) ) { string key string = command . substring ( str . length ( ) ) ; byte buf key = unpooled . wrapped buffer ( key string . get bytes ( charset util . utf 8 ) ) ; binary memcache request req = new default binary memcache request ( key ) ; req . set opcode ( binary memcache opcodes . get ) ; ctx . write ( req , promise ) ; } else if ( command . starts with ( str ) ) { string [ ] parts = command . split ( str , num ) ; if ( parts . length < num ) { throw new illegal argument exception ( str + command ) ; } string key string = parts [ num ] ; string value = parts [ num ] ; byte buf key = unpooled . wrapped buffer ( key string . get bytes ( charset util . utf 8 ) ) ; byte buf content = unpooled . wrapped buffer ( value . get bytes ( charset util . utf 8 ) ) ; byte buf extras = ctx . alloc ( ) . buffer ( num ) ; extras . write zero ( num ) ; binary memcache request req = new default full binary memcache request ( key , extras , content ) ; req . set opcode ( binary memcache opcodes . set ) ; ctx . write ( req , promise ) ; } else { throw new illegal state exception ( str + msg ) ; } }	Transforms basic string requests to binary memcache requests.
public string get ( char sequence name , string default value ) { string value = get ( name ) ; if ( value == null ) { return default value ; } return value ; }	Returns the value of a header with the specified name.
public http headers add ( char sequence name , object value ) { return add ( name . to string ( ) , value ) ; }	Adds a new header with the specified name and value.If the specified value is not a {.
public http headers add ( char sequence name , iterable < ? > values ) { return add ( name . to string ( ) , values ) ; }	Adds a new header with the specified name and values.This getMethod can be represented approximately as the following code: for (Object v: values) {if (v == null) {break;}headers.add(name, v);}.
public http headers set ( char sequence name , object value ) { return set ( name . to string ( ) , value ) ; }	Sets a header with the specified name and value.If there is an existing header with the same name, it is removed.If the specified value is not a {.
public http headers set ( char sequence name , iterable < ? > values ) { return set ( name . to string ( ) , values ) ; }	Sets a header with the specified name and values.If there is an existing header with the same name, it is removed.This getMethod can be represented approximately as the following code: headers.remove(name);for (Object v: values) {if (v == null) {break;}headers.add(name, v);}.
public default headers < k , v , t > copy ( ) { default headers < k , v , t > copy = new default headers < k , v , t > ( hashing strategy , value converter , name validator , entries . length ) ; copy . add impl ( this ) ; return copy ; }	Returns a deep copy of this instance.
protected void read timed out ( channel handler context ctx ) throws exception { if ( ! closed ) { ctx . fire exception caught ( read timeout exception . instance ) ; ctx . close ( ) ; closed = bool ; } }	Is called when a read timeout was detected.
public string raw query ( ) { int start = path end idx ( ) + num ; return start < uri . length ( ) ? uri . substring ( start ) : empty string ; }	Returns raw query string of the URI.
static int get unsigned short ( byte buf buf , int offset ) { return ( buf . get byte ( offset ) & num ) << num | buf . get byte ( offset + num ) & num ; }	Reads a big-endian unsigned short integer from the buffer.
static int get unsigned medium ( byte buf buf , int offset ) { return ( buf . get byte ( offset ) & num ) << num | ( buf . get byte ( offset + num ) & num ) << num | buf . get byte ( offset + num ) & num ; }	Reads a big-endian unsigned medium integer from the buffer.
static int get signed int ( byte buf buf , int offset ) { return ( buf . get byte ( offset ) & num ) << num | ( buf . get byte ( offset + num ) & num ) << num | ( buf . get byte ( offset + num ) & num ) << num | buf . get byte ( offset + num ) & num ; }	Reads a big-endian signed integer from the buffer.
static void validate header name ( char sequence name ) { if ( name == null ) { throw new null pointer exception ( str ) ; } if ( name . length ( ) == num ) { throw new illegal argument exception ( str ) ; }	Validate a SPDY header name.
static void validate header value ( char sequence value ) { if ( value == null ) { throw new null pointer exception ( str ) ; } for ( int i = num ; i < value . length ( ) ; i ++ ) { char c = value . char at ( i ) ; if ( c == num ) { throw new illegal argument exception ( str + value ) ; } } }	Validate a SPDY header value.
void free ( boolean finalizer ) {	Should be called if the Thread that uses this cache is about to exist to release resources out of the cache.
protected void cancel scheduled tasks ( ) { assert in event loop ( ) ; priority queue < scheduled future task < ? > > scheduled task queue = this . scheduled task queue ; if ( is null or empty ( scheduled task queue ) ) { return ; } final scheduled future task < ? > [ ] scheduled tasks = scheduled task queue . to array ( new scheduled future task < ? > [ num ] ) ; for ( scheduled future task < ? > task : scheduled tasks ) { task . cancel without remove ( bool ) ; } scheduled task queue . clear ignoring indexes ( ) ; }	Cancel all scheduled tasks.This method MUST be called only when {.
public static string int to ip address ( int i ) { string builder buf = new string builder ( num ) ; buf . append ( i > > num & num ) ; buf . append ( str ) ; buf . append ( i > > num & num ) ; buf . append ( str ) ; buf . append ( i > > num & num ) ; buf . append ( str ) ; buf . append ( i & num ) ; return buf . to string ( ) ; }	Converts a 32-bit integer into an IPv4 address.
public static string bytes to ip address ( byte [ ] bytes , int offset , int length ) { switch ( length ) { case num : { return new string builder ( num ) . append ( bytes [ offset ] & num ) . append ( str ) . append ( bytes [ offset + num ] & num ) . append ( str ) . append ( bytes [ offset + num ] & num ) . append ( str ) . append ( bytes [ offset + num ] & num ) . to string ( ) ; } case num : return to address string ( bytes , offset , bool ) ; default : throw new illegal argument exception ( str + length + str ) ; } }	Converts 4-byte or 16-byte data into an IPv4 or IPv6 string respectively.
private void destroy ( ) { lock writer lock = ctx lock . write lock ( ) ; writer lock . lock ( ) ; try { if ( ctx != num ) { if ( enable ocsp ) { ssl . disable ocsp ( ctx ) ; } ssl . free ( ctx ) ; ctx = num ; open ssl session context context = session context ( ) ; if ( context != null ) { context . destroy ( ) ; } } } finally { writer lock . unlock ( ) ; } }	producing a segfault.
public string application protocol ( ) { ssl engine = engine ( ) ; if ( ! ( engine instanceof application protocol accessor ) ) { return null ; } return ( ( application protocol accessor ) engine ) . get negotiated application protocol ( ) ; }	Returns the name of the current application-level protocol.
private void set handshake success ( ) { handshake promise . try success ( ctx . channel ( ) ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( str , ctx . channel ( ) , engine . get session ( ) . get cipher suite ( ) ) ; } ctx . fire user event triggered ( ssl handshake completion event . success ) ; if ( read during handshake && ! ctx . channel ( ) . config ( ) . is auto read ( ) ) { read during handshake = bool ; ctx . read ( ) ; } }	Notify all the handshake futures about the successfully handshake.
@ override public http post standard request decoder offer ( http content content ) { check destroyed ( ) ;	Initialized the internals from a new chunk.
@ override public boolean has next ( ) { check destroyed ( ) ; if ( current status == multi part status . epilogue ) {	True if at current getStatus, there is an available decodedInterfaceHttpData from the Body.This getMethod works for chunked and not chunked request.
protected void add http data ( interface http data data ) { if ( data == null ) { return ; } list < interface http data > datas = body map http data . get ( data . get name ( ) ) ; if ( datas == null ) { datas = new array list < interface http data > ( num ) ; body map http data . put ( data . get name ( ) , datas ) ; } datas . add ( data ) ; body list http data . add ( data ) ; }	Utility function to add a new decoded data.
static int find non whitespace ( string sb , int offset ) { int result ; for ( result = offset ; result < sb . length ( ) ; result ++ ) { if ( ! character . is whitespace ( sb . char at ( result ) ) ) { break ; } } return result ; }	Find the first non whitespace.
static int find end of string ( string sb ) { int result ; for ( result = sb . length ( ) ; result > num ; result -- ) { if ( ! character . is whitespace ( sb . char at ( result - num ) ) ) { break ; } } return result ; }	Find the end of String.
public void start ( ) { switch ( worker state updater . get ( this ) ) { case worker state init : if ( worker state updater . compare and set ( this , worker state init , worker state started ) ) { worker thread . start ( ) ; } break ; case worker state started : break ; case worker state shutdown : throw new illegal state exception ( str ) ; default : throw new error ( str ) ; }	Starts the background thread explicitly.
private static string read string ( string field name , byte buf in ) { int length = in . bytes before ( max field length + num , ( byte ) num ) ; if ( length < num ) { throw new decoder exception ( str + field name + str + max field length + str ) ; } string value = in . read slice ( length ) . to string ( charset util . us ascii ) ; in . skip bytes ( num ) ;	Reads a variable-length NUL-terminated string as defined in SOCKS4.
long allocate ( ) { if ( elem size == num ) { return to handle ( num ) ; } if ( num avail == num || ! do not destroy ) { return - num ; } final int bitmap idx = get next avail ( ) ; int q = bitmap idx > > > num ; int r = bitmap idx & num ; assert ( bitmap [ q ] > > > r & num ) == num ; bitmap [ q ] |= num << r ; if ( -- num avail == num ) { remove from pool ( ) ; } return to handle ( bitmap idx ) ; }	Returns the bitmap index of the subpage allocation.
private static int compression level ( int block size ) { if ( block size < min block size || block size > max block size ) { throw new illegal argument exception ( string . format ( str , block size , min block size , max block size ) ) ; } int compression level = num - integer . number of leading zeros ( block size - num ) ;	Calculates compression level on the basis of block size.
protected void write timed out ( channel handler context ctx ) throws exception { if ( ! closed ) { ctx . fire exception caught ( write timeout exception . instance ) ; ctx . close ( ) ; closed = bool ; } }	Is called when a write timeout was detected.
protected boolean do connect ( socket address remote address , socket address local address ) throws exception { if ( local address instanceof inet socket address ) { check resolvable ( ( inet socket address ) local address ) ; } inet socket address remote socket addr = remote address instanceof inet socket address ? ( inet socket address ) remote address : null ; if ( remote socket addr != null ) { check resolvable ( remote socket addr ) ; } if ( remote != null ) {	Connect to the remote peer.
static string base64 ( byte [ ] data ) { byte buf encoded data = unpooled . wrapped buffer ( data ) ; byte buf encoded = base64 . encode ( encoded data ) ; string encoded string = encoded . to string ( charset util . utf 8 ) ; encoded . release ( ) ; return encoded string ; }	Performs base64 encoding on the specified data.
static int random number ( int minimum , int maximum ) { assert minimum < maximum ; double fraction = platform dependent . thread local random ( ) . next double ( ) ;	Generates a pseudo-random number.
protected final void remove message ( http2 stream stream , boolean release ) { full http message msg = stream . remove property ( message key ) ; if ( release && msg != null ) { msg . release ( ) ; } }	The stream is out of scope for the HTTP message flow and will no longer be tracked.
protected void fire channel read ( channel handler context ctx , full http message msg , boolean release , http2 stream stream ) { remove message ( stream , release ) ; http util . set content length ( msg , msg . content ( ) . readable bytes ( ) ) ; ctx . fire channel read ( msg ) ; }	Set final headers and fire a channel read event.
public dns name resolver builder search domains ( iterable < string > search domains ) { check not null ( search domains , str ) ; final list < string > list = new array list < string > ( num ) ; for ( string f : search domains ) { if ( f == null ) { break ; }	Set the list of search domains of the resolver.
static internal logger wrap logger ( logger logger ) { return logger instanceof location aware logger ? new j ( ( location aware logger ) logger ) : new j ( logger ) ; }	package-private for testing.
private future < channel > acquire healthy from pool or new ( final promise < channel > promise ) { try { final channel ch = poll channel ( ) ; if ( ch == null ) {	Tries to retrieve healthy channel from the pool if any or creates a new channel otherwise.
private void release and offer if healthy ( channel channel , promise < void > promise , future < boolean > future ) throws exception { if ( future . get now ( ) ) {	Adds the channel back to the pool only if the channel is healthy.
public void copy ( int src idx , byte [ ] dst , int dst idx , int length ) { if ( is out of bounds ( src idx , length , length ( ) ) ) { throw new index out of bounds exception ( str + str + src idx + str + length + str + length ( ) + str ) ; } system . arraycopy ( value , src idx + offset , check not null ( dst , str ) , dst idx , length ) ; }	Copies the content of this string to a byte array.
public ascii string concat ( char sequence string ) { int this len = length ( ) ; int that len = string . length ( ) ; if ( that len == num ) { return this ; } if ( string . get class ( ) == ascii string . class ) { ascii string that = ( ascii string ) string ; if ( is empty ( ) ) { return that ; } byte [ ] new value = platform dependent . allocate uninitialized array ( this len + that len ) ; system . arraycopy ( value , array offset ( ) , new value , num , this len ) ; system . arraycopy ( that . value , that . array offset ( ) , new value , this len , that len ) ; return new ascii string ( new value , bool ) ; } if ( is empty ( ) ) { return new ascii string ( string ) ; } byte [ ] new value = platform dependent . allocate uninitialized array ( this len + that len ) ; system . arraycopy ( value , array offset ( ) , new value , num , this len ) ; for ( int i = this len , j = num ; i < new value . length ; i ++ , j ++ ) { new value [ i ] = c2b ( string . char at ( j ) ) ; } return new ascii string ( new value , bool ) ; }	Concatenates this string and the specified string.
public boolean ends with ( char sequence suffix ) { int suffix len = suffix . length ( ) ; return region matches ( length ( ) - suffix len , suffix , num , suffix len ) ; }	Compares the specified string to this string to determine if the specified string is a suffix.
public boolean content equals ignore case ( char sequence string ) { if ( string == null || string . length ( ) != length ( ) ) { return bool ; } if ( string . get class ( ) == ascii string . class ) { ascii string rhs = ( ascii string ) string ; for ( int i = array offset ( ) , j = rhs . array offset ( ) ; i < length ( ) ; ++ i , ++ j ) { if ( ! equals ignore case ( value [ i ] , rhs . value [ j ] ) ) { return bool ; } } return bool ; } for ( int i = array offset ( ) , j = num ; i < length ( ) ; ++ i , ++ j ) { if ( ! equals ignore case ( b2c ( value [ i ] ) , string . char at ( j ) ) ) { return bool ; } } return bool ; }	Compares the specified string to this string ignoring the case of the characters and returns true if they areequal.
public char [ ] to char array ( int start , int end ) { int length = end - start ; if ( length == num ) { return empty arrays . empty chars ; } if ( is out of bounds ( start , length , length ( ) ) ) { throw new index out of bounds exception ( str + str + start + str + length + str + length ( ) + str ) ; } final char [ ] buffer = new char [ length ] ; for ( int i = num , j = start + array offset ( ) ; i < length ; i ++ , j ++ ) { buffer [ i ] = b2c ( value [ j ] ) ; } return buffer ; }	Copies the characters in this string to a character array.
public void copy ( int src idx , char [ ] dst , int dst idx , int length ) { if ( dst == null ) { throw new null pointer exception ( str ) ; } if ( is out of bounds ( src idx , length , length ( ) ) ) { throw new index out of bounds exception ( str + str + src idx + str + length + str + length ( ) + str ) ; } final int dst end = dst idx + length ; for ( int i = dst idx , j = src idx + array offset ( ) ; i < dst end ; i ++ , j ++ ) { dst [ i ] = b2c ( value [ j ] ) ; } }	Copied the content of this string to a character array.
public ascii string sub sequence ( int start , int end , boolean copy ) { if ( is out of bounds ( start , end - start , length ( ) ) ) { throw new index out of bounds exception ( str + start + str + end + str + length ( ) + str ) ; } if ( start == num && end == length ( ) ) { return this ; } if ( end == start ) { return empty string ; } return new ascii string ( value , start + offset , end - start , copy ) ; }	Either copy or share a subset of underlying sub-sequence of bytes.
public int index of ( char sequence sub string , int start ) { final int sub count = sub string . length ( ) ; if ( start < num ) { start = num ; } if ( sub count <= num ) { return start < length ? start : length ; } if ( sub count > length - start ) { return index not found ; } final char first char = sub string . char at ( num ) ; if ( first char > max char value ) { return index not found ; } final byte first char as byte = c2b0 ( first char ) ; final int len = offset + length - sub count ; for ( int i = start + offset ; i <= len ; ++ i ) { if ( value [ i ] == first char as byte ) { int o1 = i , o2 = num ; while ( ++ o2 < sub count && b2c ( value [ ++ o1 ] ) == sub string . char at ( o2 ) ) {	Searches in this string for the index of the specified string.
public boolean region matches ( int this start , char sequence string , int start , int length ) { if ( string == null ) { throw new null pointer exception ( str ) ; } if ( start < num || string . length ( ) - start < length ) { return bool ; } final int this len = length ( ) ; if ( this start < num || this len - this start < length ) { return bool ; } if ( length <= num ) { return bool ; } final int that end = start + length ; for ( int i = start , j = this start + array offset ( ) ; i < that end ; i ++ , j ++ ) { if ( b2c ( value [ j ] ) != string . char at ( i ) ) { return bool ; } } return bool ; }	Compares the specified string to this string and compares the specified range of characters to determine if theyare the same.
public boolean region matches ( boolean ignore case , int this start , char sequence string , int start , int length ) { if ( ! ignore case ) { return region matches ( this start , string , start , length ) ; } if ( string == null ) { throw new null pointer exception ( str ) ; } final int this len = length ( ) ; if ( this start < num || length > this len - this start ) { return bool ; } if ( start < num || length > string . length ( ) - start ) { return bool ; } this start += array offset ( ) ; final int this end = this start + length ; while ( this start < this end ) { if ( ! equals ignore case ( b2c ( value [ this start ++ ] ) , string . char at ( start ++ ) ) ) { return bool ; } } return bool ; }	Compares the specified string to this string and compares the specified range of characters to determine if theyare the same.
public ascii string replace ( char old char , char new char ) { if ( old char > max char value ) { return this ; } final byte old char as byte = c2b0 ( old char ) ; final byte new char as byte = c2b ( new char ) ; final int len = offset + length ; for ( int i = offset ; i < len ; ++ i ) { if ( value [ i ] == old char as byte ) { byte [ ] buffer = platform dependent . allocate uninitialized array ( length ( ) ) ; system . arraycopy ( value , offset , buffer , num , i - offset ) ; buffer [ i - offset ] = new char as byte ; ++ i ; for ( ; i < len ; ++ i ) { byte old value = value [ i ] ; buffer [ i - offset ] = old value != old char as byte ? old value : new char as byte ; } return new ascii string ( buffer , bool ) ; } } return this ; }	Copies this string replacing occurrences of the specified character with another character.
public ascii string to lower case ( ) { boolean lowercased = bool ; int i , j ; final int len = length ( ) + array offset ( ) ; for ( i = array offset ( ) ; i < len ; ++ i ) { byte b = value [ i ] ; if ( b >= str && b <= str ) { lowercased = bool ; break ; } }	Converts the characters in this string to lowercase, using the default Locale.
public ascii string to upper case ( ) { boolean uppercased = bool ; int i , j ; final int len = length ( ) + array offset ( ) ; for ( i = array offset ( ) ; i < len ; ++ i ) { byte b = value [ i ] ; if ( b >= str && b <= str ) { uppercased = bool ; break ; } }	Converts the characters in this string to uppercase, using the default Locale.
public static char sequence trim ( char sequence c ) { if ( c . get class ( ) == ascii string . class ) { return ( ( ascii string ) c ) . trim ( ) ; } if ( c instanceof string ) { return ( ( string ) c ) . trim ( ) ; } int start = num , last = c . length ( ) - num ; int end = last ; while ( start <= end && c . char at ( start ) <= str ) { start ++ ; } while ( end >= start && c . char at ( end ) <= str ) { end -- ; } if ( start == num && end == last ) { return c ; } return c . sub sequence ( start , end ) ; }	Copies this string removing white space characters from the beginning and end of the string, and tries not tocopy if possible.
public ascii string trim ( ) { int start = array offset ( ) , last = array offset ( ) + length ( ) - num ; int end = last ; while ( start <= end && value [ start ] <= str ) { start ++ ; } while ( end >= start && value [ end ] <= str ) { end -- ; } if ( start == num && end == last ) { return this ; } return new ascii string ( value , start , end - start + num , bool ) ; }	Duplicates this string removing white space characters from the beginning and end of thestring, without copying.
public static boolean region matches ( final char sequence cs , final boolean ignore case , final int cs start , final char sequence string , final int start , final int length ) { if ( cs == null || string == null ) { return bool ; } if ( cs instanceof string && string instanceof string ) { return ( ( string ) cs ) . region matches ( ignore case , cs start , ( string ) string , start , length ) ; } if ( cs instanceof ascii string ) { return ( ( ascii string ) cs ) . region matches ( ignore case , cs start , string , start , length ) ; } return region matches char sequences ( cs , cs start , string , start , length , ignore case ? general case insensitive char equality comparator . instance : default char equality comparator . instance ) ; }	This methods make regionMatches operation correctly for any chars in strings.
@ deprecated public static < t > t release later ( t msg , int decrement ) { if ( msg instanceof reference counted ) { thread death watcher . watch ( thread . current thread ( ) , new releasing task ( ( reference counted ) msg , decrement ) ) ; } return msg ; }	Schedules the specified object to be released when the caller thread terminates.
public void window update ratio ( float ratio ) { assert ctx == null || ctx . executor ( ) . in event loop ( ) ; check valid ratio ( ratio ) ; window update ratio = ratio ; }	The window update ratio is used to determine when a window update must be sent.
void release read suspended ( channel handler context ctx ) { channel channel = ctx . channel ( ) ; channel . attr ( read suspended ) . set ( bool ) ; channel . config ( ) . set auto read ( bool ) ; }	Release the Read suspension.
void check write suspend ( channel handler context ctx , long delay , long queue size ) { if ( queue size > max write size || delay > max write delay ) { set user defined writability ( ctx , bool ) ; } }	Check the writability according to delay and size for the channel.Set if necessary setUserDefinedWritability status.
static ha decode header ( string header ) { if ( header == null ) { throw new ha ( str ) ; } string [ ] parts = header . split ( str ) ; int num parts = parts . length ; if ( num parts < num ) { throw new ha ( str + header + str ) ; } if ( ! str . equals ( parts [ num ] ) ) { throw new ha ( str + parts [ num ] ) ; } ha prot and fam ; try { prot and fam = ha . value of ( parts [ num ] ) ; } catch ( illegal argument exception e ) { throw new ha ( e ) ; } if ( prot and fam != ha . tc && prot and fam != ha . tc && prot and fam != ha . unknown ) { throw new ha ( str + parts [ num ] ) ; } if ( prot and fam == ha . unknown ) { return unknown msg ; } if ( num parts != num ) { throw new ha ( str + header + str ) ; } return new ha ( ha . v1 , ha . proxy , prot and fam , parts [ num ] , parts [ num ] , parts [ num ] , parts [ num ] ) ; }	Decodes a version 1, human-readable proxy protocol header.
private static string ip bytes to string ( byte buf header , int address len ) { string builder sb = new string builder ( ) ; if ( address len == num ) { sb . append ( header . read byte ( ) & num ) ; sb . append ( str ) ; sb . append ( header . read byte ( ) & num ) ; sb . append ( str ) ; sb . append ( header . read byte ( ) & num ) ; sb . append ( str ) ; sb . append ( header . read byte ( ) & num ) ; } else { sb . append ( integer . to hex string ( header . read unsigned short ( ) ) ) ; sb . append ( str ) ; sb . append ( integer . to hex string ( header . read unsigned short ( ) ) ) ; sb . append ( str ) ; sb . append ( integer . to hex string ( header . read unsigned short ( ) ) ) ; sb . append ( str ) ; sb . append ( integer . to hex string ( header . read unsigned short ( ) ) ) ; sb . append ( str ) ; sb . append ( integer . to hex string ( header . read unsigned short ( ) ) ) ; sb . append ( str ) ; sb . append ( integer . to hex string ( header . read unsigned short ( ) ) ) ; sb . append ( str ) ; sb . append ( integer . to hex string ( header . read unsigned short ( ) ) ) ; sb . append ( str ) ; sb . append ( integer . to hex string ( header . read unsigned short ( ) ) ) ; } return sb . to string ( ) ; }	Convert ip address bytes to string representation.
private static int port string to int ( string value ) { int port ; try { port = integer . parse int ( value ) ; } catch ( number format exception e ) { throw new ha ( str + value , e ) ; } if ( port <= num || port > num ) { throw new ha ( str + value + str ) ; } return port ; }	Convert port to integer.
public static full http response to full http response ( int stream id , http2 headers http2 headers , byte buf allocator alloc , boolean validate http headers ) throws http2 exception { http response status status = parse status ( http2 headers . status ( ) ) ;	Create a new object to contain the response data.
public static full http request to full http request ( int stream id , http2 headers http2 headers , byte buf allocator alloc , boolean validate http headers ) throws http2 exception {	Create a new object to contain the request data.
public static http request to http request ( int stream id , http2 headers http2 headers , boolean validate http headers ) throws http2 exception {	Create a new object to contain the request data.
public static http response to http response ( final int stream id , final http2 headers http2 headers , final boolean validate http headers ) throws http2 exception { final http response status status = parse status ( http2 headers . status ( ) ) ;	Create a new object to contain the response data.
static void set http2 authority ( string authority , http2 headers out ) {	package-private for testing only.
static int major version ( final string java spec version ) { final string [ ] components = java spec version . split ( str ) ; final int [ ] version = new int [ components . length ] ; for ( int i = num ; i < components . length ; i ++ ) { version [ i ] = integer . parse int ( components [ i ] ) ; } if ( version [ num ] == num ) { assert version [ num ] >= num ; return version [ num ] ; } else { return version [ num ] ; } }	Package-private for testing only.
private static void set extended parent pointers ( final int [ ] array ) { final int length = array . length ; array [ num ] += array [ num ] ; for ( int head node = num , tail node = num , top node = num ; tail node < length - num ; tail node ++ ) { int temp ; if ( top node >= length || array [ head node ] < array [ top node ] ) { temp = array [ head node ] ; array [ head node ++ ] = tail node ; } else { temp = array [ top node ++ ] ; } if ( top node >= length || ( head node < tail node && array [ head node ] < array [ top node ] ) ) { temp += array [ head node ] ; array [ head node ++ ] = tail node + length ; } else { temp += array [ top node ++ ] ; } array [ tail node ] = temp ; } }	Fills the code array with extended parent pointers.
private static int find nodes to relocate ( final int [ ] array , final int maximum length ) { int current node = array . length - num ; for ( int current depth = num ; current depth < maximum length - num && current node > num ; current depth ++ ) { current node = first ( array , current node - num , num ) ; } return current node ; }	Finds the number of nodes to relocate in order to achieve a given code length limit.
private static void allocate node lengths ( final int [ ] array ) { int first node = array . length - num ; int next node = array . length - num ; for ( int current depth = num , available nodes = num ; available nodes > num ; current depth ++ ) { final int last node = first node ; first node = first ( array , last node - num , num ) ; for ( int i = available nodes - ( last node - first node ) ; i > num ; i -- ) { array [ next node -- ] = current depth ; } available nodes = ( last node - first node ) << num ; } }	A final allocation pass with no code length limit.
private static void allocate node lengths with relocation ( final int [ ] array , final int nodes to move , final int insert depth ) { int first node = array . length - num ; int next node = array . length - num ; int current depth = insert depth == num ? num : num ; int nodes left to move = insert depth == num ? nodes to move - num : nodes to move ; for ( int available nodes = current depth << num ; available nodes > num ; current depth ++ ) { final int last node = first node ; first node = first node <= nodes to move ? first node : first ( array , last node - num , nodes to move ) ; int offset = num ; if ( current depth >= insert depth ) { offset = math . min ( nodes left to move , num << ( current depth - insert depth ) ) ; } else if ( current depth == insert depth - num ) { offset = num ; if ( array [ first node ] == last node ) { first node ++ ; } } for ( int i = available nodes - ( last node - first node + offset ) ; i > num ; i -- ) { array [ next node -- ] = current depth ; } nodes left to move -= offset ; available nodes = ( last node - first node + offset ) << num ; } }	A final allocation pass that relocates nodes in order to achieve a maximum code length limit.
static void allocate huffman code lengths ( final int [ ] array , final int maximum length ) { switch ( array . length ) { case num : array [ num ] = num ;	Allocates Canonical Huffman code lengths in place based on a sorted frequency array.
public void close ( ) throws io { for ( ; ; ) { int state = this . state ; if ( is closed ( state ) ) { return ; }	Close the file descriptor.
protected final void init ( i inbound handler , o outbound handler ) { validate ( inbound handler , outbound handler ) ; this . inbound handler = inbound handler ; this . outbound handler = outbound handler ; }	Initialized this handler with the specified handlers.
public static boolean common suffix of length ( string s , string p , int len ) { return s != null && p != null && len >= num && s . region matches ( s . length ( ) - len , p , p . length ( ) - len , len ) ; }	Checks if two strings have the same suffix of specified length.
public static < t extends appendable > t byte to hex string ( t buf , int value ) { try { buf . append ( byte to hex string ( value ) ) ; } catch ( io e ) { platform dependent . throw exception ( e ) ; } return buf ; }	Converts the specified byte value into a hexadecimal integer and appends it to the specified buffer.
public static byte decode hex byte ( char sequence s , int pos ) { int hi = decode hex nibble ( s . char at ( pos ) ) ; int lo = decode hex nibble ( s . char at ( pos + num ) ) ; if ( hi == - num || lo == - num ) { throw new illegal argument exception ( string . format ( str , s . sub sequence ( pos , pos + num ) , pos , s ) ) ; } return ( byte ) ( ( hi << num ) + lo ) ; }	Decode a 2-digit hex byte from within a string.
protected final string exception message ( string msg ) { if ( msg == null ) { msg = str ; } string builder buf = new string builder ( num + msg . length ( ) ) . append ( protocol ( ) ) . append ( str ) . append ( auth scheme ( ) ) . append ( str ) . append ( proxy address ) . append ( str ) . append ( destination address ) ; if ( ! msg . is empty ( ) ) { buf . append ( str ) . append ( msg ) ; } return buf . to string ( ) ; }	Decorates the specified exception message with the common information such as the current protocol,authentication scheme, proxy address, and destination address.
static void close on flush ( channel ch ) { if ( ch . is active ( ) ) { ch . write and flush ( unpooled . empty buffer ) . add listener ( channel future listener . close ) ; } }	Closes the specified channel after all queued write requests are flushed.
public future < addressed envelope < dns response , inet socket address > > query ( dns question question , iterable < dns record > additionals ) { return query ( next name server address ( ) , question , additionals ) ; }	Sends a DNS query with the specified question with additional records.
public future < addressed envelope < dns response , inet socket address > > query ( inet socket address name server addr , dns question question ) { return query0 ( name server addr , question , empty additionals , bool , ch . new promise ( ) , ch . event loop ( ) . < addressed envelope < ? extends dns response , inet socket address > > new promise ( ) ) ; }	Sends a DNS query with the specified question using the specified name server list.
public future < addressed envelope < dns response , inet socket address > > query ( inet socket address name server addr , dns question question , iterable < dns record > additionals ) { return query0 ( name server addr , question , to array ( additionals , bool ) , bool , ch . new promise ( ) , ch . event loop ( ) . < addressed envelope < ? extends dns response , inet socket address > > new promise ( ) ) ; }	Sends a DNS query with the specified question with additional records using the specified name server list.
static int get index ( char sequence name ) { integer index = static index by name . get ( name ) ; if ( index == null ) { return - num ; } return index ; }	Returns the lowest index value for the given header field name in the static table.
static int get index ( char sequence name , char sequence value ) { int index = get index ( name ) ; if ( index == - num ) { return - num ; }	Returns the index value for the given header field in the static table.
private static char sequence map < integer > create map ( ) { int length = static table . size ( ) ; @ suppress warnings ( str ) char sequence map < integer > ret = new char sequence map < integer > ( bool , unsupported value converter . < integer > instance ( ) , length ) ;	create a map CharSequenceMap header name to index value to allow quick lookup.
protected void report untraced leak ( string resource type ) { logger . error ( str + str + str + str + str , resource type , prop level , level . advanced . name ( ) . to lower case ( ) , simple class name ( this ) ) ; }	This method is called when an untraced leak is detected.
public synchronized void stop ( ) { if ( ! monitor active ) { return ; } monitor active = bool ; reset accounting ( milli second from nano ( ) ) ; if ( traffic shaping handler != null ) { traffic shaping handler . do accounting ( this ) ; } if ( scheduled future != null ) { scheduled future . cancel ( bool ) ; } }	Stop the monitoring process.
synchronized void reset accounting ( long new last time ) { long interval = new last time - last time . get and set ( new last time ) ; if ( interval == num ) {	Reset the accounting on Read and Write.
public void configure ( long new check interval ) { long new interval = new check interval / num * num ; if ( check interval . get and set ( new interval ) != new interval ) { if ( new interval <= num ) { stop ( ) ;	Change checkInterval between two computations in millisecond.
private void update parents alloc ( int id ) { while ( id > num ) { int parent id = id > > > num ; byte val1 = value ( id ) ; byte val2 = value ( id ^ num ) ; byte val = val1 < val2 ? val1 : val2 ; set value ( parent id , val ) ; id = parent id ; } }	Update method used by allocateThis is triggered only when a successor is allocated and all its predecessorsneed to update their stateThe minimal depth at which subtree rooted at id has some free space.
private int allocate node ( int d ) { int id = num ; int initial = - ( num << d ) ;	Algorithm to allocate an index in memoryMap when we query for a free nodeat depth d.
void free ( long handle , byte buffer nio buffer ) { int memory map idx = memory map idx ( handle ) ; int bitmap idx = bitmap idx ( handle ) ; if ( bitmap idx != num ) {	Free a subpage or a run of pagesWhen a subpage is freed from PoolSubpage, it might be added back to subpage pool of the owning PoolArenaIf the subpage pool in PoolArena has at least one other PoolSubpage of given elemSize, we cancompletely free the owning Page so it is available for subsequent allocations.
private static int read raw varint32 ( byte buf buffer ) { if ( ! buffer . is readable ( ) ) { return num ; } buffer . mark reader index ( ) ; byte tmp = buffer . read byte ( ) ; if ( tmp >= num ) { return tmp ; } else { int result = tmp & num ; if ( ! buffer . is readable ( ) ) { buffer . reset reader index ( ) ; return num ; } if ( ( tmp = buffer . read byte ( ) ) >= num ) { result |= tmp << num ; } else { result |= ( tmp & num ) << num ; if ( ! buffer . is readable ( ) ) { buffer . reset reader index ( ) ; return num ; } if ( ( tmp = buffer . read byte ( ) ) >= num ) { result |= tmp << num ; } else { result |= ( tmp & num ) << num ; if ( ! buffer . is readable ( ) ) { buffer . reset reader index ( ) ; return num ; } if ( ( tmp = buffer . read byte ( ) ) >= num ) { result |= tmp << num ; } else { result |= ( tmp & num ) << num ; if ( ! buffer . is readable ( ) ) { buffer . reset reader index ( ) ; return num ; } result |= ( tmp = buffer . read byte ( ) ) << num ; if ( tmp < num ) { throw new corrupted frame exception ( str ) ; } } } } return result ; } }	Reads variable length 32bit int from buffer.
public static int splice ( int fd , long off in , int fd out , long off out , long len ) throws io { int res = splice0 ( fd , off in , fd out , off out , len ) ; if ( res >= num ) { return res ; } return io result ( str , res , splice connection reset exception , splice closed channel exception ) ; }	File-descriptor operations.
void recycle ( ) { for ( int i = num ; i < size ; i ++ ) { array [ i ] = null ; } size = num ; insert since recycled = bool ; recycler . recycle ( this ) ; }	Recycle the array which will clear it and null out all entries in the internal storage.
private static int content length ( object msg ) { if ( msg instanceof memcache content ) { return ( ( memcache content ) msg ) . content ( ) . readable bytes ( ) ; } if ( msg instanceof byte buf ) { return ( ( byte buf ) msg ) . readable bytes ( ) ; } if ( msg instanceof file region ) { return ( int ) ( ( file region ) msg ) . count ( ) ; } throw new illegal state exception ( str + string util . simple class name ( msg ) ) ; }	Determine the content length of the given object.
static string normalize hostname ( string hostname ) { if ( needs normalization ( hostname ) ) { hostname = idn . to ascii ( hostname , idn . allow unassigned ) ; } return hostname . to lower case ( locale . us ) ; }	IDNA ASCII conversion and case normalization.
public void set max header table size ( long max header table size ) throws http2 exception { if ( max header table size < min header table size || max header table size > max header table size ) { throw connection error ( protocol error , str , min header table size , max header table size , max header table size ) ; } max dynamic table size = max header table size ; if ( max dynamic table size < encoder max dynamic table size ) {	Set the maximum table size.
private static int max output buffer length ( int input length ) { double factor ; if ( input length < num ) { factor = num ; } else if ( input length < num ) { factor = num ; } else if ( input length < num ) { factor = num ; } else if ( input length < num ) { factor = num ; } else { factor = num ; } return num + ( int ) ( input length * factor ) ; }	Calculates maximum possible size of output buffer for not compressible data.
@ override public channel future close ( channel channel , close web socket frame frame , channel promise promise ) { return channel . write and flush ( frame , promise ) ; }	Echo back the closing frame.
public void set body http datas ( list < interface http data > datas ) throws error data encoder exception { if ( datas == null ) { throw new null pointer exception ( str ) ; } global body size = num ; body list datas . clear ( ) ; current file upload = null ; during mixed mode = bool ; multipart http datas . clear ( ) ; for ( interface http data data : datas ) { add body http data ( data ) ; } }	Set the Body HttpDatas list.
public void add body attribute ( string name , string value ) throws error data encoder exception { string svalue = value != null ? value : string util . empty string ; attribute data = factory . create attribute ( request , check not null ( name , str ) , svalue ) ; add body http data ( data ) ; }	Add a simple attribute in the body as Name=Value.
public void add body file uploads ( string name , file [ ] file , string [ ] content type , boolean [ ] is text ) throws error data encoder exception { if ( file . length != content type . length && file . length != is text . length ) { throw new illegal argument exception ( str ) ; } for ( int i = num ; i < file . length ; i ++ ) { add body file upload ( name , file [ i ] , content type [ i ] , is text [ i ] ) ; } }	Add a series of Files associated with one File parameter.
@ suppress warnings ( str ) private string encode attribute ( string s , charset charset ) throws error data encoder exception { if ( s == null ) { return str ; } try { string encoded = url . encode ( s , charset . name ( ) ) ; if ( encoder mode == encoder mode . rf ) { for ( map . entry < pattern , string > entry : percent encodings ) { string replacement = entry . get value ( ) ; encoded = entry . get key ( ) . matcher ( encoded ) . replace all ( replacement ) ; } } return encoded ; } catch ( unsupported encoding exception e ) { throw new error data encoder exception ( charset . name ( ) , e ) ; } }	Encode one attribute.
void create huffman decoding tables ( ) { final int alphabet size = this . alphabet size ; for ( int table = num ; table < table code lengths . length ; table ++ ) { final int [ ] table bases = code bases [ table ] ; final int [ ] table limits = code limits [ table ] ; final int [ ] table symbols = code symbols [ table ] ; final byte [ ] code lengths = table code lengths [ table ] ; int minimum length = huffman decode max code length ; int maximum length = num ;	Constructs Huffman decoding tables from lists of Canonical Huffman code lengths.
int next symbol ( ) {	Decodes and returns the next symbol.
public long number ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session number ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the current number of sessions in the internal session cache.
public long connect renegotiate ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session connect renegotiate ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of start renegotiations in client mode.
public long accept renegotiate ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session accept renegotiate ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of start renegotiations in server mode.
public long cb hits ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session cb hits ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of successfully retrieved sessions from the external session cache in server mode.
public long misses ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session misses ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of sessions proposed by clients that were not found in the internal session cachein server mode.
public long cache full ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session cache full ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of sessions that were removed because the maximum session cache size was exceeded.
public long ticket key fail ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session ticket key fail ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of times a client presented a ticket that did not match any key in the list.
public long ticket key new ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session ticket key new ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of times a client did not present a ticket and we issued a new one.
public long ticket key renew ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session ticket key renew ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of times a client presented a ticket derived from an older key,and we upgraded to the primary key.
public long ticket key resume ( ) { lock reader lock = context . ctx lock . read lock ( ) ; reader lock . lock ( ) ; try { return ssl . session ticket key resume ( context . ctx ) ; } finally { reader lock . unlock ( ) ; } }	Returns the number of times a client presented a ticket derived from the primary key.
@ setup public void setup ( ) { system . set property ( str , check accessible ) ; system . set property ( str , check bounds ) ; buffer = buffer type . new buffer ( ) ; }	applies only to readBatch benchmark.
@ suppress warnings ( str ) private static int unsigned short be ( byte buf buffer , int offset ) { return buffer . order ( ) == byte order . big endian ? buffer . get unsigned short ( offset ) : buffer . get unsigned short le ( offset ) ; }	Reads a big-endian unsigned short integer from the buffer.
static boolean is valid host name for sni ( string hostname ) { return hostname != null && hostname . index of ( str ) > num && ! hostname . ends with ( str ) && ! net util . is valid ip v4 address ( hostname ) && ! net util . is valid ip v6 address ( hostname ) ; }	Validate that the given hostname can be used in SNI extension.
private boolean accept stream ( int stream id , byte priority , boolean remote side closed , boolean local side closed ) {	need to synchronize accesses to sentGoAwayFrame, lastGoodStreamId, and initial window sizes.
private static int find matching length ( byte buf in , int min index , int in index , int max index ) { int matched = num ; while ( in index <= max index - num && in . get int ( in index ) == in . get int ( min index + matched ) ) { in index += num ; matched += num ; } while ( in index < max index && in . get byte ( min index + matched ) == in . get byte ( in index ) ) { ++ in index ; ++ matched ; } return matched ; }	Iterates over the supplied input buffer between the supplied minIndex andmaxIndex to find how long our matched copy overlaps with an already-writtenliteral value.
static void encode literal ( byte buf in , byte buf out , int length ) { if ( length < num ) { out . write byte ( length - num << num ) ; } else { int bit length = bits to encode ( length - num ) ; int bytes to encode = num + bit length / num ; out . write byte ( num + bytes to encode << num ) ; for ( int i = num ; i < bytes to encode ; i ++ ) { out . write byte ( length - num > > i * num & num ) ; } } out . write bytes ( in , length ) ; }	Writes a literal to the supplied output buffer by directly copying fromthe input buffer.
private static void encode copy ( byte buf out , int offset , int length ) { while ( length >= num ) { encode copy with offset ( out , offset , num ) ; length -= num ; } if ( length > num ) { encode copy with offset ( out , offset , num ) ; length -= num ; } encode copy with offset ( out , offset , length ) ; }	Encodes a series of copies, each at most 64 bytes in length.
static int calculate checksum ( byte buf data , int offset , int length ) { crc32c crc32 = new crc32c ( ) ; try { crc32 . update ( data , offset , length ) ; return mask checksum ( ( int ) crc32 . get value ( ) ) ; } finally { crc32 . reset ( ) ; } }	Computes the CRC32C checksum of the supplied data and performs the "mask" operationon the computed checksum.
static boolean is j2 o ( string key , string value ) { return value . equals ( j2o . get ( key ) ) ; }	Tests if the specified key-value pair has been cached in Java-to-OpenSSL cache.
static boolean is o2 j ( string key , string protocol , string value ) { map < string , string > p2j = o2j . get ( key ) ; if ( p2j == null ) { return bool ; } else { return value . equals ( p2j . get ( protocol ) ) ; } }	Tests if the specified key-value pair has been cached in OpenSSL-to-Java cache.
static string to open ssl ( string java cipher suite , boolean boring ssl ) { string converted = j2o . get ( java cipher suite ) ; if ( converted != null ) { return converted ; } return cache from java ( java cipher suite , boring ssl ) ; }	Converts the specified Java cipher suite to its corresponding OpenSSL cipher suite name.
static string to java ( string open ssl cipher suite , string protocol ) { map < string , string > p2j = o2j . get ( open ssl cipher suite ) ; if ( p2j == null ) { p2j = cache from open ssl ( open ssl cipher suite ) ;	Convert from OpenSSL cipher suite name convention to java cipher suite name convention.
protected long delay nanos ( long current time nanos ) { scheduled future task < ? > scheduled task = peek scheduled task ( ) ; if ( scheduled task == null ) { return schedule purge interval ; } return scheduled task . delay nanos ( current time nanos ) ; }	Returns the amount of time left until the scheduled task with the closest dead line is executed.
protected boolean confirm shutdown ( ) { if ( ! is shutting down ( ) ) { return bool ; } if ( ! in event loop ( ) ) { throw new illegal state exception ( str ) ; } cancel scheduled tasks ( ) ; if ( graceful shutdown start time == num ) { graceful shutdown start time = scheduled future task . nano time ( ) ; } if ( run all tasks ( ) || run shutdown hooks ( ) ) { if ( is shutdown ( ) ) {	Confirm that the shutdown if the instance should be done now!.
public void set max header table size ( byte buf out , long max header table size ) throws http2 exception { if ( max header table size < min header table size || max header table size > max header table size ) { throw connection error ( protocol error , str , min header table size , max header table size , max header table size ) ; } if ( this . max header table size == max header table size ) { return ; } this . max header table size = max header table size ; ensure capacity ( num ) ;	Set the maximum table size.
private void encode string literal ( byte buf out , char sequence string ) { int huffman length = hpack huffman encoder . get encoded length ( string ) ; if ( huffman length < string . length ( ) ) { encode integer ( out , num , num , huffman length ) ; hpack huffman encoder . encode ( out , string ) ; } else { encode integer ( out , num , num , string . length ( ) ) ; if ( string instanceof ascii string ) {	Encode string literal according to Section 5.2.
private void encode literal ( byte buf out , char sequence name , char sequence value , index type index type , int name index ) { boolean name index valid = name index != - num ; switch ( index type ) { case incremental : encode integer ( out , num , num , name index valid ? name index : num ) ; break ; case none : encode integer ( out , num , num , name index valid ? name index : num ) ; break ; case never : encode integer ( out , num , num , name index valid ? name index : num ) ; break ; default : throw new error ( str ) ; } if ( ! name index valid ) { encode string literal ( out , name ) ; } encode string literal ( out , value ) ; }	Encode literal header field according to Section 6.2.
hpack header field get header field ( int index ) { header entry entry = head ; while ( index -- >= num ) { entry = entry . before ; } return entry ; }	Return the header field at the given index.
private header entry get entry ( char sequence name , char sequence value ) { if ( length ( ) == num || name == null || value == null ) { return null ; } int h = ascii string . hash code ( name ) ; int i = index ( h ) ; for ( header entry e = header fields [ i ] ; e != null ; e = e . next ) {	Returns the header entry with the lowest index value for the header field.
private int get index ( char sequence name ) { if ( length ( ) == num || name == null ) { return - num ; } int h = ascii string . hash code ( name ) ; int i = index ( h ) ; for ( header entry e = header fields [ i ] ; e != null ; e = e . next ) { if ( e . hash == h && equals constant time ( name , e . name ) != num ) { return get index ( e . index ) ; } } return - num ; }	Returns the lowest index value for the header field name in the dynamic table.
private void add ( char sequence name , char sequence value , long header size ) {	Add the header field to the dynamic table.
private static list < string > dedup ( list < string > encoded , map < string , integer > name to last index ) { boolean [ ] is last instance = new boolean [ encoded . size ( ) ] ; for ( int idx : name to last index . values ( ) ) { is last instance [ idx ] = bool ; } list < string > dedupd = new array list < string > ( name to last index . size ( ) ) ; for ( int i = num , n = encoded . size ( ) ; i < n ; i ++ ) { if ( is last instance [ i ] ) { dedupd . add ( encoded . get ( i ) ) ; } } return dedupd ; }	Deduplicate a list of encoded cookies by keeping only the last instance with a given name.
@ override public void trace ( string format , object arg ) { if ( is trace enabled ( ) ) { formatting tuple ft = message formatter . format ( format , arg ) ; logger . log ( fqcn , trace capable ? level . trace : level . debug , ft . get message ( ) , ft . get throwable ( ) ) ; } }	Log a message at level TRACE according to the specified format andargument.
@ override public void warn ( string msg ) { logger . log ( fqcn , level . warn , msg , null ) ; }	Log a message object at the WARN level.
public ssl context builder protocols ( string ... protocols ) { this . protocols = protocols == null ? null : protocols . clone ( ) ; return this ; }	The TLS protocol versions to enable.
public int length ( ) { int length ; if ( head < tail ) { length = hpack header fields . length - tail + head ; } else { length = head - tail ; } return length ; }	Return the number of header fields in the dynamic table.
public void add ( hpack header field header ) { int header size = header . size ( ) ; if ( header size > capacity ) { clear ( ) ; return ; } while ( capacity - size < header size ) { remove ( ) ; } hpack header fields [ head ++ ] = header ; size += header . size ( ) ; if ( head == hpack header fields . length ) { head = num ; } }	Add the header field to the dynamic table.
public hpack header field remove ( ) { hpack header field removed = hpack header fields [ tail ] ; if ( removed == null ) { return null ; } size -= removed . size ( ) ; hpack header fields [ tail ++ ] = null ; if ( tail == hpack header fields . length ) { tail = num ; } return removed ; }	Remove and return the oldest header field from the dynamic table.
public void clear ( ) { while ( tail != head ) { hpack header fields [ tail ++ ] = null ; if ( tail == hpack header fields . length ) { tail = num ; } } head = num ; tail = num ; size = num ; }	Remove all entries from the dynamic table.
public void set capacity ( long capacity ) { if ( capacity < min header table size || capacity > max header table size ) { throw new illegal argument exception ( str + capacity ) ; }	Set the maximum size of the dynamic table.
private void set upgrade request headers ( channel handler context ctx , http request request ) {	Adds all upgrade request headers necessary for an upgrade to the supported protocols.
private void send not modified ( channel handler context ctx ) { full http response response = new default full http response ( http 1 1 , not modified ) ; set date header ( response ) ; this . send and cleanup connection ( ctx , response ) ; }	When file timestamp is the same as what the browser is sending up, send a "304 Not Modified".
private void bind compressor to stream ( embedded channel compressor , int stream id ) { if ( compressor != null ) { http2 stream stream = connection ( ) . stream ( stream id ) ; if ( stream != null ) { stream . set property ( property key , compressor ) ; } } }	Called after the super class has written the headers and created any associated stream objects.
static void write raw varint32 ( byte buf out , int value ) { while ( bool ) { if ( ( value & ~ num ) == num ) { out . write byte ( value ) ; return ; } else { out . write byte ( ( value & num ) | num ) ; value >>>= num ; } } }	Writes protobuf varint32 to (.
private static mqtt fixed header decode fixed header ( byte buf buffer ) { short b1 = buffer . read unsigned byte ( ) ; mqtt message type message type = mqtt message type . value of ( b1 > > num ) ; boolean dup flag = ( b1 & num ) == num ; int qos level = ( b1 & num ) > > num ; boolean retain = ( b1 & num ) != num ; int remaining length = num ; int multiplier = num ; short digit ; int loops = num ; do { digit = buffer . read unsigned byte ( ) ; remaining length += ( digit & num ) * multiplier ; multiplier *= num ; loops ++ ; } while ( ( digit & num ) != num && loops < num ) ;	Decodes the fixed header.
private static result < ? > decode payload ( byte buf buffer , mqtt message type message type , int bytes remaining in variable part , object variable header ) { switch ( message type ) { case connect : return decode connection payload ( buffer , ( mqtt connect variable header ) variable header ) ; case subscribe : return decode subscribe payload ( buffer , bytes remaining in variable part ) ; case suback : return decode suback payload ( buffer , bytes remaining in variable part ) ; case unsubscribe : return decode unsubscribe payload ( buffer , bytes remaining in variable part ) ; case publish : return decode publish payload ( buffer , bytes remaining in variable part ) ; default :	Decodes the payload.
public final void add ( byte buf buf , channel future listener listener ) {	Add a buffer to the end of the queue and associate a listener with it that should be completed whenall the buffers bytes have been consumed from the queue and written.
public final void copy to ( abstract coalescing buffer queue dest ) { dest . buf and listener pairs . add all ( buf and listener pairs ) ; dest . increment readable bytes ( readable bytes ) ; }	Copy all pending entries in this queue into the destination queue.
public final void write and remove all ( channel handler context ctx ) { decrement readable bytes ( readable bytes ) ; throwable pending = null ; byte buf previous buf = null ; for ( ; ; ) { object entry = buf and listener pairs . poll ( ) ; try { if ( entry == null ) { if ( previous buf != null ) { ctx . write ( previous buf , ctx . void promise ( ) ) ; } break ; } if ( entry instanceof byte buf ) { if ( previous buf != null ) { ctx . write ( previous buf , ctx . void promise ( ) ) ; } previous buf = ( byte buf ) entry ; } else if ( entry instanceof channel promise ) { ctx . write ( previous buf , ( channel promise ) entry ) ; previous buf = null ; } else { ctx . write ( previous buf ) . add listener ( ( channel future listener ) entry ) ; previous buf = null ; } } catch ( throwable t ) { if ( pending == null ) { pending = t ; } else { logger . info ( str , pending , t ) ; } } } if ( pending != null ) { throw new illegal state exception ( pending ) ; } }	Writes all remaining elements in this queue.
public static hosts file entries parse ( file file , charset ... charsets ) throws io { check not null ( file , str ) ; check not null ( charsets , str ) ; if ( file . exists ( ) && file . is file ( ) ) { for ( charset charset : charsets ) { hosts file entries entries = parse ( new buffered reader ( new input stream reader ( new file input stream ( file ) , charset ) ) ) ; if ( entries != hosts file entries . empty ) { return entries ; } } } return hosts file entries . empty ; }	Parse a hosts file.
public static hosts file entries parse ( reader reader ) throws io { check not null ( reader , str ) ; buffered reader buff = new buffered reader ( reader ) ; try { map < string , inet4 address > ipv4 entries = new hash map < string , inet4 address > ( ) ; map < string , inet6 address > ipv6 entries = new hash map < string , inet6 address > ( ) ; string line ; while ( ( line = buff . read line ( ) ) != null ) {	Parse a reader of hosts file format.
final int calculate out net buf size ( int plaintext bytes , int num buffers ) {	Calculates the maximum size of the encrypted output buffer required to wrap the given plaintext bytes.
private static int find version ( final byte buf buffer ) { final int n = buffer . readable bytes ( ) ;	Returns the proxy protocol specification version in the buffer if the version is found.Returns -1 if no version was found in the buffer.
private static int find end of header ( final byte buf buffer ) { final int n = buffer . readable bytes ( ) ;	Returns the index in the buffer of the end of header if found.Returns -1 if no end of header was found in the buffer.
void create global traffic counter ( scheduled executor service executor ) {	Create the global TrafficCounter.
public collection < traffic counter > channel traffic counters ( ) { return new abstract collection < traffic counter > ( ) { @ override public iterator < traffic counter > iterator ( ) { return new iterator < traffic counter > ( ) { final iterator < per channel > iter = channel queues . values ( ) . iterator ( ) ; @ override public boolean has next ( ) { return iter . has next ( ) ; } @ override public traffic counter next ( ) { return iter . next ( ) . channel traffic counter ; } @ override public void remove ( ) { throw new unsupported operation exception ( ) ; } } ; } @ override public int size ( ) { return channel queues . size ( ) ; } } ; }	To allow for instance doAccounting to use the TrafficCounter per channel.
boolean has readable bits ( int count ) { if ( count < num ) { throw new illegal argument exception ( str + count + str ) ; } return bit count >= count || ( in . readable bytes ( ) << num & integer . max value ) >= count - bit count ; }	Checks that the specified number of bits available for reading.
boolean has readable bytes ( int count ) { if ( count < num || count > max count of readable bytes ) { throw new illegal argument exception ( str + count + str + max count of readable bytes + str ) ; } return has readable bits ( count << num ) ; }	Checks that the specified number of bytes available for reading.
private static int parse code ( byte buf buffer ) { final int first = parse number ( buffer . read byte ( ) ) * num ; final int second = parse number ( buffer . read byte ( ) ) * num ; final int third = parse number ( buffer . read byte ( ) ) ; return first + second + third ; }	Parses the io.netty.handler.codec.smtp code without any allocation, which is three digits.
private static boolean is line based ( final byte buf [ ] delimiters ) { if ( delimiters . length != num ) { return bool ; } byte buf a = delimiters [ num ] ; byte buf b = delimiters [ num ] ; if ( a . capacity ( ) < b . capacity ( ) ) { a = delimiters [ num ] ; b = delimiters [ num ] ; } return a . capacity ( ) == num && b . capacity ( ) == num && a . get byte ( num ) == str && a . get byte ( num ) == str && b . get byte ( num ) == str ; }	Returns true if the delimiters are "\n" and "\r\n".
protected s state ( s new state ) { s old state = state ; state = new state ; return old state ; }	Sets the current state of this decoder.
private static boolean is last ( http message http message ) { if ( http message instanceof full http message ) { full http message full message = ( full http message ) http message ; if ( full message . trailing headers ( ) . is empty ( ) && ! full message . content ( ) . is readable ( ) ) { return bool ; } } return bool ; }	Checks if the given HTTP message should be considered as a last SPDY frame.
map < integer , stream state > active streams ( ) { map < integer , stream state > streams = new tree map < integer , stream state > ( stream comparator ) ; streams . put all ( active streams ) ; return streams ; }	Stream-IDs should be iterated in priority order.
public set < string > subprotocols ( ) { set < string > ret = new linked hash set < string > ( ) ; collections . add all ( ret , subprotocols ) ; return ret ; }	Returns the CSV of supported sub protocols.
protected string select subprotocol ( string requested subprotocols ) { if ( requested subprotocols == null || subprotocols . length == num ) { return null ; } string [ ] requested subprotocol array = requested subprotocols . split ( str ) ; for ( string p : requested subprotocol array ) { string requested subprotocol = p . trim ( ) ; for ( string supported subprotocol : subprotocols ) { if ( sub protocol wildcard . equals ( supported subprotocol ) || requested subprotocol . equals ( supported subprotocol ) ) { selected subprotocol = requested subprotocol ; return requested subprotocol ; } } }	Selects the first matching supported sub protocol.
public static int index of ( byte buf needle , byte buf haystack ) {	Returns the reader index of needle in haystack, or -1 if needle is not in haystack.
@ suppress warnings ( str ) public static byte buf write short be ( byte buf buf , int short value ) { return buf . order ( ) == byte order . big endian ? buf . write short ( short value ) : buf . write short le ( short value ) ; }	Writes a big-endian 16-bit short integer to the buffer.
@ suppress warnings ( str ) public static byte buf set short be ( byte buf buf , int index , int short value ) { return buf . order ( ) == byte order . big endian ? buf . set short ( index , short value ) : buf . set short le ( index , short value ) ; }	Sets a big-endian 16-bit short integer to the buffer.
@ suppress warnings ( str ) public static byte buf write medium be ( byte buf buf , int medium value ) { return buf . order ( ) == byte order . big endian ? buf . write medium ( medium value ) : buf . write medium le ( medium value ) ; }	Writes a big-endian 24-bit medium integer to the buffer.
public static byte buf thread local direct buffer ( ) { if ( thread local buffer size <= num ) { return null ; } if ( platform dependent . has unsafe ( ) ) { return thread local unsafe direct byte buf . new instance ( ) ; } else { return thread local direct byte buf . new instance ( ) ; } }	Returns a cached thread-local direct buffer, if available.
public static void throw exception ( throwable t ) { if ( has unsafe ( ) ) { platform dependent0 . throw exception ( t ) ; } else { platform dependent . < runtime exception > throw exception0 ( t ) ; } }	Raises an exception bypassing compiler checks for checked exceptions.
public static boolean is zero ( byte [ ] bytes , int start pos , int length ) { return ! has unsafe ( ) || ! unaligned access ( ) ? is zero safe ( bytes , start pos , length ) : platform dependent0 . is zero ( bytes , start pos , length ) ; }	Determine if a subsection of an array is zero.
public static int hash code ascii ( byte [ ] bytes , int start pos , int length ) { return ! has unsafe ( ) || ! unaligned access ( ) ? hash code ascii safe ( bytes , start pos , length ) : platform dependent0 . hash code ascii ( bytes , start pos , length ) ; }	Calculate a hash code of a byte array assuming ASCII character encoding.The resulting hash code will be case insensitive.
static int hash code ascii safe ( byte [ ] bytes , int start pos , int length ) { int hash = hash code ascii seed ; final int remaining bytes = length & num ; final int end = start pos + remaining bytes ; for ( int i = start pos - num + length ; i >= end ; i -= num ) { hash = platform dependent0 . hash code ascii compute ( get long safe ( bytes , i ) , hash ) ; } switch ( remaining bytes ) { case num : return ( ( hash * hash code + hash code ascii sanitize ( bytes [ start pos ] ) ) * hash code + hash code ascii sanitize ( get short safe ( bytes , start pos + num ) ) ) * hash code + hash code ascii sanitize ( get int safe ( bytes , start pos + num ) ) ; case num : return ( hash * hash code + hash code ascii sanitize ( get short safe ( bytes , start pos ) ) ) * hash code + hash code ascii sanitize ( get int safe ( bytes , start pos + num ) ) ; case num : return ( hash * hash code + hash code ascii sanitize ( bytes [ start pos ] ) ) * hash code + hash code ascii sanitize ( get int safe ( bytes , start pos + num ) ) ; case num : return hash * hash code + hash code ascii sanitize ( get int safe ( bytes , start pos ) ) ; case num : return ( hash * hash code + hash code ascii sanitize ( bytes [ start pos ] ) ) * hash code + hash code ascii sanitize ( get short safe ( bytes , start pos + num ) ) ; case num : return hash * hash code + hash code ascii sanitize ( get short safe ( bytes , start pos ) ) ; case num : return hash * hash code + hash code ascii sanitize ( bytes [ start pos ] ) ; default : return hash ; } }	Package private for testing purposes only!.
private t get or create ( string name ) { t constant = constants . get ( name ) ; if ( constant == null ) { final t temp constant = new constant ( next id ( ) , name ) ; constant = constants . put if absent ( name , temp constant ) ; if ( constant == null ) { return temp constant ; } } return constant ; }	Get existing constant by name or creates new one if not exists. Threadsafe.
private t create or throw ( string name ) { t constant = constants . get ( name ) ; if ( constant == null ) { final t temp constant = new constant ( next id ( ) , name ) ; constant = constants . put if absent ( name , temp constant ) ; if ( constant == null ) { return temp constant ; } } throw new illegal argument exception ( string . format ( str , name ) ) ; }	Creates constant by name or throws exception. Threadsafe.
private static chunk type map chunk type ( byte type ) { if ( type == num ) { return chunk type . compressed data ; } else if ( type == num ) { return chunk type . uncompressed data ; } else if ( type == ( byte ) num ) { return chunk type . stream identifier ; } else if ( ( type & num ) == num ) { return chunk type . reserved skippable ; } else { return chunk type . reserved unskippable ; } }	Decodes the chunk type from the type tag byte.
private void set multipart ( string content type ) { string [ ] data boundary = http post request decoder . get multipart data boundary ( content type ) ; if ( data boundary != null ) { multipart data boundary = data boundary [ num ] ; if ( data boundary . length > num && data boundary [ num ] != null ) { charset = charset . for name ( data boundary [ num ] ) ; } } else { multipart data boundary = null ; } current status = multi part status . headerdelimiter ; }	Set from the request ContentType the multipartDataBoundary and the possible charset.
private void parse body multipart ( ) { if ( undecoded chunk == null || undecoded chunk . readable bytes ( ) == num ) {	Parse the Body for multipart.
private static void skip control characters ( byte buf undecoded chunk ) { if ( ! undecoded chunk . has array ( ) ) { try { skip control characters standard ( undecoded chunk ) ; } catch ( index out of bounds exception e1 ) { throw new not enough data decoder exception ( e1 ) ; } return ; } seek ahead optimize sao = new seek ahead optimize ( undecoded chunk ) ; while ( sao . pos < sao . limit ) { char c = ( char ) ( sao . bytes [ sao . pos ++ ] & num ) ; if ( ! character . is iso ( c ) && ! character . is whitespace ( c ) ) { sao . set read position ( num ) ; return ; } } throw new not enough data decoder exception ( str ) ; }	Skip control Characters.
private interface http data find multipart delimiter ( string delimiter , multi part status disposition status , multi part status close delimiter status ) {	Find the next Multipart Delimiter.
private void clean mixed attributes ( ) { current field attributes . remove ( http header values . charset ) ; current field attributes . remove ( http header names . content length ) ; current field attributes . remove ( http header names . content transfer encoding ) ; current field attributes . remove ( http header names . content type ) ; current field attributes . remove ( http header values . filename ) ; }	Remove all Attributes that should be cleaned between two FileUpload inMixed mode.
private static boolean load data multipart standard ( byte buf undecoded chunk , string delimiter , http data http data ) { final int start reader index = undecoded chunk . reader index ( ) ; final int delimeter length = delimiter . length ( ) ; int index = num ; int last position = start reader index ; byte prev byte = http constants . lf ; boolean delimiter found = bool ; while ( undecoded chunk . is readable ( ) ) { final byte next byte = undecoded chunk . read byte ( ) ;	Load the field value or file data from a Multipart request.
private static boolean load data multipart ( byte buf undecoded chunk , string delimiter , http data http data ) { if ( ! undecoded chunk . has array ( ) ) { return load data multipart standard ( undecoded chunk , delimiter , http data ) ; } final seek ahead optimize sao = new seek ahead optimize ( undecoded chunk ) ; final int start reader index = undecoded chunk . reader index ( ) ; final int delimeter length = delimiter . length ( ) ; int index = num ; int last real pos = sao . pos ; byte prev byte = http constants . lf ; boolean delimiter found = bool ; while ( sao . pos < sao . limit ) { final byte next byte = sao . bytes [ sao . pos ++ ] ;	Load the field value from a Multipart request.
private static string clean string ( string field ) { int size = field . length ( ) ; string builder sb = new string builder ( size ) ; for ( int i = num ; i < size ; i ++ ) { char next char = field . char at ( i ) ; switch ( next char ) { case http constants . colon : case http constants . comma : case http constants . equals : case http constants . semicolon : case http constants . ht : sb . append ( http constants . sp char ) ; break ; case http constants . double quote :	Clean the String from any unallowed character.
private boolean skip one line ( ) { if ( ! undecoded chunk . is readable ( ) ) { return bool ; } byte next byte = undecoded chunk . read byte ( ) ; if ( next byte == http constants . cr ) { if ( ! undecoded chunk . is readable ( ) ) { undecoded chunk . reader index ( undecoded chunk . reader index ( ) - num ) ; return bool ; } next byte = undecoded chunk . read byte ( ) ; if ( next byte == http constants . lf ) { return bool ; } undecoded chunk . reader index ( undecoded chunk . reader index ( ) - num ) ; return bool ; } if ( next byte == http constants . lf ) { return bool ; } undecoded chunk . reader index ( undecoded chunk . reader index ( ) - num ) ; return bool ; }	Skip one empty line.
private static string [ ] split multipart header ( string sb ) { array list < string > headers = new array list < string > ( num ) ; int name start ; int name end ; int colon end ; int value start ; int value end ; name start = http post body util . find non whitespace ( sb , num ) ; for ( name end = name start ; name end < sb . length ( ) ; name end ++ ) { char ch = sb . char at ( name end ) ; if ( ch == str || character . is whitespace ( ch ) ) { break ; } } for ( colon end = name end ; colon end < sb . length ( ) ; colon end ++ ) { if ( sb . char at ( colon end ) == str ) { colon end ++ ; break ; } } value start = http post body util . find non whitespace ( sb , colon end ) ; value end = http post body util . find end of string ( sb ) ; headers . add ( sb . substring ( name start , name end ) ) ; string svalue = ( value start >= value end ) ? string util . empty string : sb . substring ( value start , value end ) ; string [ ] values ; if ( svalue . index of ( str ) >= num ) { values = split multipart header values ( svalue ) ; } else { values = svalue . split ( str ) ; } for ( string value : values ) { headers . add ( value . trim ( ) ) ; } string [ ] array = new string [ headers . size ( ) ] ; for ( int i = num ; i < headers . size ( ) ; i ++ ) { array [ i ] = headers . get ( i ) ; } return array ; }	Split one header in Multipart.
private static string [ ] split multipart header values ( string svalue ) { list < string > values = internal thread local map . get ( ) . array list ( num ) ; boolean in quote = bool ; boolean escape next = bool ; int start = num ; for ( int i = num ; i < svalue . length ( ) ; i ++ ) { char c = svalue . char at ( i ) ; if ( in quote ) { if ( escape next ) { escape next = bool ; } else { if ( c == str ) { escape next = bool ; } else if ( c == str ) { in quote = bool ; } } } else { if ( c == str ) { in quote = bool ; } else if ( c == str ) { values . add ( svalue . substring ( start , i ) ) ; start = i + num ; } } } values . add ( svalue . substring ( start ) ) ; return values . to array ( new string [ num ] ) ; }	Split one header value in Multipart.
static void handle ( channel handler context ctx , http2 connection connection , http2 frame listener listener , full http message message ) throws http2 exception { try { int stream id = get stream id ( connection , message . headers ( ) ) ; http2 stream stream = connection . stream ( stream id ) ; if ( stream == null ) { stream = connection . remote ( ) . create stream ( stream id , bool ) ; } message . headers ( ) . set ( http conversion util . extension header names . scheme . text ( ) , http scheme . http . name ( ) ) ; http2 headers message headers = http conversion util . to http2 headers ( message , bool ) ; boolean has content = message . content ( ) . is readable ( ) ; boolean has trailers = ! message . trailing headers ( ) . is empty ( ) ; listener . on headers read ( ctx , stream id , message headers , num , ! ( has content || has trailers ) ) ; if ( has content ) { listener . on data read ( ctx , stream id , message . content ( ) , num , ! has trailers ) ; } if ( has trailers ) { http2 headers headers = http conversion util . to http2 headers ( message . trailing headers ( ) , bool ) ; listener . on headers read ( ctx , stream id , headers , num , bool ) ; } stream . close remote side ( ) ; } finally { message . release ( ) ; } }	control, but there is not yet an API for signaling that.
private void write symbol map ( byte buf out ) { bzip2 bit writer writer = this . writer ; final boolean [ ] block values present = this . block values present ; final boolean [ ] condensed in use = new boolean [ num ] ; for ( int i = num ; i < condensed in use . length ; i ++ ) { for ( int j = num , k = i << num ; j < huffman symbol range size ; j ++ , k ++ ) { if ( block values present [ k ] ) { condensed in use [ i ] = bool ; } } } for ( boolean is condensed in use : condensed in use ) { writer . write boolean ( out , is condensed in use ) ; } for ( int i = num ; i < condensed in use . length ; i ++ ) { if ( condensed in use [ i ] ) { for ( int j = num , k = i << num ; j < huffman symbol range size ; j ++ , k ++ ) { writer . write boolean ( out , block values present [ k ] ) ; } } } }	Write the Huffman symbol to output byte map.
private void write run ( final int value , int run length ) { final int block length = this . block length ; final byte [ ] block = this . block ; block values present [ value ] = bool ; crc . update crc ( value , run length ) ; final byte byte value = ( byte ) value ; switch ( run length ) { case num : block [ block length ] = byte value ; this . block length = block length + num ; break ; case num : block [ block length ] = byte value ; block [ block length + num ] = byte value ; this . block length = block length + num ; break ; case num : block [ block length ] = byte value ; block [ block length + num ] = byte value ; block [ block length + num ] = byte value ; this . block length = block length + num ; break ; default : run length -= num ; block values present [ run length ] = bool ; block [ block length ] = byte value ; block [ block length + num ] = byte value ; block [ block length + num ] = byte value ; block [ block length + num ] = byte value ; block [ block length + num ] = ( byte ) run length ; this . block length = block length + num ; break ; } }	Writes an RLE run to the block array, updating the block CRC and present values array as required.
boolean write ( final int value ) { if ( block length > block length limit ) { return bool ; } final int rle current value = this . rle current value ; final int rle length = this . rle length ; if ( rle length == num ) { this . rle current value = value ; this . rle length = num ; } else if ( rle current value != value ) {	Writes a byte to the block, accumulating to an RLE run where possible.
int write ( final byte buf buffer , int offset , int length ) { int index = buffer . for each byte ( offset , length , write processor ) ; return index == - num ? length : index - offset ; }	Writes an array to the block.
void close ( byte buf out ) {	Compresses and writes out the block.
private channel future respond ( encodable result ) { socket address remote address = channel . remote address ( ) ; return channel . write and flush ( result ) . add listener ( future -> { if ( future . is success ( ) ) { logger . trace ( str , result , remote address ) ; } else { logger . error ( string . format ( str , result , remote address ) , future . cause ( ) ) ; channel . close ( ) ; } } ) ; }	Responds to a single message with some Encodable object.
private void receive ( ) { try { socket socket = null ; buffered reader reader = null ; try {	Create a socket connection and receive data until receiver is stopped.
public static storage level create ( boolean use disk , boolean use memory , boolean use off heap , boolean deserialized , int replication ) { return storage level . apply ( use disk , use memory , use off heap , deserialized , replication ) ; }	Create a new StorageLevel object.
public static columnar batch to batch ( struct type schema , memory mode mem mode , iterator < row > row ) { int capacity = num * num ; writable column vector [ ] column vectors ; if ( mem mode == memory mode . off heap ) { column vectors = off heap column vector . allocate columns ( capacity , schema ) ; } else { column vectors = on heap column vector . allocate columns ( capacity , schema ) ; } int n = num ; while ( row . has next ( ) ) { row r = row . next ( ) ; for ( int i = num ; i < schema . fields ( ) . length ; i ++ ) { append value ( column vectors [ i ] , schema . fields ( ) [ i ] . data type ( ) , r , i ) ; } n ++ ; } columnar batch batch = new columnar batch ( column vectors ) ; batch . set num rows ( n ) ; return batch ; }	Converts an iterator of rows into a single ColumnBatch.
public void register executor ( string app id , string exec id , executor shuffle info executor info ) { app exec id full id = new app exec id ( app id , exec id ) ; logger . info ( str , full id , executor info ) ; if ( ! known managers . contains ( executor info . shuffle manager ) ) { throw new unsupported operation exception ( str + executor info ) ; } try { if ( db != null ) { byte [ ] key = db app exec key ( full id ) ; byte [ ] value = mapper . write value as string ( executor info ) . get bytes ( standard charsets . utf 8 ) ; db . put ( key , value ) ; } } catch ( exception e ) { logger . error ( str , e ) ; } executors . put ( full id , executor info ) ; }	Registers a new Executor with all the configuration we need to find its shuffle files.
public void application removed ( string app id , boolean cleanup local dirs ) { logger . info ( str , app id , cleanup local dirs ) ; iterator < map . entry < app exec id , executor shuffle info > > it = executors . entry set ( ) . iterator ( ) ; while ( it . has next ( ) ) { map . entry < app exec id , executor shuffle info > entry = it . next ( ) ; app exec id full id = entry . get key ( ) ; final executor shuffle info executor = entry . get value ( ) ;	Removes our metadata of all executors registered for the given application, and optionallyalso deletes the local directories associated with the executors of that application in aseparate thread.It is not valid to call registerExecutor() for an executor with this appId after invokingthis method.
public void executor removed ( string executor id , string app id ) { logger . info ( str , executor id ) ; app exec id full id = new app exec id ( app id , executor id ) ; final executor shuffle info executor = executors . get ( full id ) ; if ( executor == null ) {	Removes all the non-shuffle files in any local directories associated with the finishedexecutor.
private void delete executor dirs ( string [ ] dirs ) { for ( string local dir : dirs ) { try { java utils . delete recursively ( new file ( local dir ) ) ; logger . debug ( str , local dir ) ; } catch ( exception e ) { logger . error ( str + local dir , e ) ; } } }	Synchronously deletes each directory one at a time.Should be executed in its own thread, as this may take a long time.
private void delete non shuffle files ( string [ ] dirs ) { filename filter filter = new filename filter ( ) { @ override public boolean accept ( file dir , string name ) {	Synchronously deletes non-shuffle files in each directory recursively.Should be executed in its own thread, as this may take a long time.
private managed buffer get sort based shuffle block data ( executor shuffle info executor , int shuffle id , int map id , int reduce id ) { file index file = get file ( executor . local dirs , executor . sub dirs per local dir , str + shuffle id + str + map id + str ) ; try { shuffle index information shuffle index information = shuffle index cache . get ( index file ) ; shuffle index record shuffle index record = shuffle index information . get index ( reduce id ) ; return new file segment managed buffer ( conf , get file ( executor . local dirs , executor . sub dirs per local dir , str + shuffle id + str + map id + str ) , shuffle index record . get offset ( ) , shuffle index record . get length ( ) ) ; } catch ( execution exception e ) { throw new runtime exception ( str + index file , e ) ; } }	Sort-based shuffle data uses an index called "shuffle_ShuffleId_MapId_0.index" into a data filecalled "shuffle_ShuffleId_MapId_0.data".
public void insert record ( long record pointer , int partition id ) { if ( ! has space for another record ( ) ) { throw new illegal state exception ( str ) ; } array . set ( pos , packed record pointer . pack pointer ( record pointer , partition id ) ) ; pos ++ ; }	Inserts a record to be sorted.
public shuffle sorter iterator get sorted iterator ( ) { int offset = num ; if ( use radix sort ) { offset = radix sort . sort ( array , pos , packed record pointer . partition id start byte index , packed record pointer . partition id end byte index , bool , bool ) ; } else { memory block unused = new memory block ( array . get base object ( ) , array . get base offset ( ) + pos * num , ( array . size ( ) - pos ) * num ) ; long array buffer = new long array ( unused ) ; sorter < packed record pointer , long array > sorter = new sorter < > ( new shuffle sort data format ( buffer ) ) ; sorter . sort ( array , num , pos , sort comparator ) ; } return new shuffle sorter iterator ( pos , array , offset ) ; }	Return an iterator over record pointers in sorted order.
private void fail outstanding requests ( throwable cause ) { for ( map . entry < stream chunk id , chunk received callback > entry : outstanding fetches . entry set ( ) ) { try { entry . get value ( ) . on failure ( entry . get key ( ) . chunk index , cause ) ; } catch ( exception e ) { logger . warn ( str , e ) ; } } for ( map . entry < long , rpc response callback > entry : outstanding rpcs . entry set ( ) ) { try { entry . get value ( ) . on failure ( cause ) ; } catch ( exception e ) { logger . warn ( str , e ) ; } } for ( pair < string , stream callback > entry : stream callbacks ) { try { entry . get value ( ) . on failure ( entry . get key ( ) , cause ) ; } catch ( exception e ) { logger . warn ( str , e ) ; } }	Fire the failure callback for all outstanding requests.
@ override protected void service init ( configuration conf ) throws exception { conf = conf ; boolean stop on failure = conf . get boolean ( stop on failure key , default stop on failure ) ; try {	Start the shuffle server with the given configuration.
@ override protected void service stop ( ) { try { if ( shuffle server != null ) { shuffle server . close ( ) ; } if ( transport context != null ) { transport context . close ( ) ; } if ( block handler != null ) { block handler . close ( ) ; } if ( db != null ) { db . close ( ) ; } } catch ( exception e ) { logger . error ( str , e ) ; } }	Close the shuffle server to clean up any associated state.
protected file init recovery db ( string db name ) { preconditions . check not null ( recovery path , str ) ; file recovery file = new file ( recovery path . to uri ( ) . get path ( ) , db name ) ; if ( recovery file . exists ( ) ) { return recovery file ; }	Figure out the recovery path and handle moving the DB if YARN NM recovery gets enabledand DB exists in the local dir of NM by old version of shuffle service.
private static list < string > build command ( abstract command builder builder , map < string , string > env , boolean print launch command ) throws io , illegal argument exception { list < string > cmd = builder . build command ( env ) ; if ( print launch command ) { system . err . println ( str + join ( str , cmd ) ) ; system . err . println ( str ) ; } return cmd ; }	Prepare spark commands with the appropriate command builder.If printLaunchCommand is set then the commands will be printed to the stderr.
private static string prepare windows command ( list < string > cmd , map < string , string > child env ) { string builder cmdline = new string builder ( ) ; for ( map . entry < string , string > e : child env . entry set ( ) ) { cmdline . append ( string . format ( str , e . get key ( ) , e . get value ( ) ) ) ; cmdline . append ( str ) ; } for ( string arg : cmd ) { cmdline . append ( quote for batch script ( arg ) ) ; cmdline . append ( str ) ; } return cmdline . to string ( ) ; }	Prepare a command line for execution from a Windows batch script.The method quotes all arguments so that spaces are handled as expected.
private static list < string > prepare bash command ( list < string > cmd , map < string , string > child env ) { if ( child env . is empty ( ) ) { return cmd ; } list < string > new cmd = new array list < > ( ) ; new cmd . add ( str ) ; for ( map . entry < string , string > e : child env . entry set ( ) ) { new cmd . add ( string . format ( str , e . get key ( ) , e . get value ( ) ) ) ; } new cmd . add all ( cmd ) ; return new cmd ; }	Prepare the command for execution from a bash script.
public void zero out null bytes ( ) { for ( int i = num ; i < null bits size ; i += num ) { platform . put long ( get buffer ( ) , starting offset + i , num ) ; } }	Clears out null bits.
client challenge challenge ( ) throws general security exception { this . auth nonce = random bytes ( conf . encryption key length ( ) / byte . size ) ; secret key spec auth key = generate key ( conf . key factory algorithm ( ) , conf . key factory iterations ( ) , auth nonce , conf . encryption key length ( ) ) ; initialize for auth ( conf . cipher transformation ( ) , auth nonce , auth key ) ; this . challenge = random bytes ( conf . encryption key length ( ) / byte . size ) ; return new client challenge ( new string ( app id , utf 8 ) , conf . key factory algorithm ( ) , conf . key factory iterations ( ) , conf . cipher transformation ( ) , conf . encryption key length ( ) , auth nonce , challenge ( app id , auth nonce , challenge ) ) ; }	Create the client challenge.
server response respond ( client challenge client challenge ) throws general security exception { secret key spec auth key = generate key ( client challenge . kdf , client challenge . iterations , client challenge . nonce , client challenge . key length ) ; initialize for auth ( client challenge . cipher , client challenge . nonce , auth key ) ; byte [ ] challenge = validate challenge ( client challenge . nonce , client challenge . challenge ) ; byte [ ] response = challenge ( app id , client challenge . nonce , raw response ( challenge ) ) ; byte [ ] session nonce = random bytes ( conf . encryption key length ( ) / byte . size ) ; byte [ ] input iv = random bytes ( conf . iv length ( ) ) ; byte [ ] output iv = random bytes ( conf . iv length ( ) ) ; secret key spec session key = generate key ( client challenge . kdf , client challenge . iterations , session nonce , client challenge . key length ) ; this . session cipher = new transport cipher ( crypto conf , client challenge . cipher , session key , input iv , output iv ) ;	Validates the client challenge, and create the encryption backend for the channel from theparameters sent by the client.
void validate ( server response server response ) throws general security exception { byte [ ] response = validate challenge ( auth nonce , server response . response ) ; byte [ ] expected = raw response ( challenge ) ; preconditions . check argument ( arrays . equals ( expected , response ) ) ; byte [ ] nonce = decrypt ( server response . nonce ) ; byte [ ] input iv = decrypt ( server response . input iv ) ; byte [ ] output iv = decrypt ( server response . output iv ) ; secret key spec session key = generate key ( conf . key factory algorithm ( ) , conf . key factory iterations ( ) , nonce , conf . encryption key length ( ) ) ; this . session cipher = new transport cipher ( crypto conf , conf . cipher transformation ( ) , session key , input iv , output iv ) ; }	Validates the server response and initializes the cipher to use for the session.
private byte [ ] validate challenge ( byte [ ] nonce , byte [ ] encrypted challenge ) throws general security exception { byte [ ] challenge = decrypt ( encrypted challenge ) ; check sub array ( app id , challenge , num ) ; check sub array ( nonce , challenge , app id . length ) ; return arrays . copy of range ( challenge , app id . length + nonce . length , challenge . length ) ; }	Validates an encrypted challenge as defined in the protocol, and returns the byte arraythat corresponds to the actual challenge data.
private void check sub array ( byte [ ] test , byte [ ] data , int offset ) { preconditions . check argument ( data . length >= test . length + offset ) ; for ( int i = num ; i < test . length ; i ++ ) { preconditions . check argument ( test [ i ] == data [ i + offset ] ) ; } }	Checks that the "test" array is in the data array starting at the given offset.
private static int fmix ( int h1 , int length ) { h1 ^= length ; h1 ^= h1 > > > num ; h1 *= num ; h1 ^= h1 > > > num ; h1 *= num ; h1 ^= h1 > > > num ; return h1 ; }	Finalization mix - force all bits of a hash block to avalanche.
@ override public boolean accept inbound message ( object msg ) throws exception { if ( msg instanceof chunk fetch request ) { return bool ; } else { return super . accept inbound message ( msg ) ; } }	Overwrite acceptInboundMessage to properly delegate ChunkFetchRequest messagesto ChunkFetchRequestHandler.
public transport server create server ( int port , list < transport server bootstrap > bootstraps ) { return new transport server ( this , null , port , rpc handler , bootstraps ) ; }	Create a server which will attempt to bind to a specific port.
public transport server create server ( string host , int port , list < transport server bootstrap > bootstraps ) { return new transport server ( this , host , port , rpc handler , bootstraps ) ; }	Create a server which will attempt to bind to a specific host and port.
private chunk fetch request handler create chunk fetch handler ( transport channel handler channel handler , rpc handler rpc handler ) { return new chunk fetch request handler ( channel handler . get client ( ) , rpc handler . get stream manager ( ) , conf . max chunks being transferred ( ) ) ; }	Creates the dedicated ChannelHandler for ChunkFetchRequest messages.
void monitor child ( ) { process proc = child proc ; if ( proc == null ) {	Wait for the child process to exit and update the handle's state if necessary, according tothe exit code.
public t set properties file ( string path ) { check not null ( path , str ) ; builder . set properties file ( path ) ; return self ( ) ; }	Set a custom properties file with Spark configuration for the application.
public t set conf ( string key , string value ) { check not null ( key , str ) ; check not null ( value , str ) ; check argument ( key . starts with ( str ) , str ) ; builder . conf . put ( key , value ) ; return self ( ) ; }	Set a single configuration value for the application.
public t set app name ( string app name ) { check not null ( app name , str ) ; builder . app name = app name ; return self ( ) ; }	Set the application name.
public t set master ( string master ) { check not null ( master , str ) ; builder . master = master ; return self ( ) ; }	Set the Spark master for the application.
public t set deploy mode ( string mode ) { check not null ( mode , str ) ; builder . deploy mode = mode ; return self ( ) ; }	Set the deploy mode for the application.
public t add app args ( string ... args ) { for ( string arg : args ) { check not null ( arg , str ) ; builder . app args . add ( arg ) ; } return self ( ) ; }	Adds command line arguments for the application.
public t add jar ( string jar ) { check not null ( jar , str ) ; builder . jars . add ( jar ) ; return self ( ) ; }	Adds a jar file to be submitted with the application.
public t add file ( string file ) { check not null ( file , str ) ; builder . files . add ( file ) ; return self ( ) ; }	Adds a file to be submitted with the application.
public void zero out ( ) { for ( long off = base offset ; off < base offset + length * width ; off += width ) { platform . put long ( base obj , off , num ) ; } }	Fill this all with 0L.
private static case insensitive string map catalog options ( string name , sql conf ) { map < string , string > all confs = map as java map converter ( conf . get all confs ( ) ) . as java ( ) ; pattern prefix = pattern . compile ( str + name + str ) ; hash map < string , string > options = new hash map < > ( ) ; for ( map . entry < string , string > entry : all confs . entry set ( ) ) { matcher matcher = prefix . matcher ( entry . get key ( ) ) ; if ( matcher . matches ( ) && matcher . group count ( ) > num ) { options . put ( matcher . group ( num ) , entry . get value ( ) ) ; } } return new case insensitive string map ( options ) ; }	Extracts a named catalog's configuration from a SQLConf.
synchronized string register handle ( abstract app handle handle ) { string secret = create secret ( ) ; secret to pending apps . put ( secret , handle ) ; return secret ; }	Registers a handle with the server, and returns the secret the child app needs to connectback.
private static long [ ] [ ] get counts ( long array array , long num records , int start byte index , int end byte index ) { long [ ] [ ] counts = new long [ num ] [ ] ;	Computes a value histogram for each byte in the given array.
private static long [ ] transform counts to offsets ( long [ ] counts , long num records , long output offset , long bytes per record , boolean desc , boolean signed ) { assert counts . length == num ; int start = signed ? num : num ;	Transforms counts into the proper unsafe output offsets for the sort type.
@ override public void initialize ( string path , list < string > columns ) throws io , unsupported operation exception { super . initialize ( path , columns ) ; initialize internal ( ) ; }	Utility API that will read all the data in path.
public boolean next batch ( ) throws io { for ( writable column vector vector : column vectors ) { vector . reset ( ) ; } columnar batch . set num rows ( num ) ; if ( rows returned >= total row count ) return bool ; check end of row group ( ) ; int num = ( int ) math . min ( ( long ) capacity , total count loaded so far - rows returned ) ; for ( int i = num ; i < column readers . length ; ++ i ) { if ( column readers [ i ] == null ) continue ; column readers [ i ] . read batch ( num , column vectors [ i ] ) ; } rows returned += num ; columnar batch . set num rows ( num ) ; num batched = num ; batch idx = num ; return bool ; }	Advances to the next batch of rows.
public static void close quietly ( closeable closeable ) { try { if ( closeable != null ) { closeable . close ( ) ; } } catch ( io e ) { logger . error ( str , e ) ; } }	Closes the given object, ignoring IOExceptions.
public static void delete recursively ( file file , filename filter filter ) throws io { if ( file == null ) { return ; }	Delete a file or directory and its contents recursively.Don't follow directories if they are symlinks.
public static byte [ ] buffer to array ( byte buffer buffer ) { if ( buffer . has array ( ) && buffer . array offset ( ) == num && buffer . array ( ) . length == buffer . remaining ( ) ) { return buffer . array ( ) ; } else { byte [ ] bytes = new byte [ buffer . remaining ( ) ] ; buffer . get ( bytes ) ; return bytes ; } }	Returns a byte array with the buffer's contents, trying to avoid copying the data ifpossible.
public static void read fully ( readable byte channel channel , byte buffer dst ) throws io { int expected = dst . remaining ( ) ; while ( dst . has remaining ( ) ) { if ( channel . read ( dst ) < num ) { throw new eof ( string . format ( str , expected ) ) ; } } }	Fills a buffer with data read from the channel.
public void point to ( object base object , long base offset , int size in bytes ) {	Update this UnsafeMapData to point to different backing data.
public void reset ( ) { if ( is constant ) return ; if ( child columns != null ) { for ( column vector c : child columns ) { ( ( writable column vector ) c ) . reset ( ) ; } } elements appended = num ; if ( num nulls > num ) { put not nulls ( num , capacity ) ; num nulls = num ; } }	Resets this column for writing.
public writable column vector reserve dictionary ids ( int capacity ) { if ( dictionary ids == null ) { dictionary ids = reserve new column ( capacity , data types . integer type ) ; } else { dictionary ids . reset ( ) ; dictionary ids . reserve ( capacity ) ; } return dictionary ids ; }	Reserve a integer column for ids of dictionary.
@ override public final columnar array get array ( int row id ) { if ( is null at ( row id ) ) return null ; return new columnar array ( array data ( ) , get array offset ( row id ) , get array length ( row id ) ) ; }	array offsets and lengths in the current column vector.
@ override public final columnar map get map ( int row id ) { if ( is null at ( row id ) ) return null ; return new columnar map ( get child ( num ) , get child ( num ) , get array offset ( row id ) , get array length ( row id ) ) ; }	second child column vector, and puts the offsets and lengths in the current column vector.
public static < t > typed column < t , double > avg ( map function < t , double > f ) { return new typed average < t > ( f ) . to column java ( ) ; }	Average aggregate function.
public static < t > typed column < t , long > count ( map function < t , object > f ) { return new typed count < t > ( f ) . to column java ( ) ; }	Count aggregate function.
private void read async ( ) throws io { state change lock . lock ( ) ; final byte [ ] arr = read ahead buffer . array ( ) ; try { if ( end of stream || read in progress ) { return ; } check read exception ( ) ; read ahead buffer . position ( num ) ; read ahead buffer . flip ( ) ; read in progress = bool ; } finally { state change lock . unlock ( ) ; } executor service . execute ( ( ) -> { state change lock . lock ( ) ; try { if ( is closed ) { read in progress = bool ; return ; }	Read data from underlyingInputStream to readAheadBuffer asynchronously.
public void cache thread local raw store ( ) { long thread id = this . get id ( ) ; raw store thread local raw store = hive meta store . hms . get raw store ( ) ; if ( thread local raw store != null && ! thread raw store map . contains key ( thread id ) ) { log . debug ( str + thread local raw store + str + this . get name ( ) + str ) ; thread raw store map . put ( thread id , thread local raw store ) ; } }	Cache the ThreadLocal RawStore object.
private string convert pattern ( final string pattern , boolean datanucleus format ) { string w str ; if ( datanucleus format ) { w str = str ; } else { w str = str ; } return pattern . replace all ( str , str + w str ) . replace all ( str , str ) . replace all ( str , w str ) . replace all ( str , str ) . replace all ( str , str ) . replace all ( str , str ) ; }	Convert a pattern containing JDBC catalog search wildcards intoJava regex patterns.
public iterator < internal row > row iterator ( ) { final int max rows = num rows ; final mutable columnar row row = new mutable columnar row ( columns ) ; return new iterator < internal row > ( ) { int row id = num ; @ override public boolean has next ( ) { return row id < max rows ; } @ override public internal row next ( ) { if ( row id >= max rows ) { throw new no such element exception ( ) ; } row . row id = row id ++ ; return row ; } @ override public void remove ( ) { throw new unsupported operation exception ( ) ; } } ; }	Returns an iterator over the rows in this batch.
public internal row get row ( int row id ) { assert ( row id >= num && row id < num rows ) ; row . row id = row id ; return row ; }	Returns the row in this batch at `rowId`.
public long send rpc ( byte buffer message , rpc response callback callback ) { if ( logger . is trace enabled ( ) ) { logger . trace ( str , get remote address ( channel ) ) ; } long request id = request id ( ) ; handler . add rpc request ( request id , callback ) ; rpc channel listener listener = new rpc channel listener ( request id , callback ) ; channel . write and flush ( new rpc request ( request id , new nio managed buffer ( message ) ) ) . add listener ( listener ) ; return request id ; }	Sends an opaque message to the RpcHandler on the server-side.
public byte buffer send rpc sync ( byte buffer message , long timeout ms ) { final settable future < byte buffer > result = settable future . create ( ) ; send rpc ( message , new rpc response callback ( ) { @ override public void on success ( byte buffer response ) { byte buffer copy = byte buffer . allocate ( response . remaining ( ) ) ; copy . put ( response ) ;	Synchronously sends an opaque message to the RpcHandler on the server-side, waiting for up toa specified timeout for a response.
private boolean refill ( ) throws io { if ( ! byte buffer . has remaining ( ) ) { byte buffer . clear ( ) ; int n read = num ; while ( n read == num ) { n read = file channel . read ( byte buffer ) ; } if ( n read < num ) { return bool ; } byte buffer . flip ( ) ; } return bool ; }	Checks weather data is left to be read from the input stream.
public string sign cookie ( string str ) { if ( str == null || str . is empty ( ) ) { throw new illegal argument exception ( str ) ; } string signature = get signature ( str ) ; if ( log . is debug enabled ( ) ) { log . debug ( str + str + str + signature ) ; } return str + signature + signature ; }	Sign the cookie given the string token as input.
public string verify and extract ( string signed str ) { int index = signed str . last index of ( signature ) ; if ( index == - num ) { throw new illegal argument exception ( str + signed str ) ; } string original signature = signed str . substring ( index + signature . length ( ) ) ; string raw value = signed str . substring ( num , index ) ; string current signature = get signature ( raw value ) ; if ( log . is debug enabled ( ) ) { log . debug ( str + raw value + str + current signature ) ; } if ( ! original signature . equals ( current signature ) ) { throw new illegal argument exception ( str + original signature + str + current signature ) ; } return raw value ; }	Verify a signed string and extracts the original string.
private string get signature ( string str ) { try { message digest md = message digest . get instance ( sha string ) ; md . update ( str . get bytes ( ) ) ; md . update ( secret bytes ) ; byte [ ] digest = md . digest ( ) ; return new base64 ( num ) . encode to string ( digest ) ; } catch ( no such algorithm exception ex ) { throw new runtime exception ( str + sha string + str + ex . get message ( ) , ex ) ; } }	Get the signature of the input string based on SHA digest algorithm.
void close iterator ( db < ? > it ) throws io { synchronized ( this . db ) { db db = this . db . get ( ) ; if ( db != null ) { it . close ( ) ; } } }	Closes the given iterator if the DB is still open.
db get type info ( class < ? > type ) throws exception { db ti = types . get ( type ) ; if ( ti == null ) { db tmp = new db ( this , type , get type alias ( type ) ) ; ti = types . put if absent ( type , tmp ) ; if ( ti == null ) { ti = tmp ; } } return ti ; }	Returns metadata about indices for the given type.
db db ( ) { db db = this . db . get ( ) ; if ( db == null ) { throw new illegal state exception ( str ) ; } return db ; }	Try to avoid use-after close since that has the tendency of crashing the JVM.
public void register app ( string app id , string shuffle secret ) {	Register an application with its secret.Executors need to first authenticate themselves with the same secret beforefetching shuffle files written by other executors in this application.
public void register app ( string app id , byte buffer shuffle secret ) { register app ( app id , java utils . bytes to string ( shuffle secret ) ) ; }	Register an application with its secret specified as a byte buffer.
public void free ( ) { if ( consumer != null ) { if ( array != null ) { consumer . free array ( array ) ; } array = null ; } }	Free the memory used by pointer array.
public void insert record ( long record pointer , long key prefix , boolean prefix is null ) { if ( ! has space for another record ( ) ) { throw new illegal state exception ( str ) ; } if ( prefix is null && radix sort support != null ) {	Inserts a record to be sorted.
public static long next power of2 ( long num ) { final long high bit = long . highest one bit ( num ) ; return ( high bit == num ) ? num : high bit << num ; }	Returns the next number greater or equal num that is power of 2.
public static boolean array equals ( object left base , long left offset , object right base , long right offset , final long length ) { int i = num ;	Optimized byte array equality check for byte arrays.
public byte buffer to byte buffer ( ) {	Serializes the 'type' byte followed by the message itself.
public spark launcher set spark home ( string spark home ) { check not null ( spark home , str ) ; builder . child env . put ( env spark home , spark home ) ; return this ; }	Set a custom Spark installation location for the application.
public void add spill if not empty ( unsafe sorter iterator spill reader ) throws io { if ( spill reader . has next ( ) ) {	Add an UnsafeSorterIterator to this merger.
private void fail remaining blocks ( string [ ] failed block ids , throwable e ) { for ( string block id : failed block ids ) { try { listener . on block fetch failure ( block id , e ) ; } catch ( exception e2 ) { logger . error ( str , e2 ) ; } } }	Invokes the "onBlockFetchFailure" callback for every listed block id.
private string get client name from cookie ( cookie [ ] cookies ) {	Retrieves the client name from cookieString.
private string to cookie str ( cookie [ ] cookies ) { string cookie str = str ; for ( cookie c : cookies ) { cookie str += c . get name ( ) + str + c . get value ( ) + str ; } return cookie str ; }	Convert cookie array to human readable cookie string.
private cookie create cookie ( string str ) throws unsupported encoding exception { if ( log . is debug enabled ( ) ) { log . debug ( str + auth cookie + str + str ) ; } cookie cookie = new cookie ( auth cookie , str ) ; cookie . set max age ( cookie max age ) ; if ( cookie domain != null ) { cookie . set domain ( cookie domain ) ; } if ( cookie path != null ) { cookie . set path ( cookie path ) ; } cookie . set secure ( is cookie secure ) ; return cookie ; }	Generate a server side cookie given the cookie value as the input.
private static string get http only cookie header ( cookie cookie ) { new cookie new cookie = new new cookie ( cookie . get name ( ) , cookie . get value ( ) , cookie . get path ( ) , cookie . get domain ( ) , cookie . get version ( ) , cookie . get comment ( ) , cookie . get max age ( ) , cookie . get secure ( ) ) ; return new cookie + str ; }	Generate httponly cookie from HS2 cookie.
private string do kerberos auth ( http servlet request request ) throws http authentication exception {	Do the GSS-API kerberos authentication.We already have a logged in subject in the form of serviceUGI,which GSS-API will extract information from.In case of a SPNego request we use the httpUGI,for the authenticating service tickets.
private string get auth header ( http servlet request request , string auth type ) throws http authentication exception { string auth header = request . get header ( http auth utils . authorization ) ;	Returns the base64 encoded auth header payload.
static string join ( string sep , string ... elements ) { string builder sb = new string builder ( ) ; for ( string e : elements ) { if ( e != null ) { if ( sb . length ( ) > num ) { sb . append ( sep ) ; } sb . append ( e ) ; } } return sb . to string ( ) ; }	Joins a list of strings using the given separator.
static string first non empty value ( string key , map < ? , ? > ... maps ) { for ( map < ? , ? > map : maps ) { string value = ( string ) map . get ( key ) ; if ( ! is empty ( value ) ) { return value ; } } return null ; }	Returns the first non-empty value mapped to the given key in the given maps, or null otherwise.
static string first non empty ( string ... candidates ) { for ( string s : candidates ) { if ( ! is empty ( s ) ) { return s ; } } return null ; }	Returns the first non-empty, non-null string in the given list, or null otherwise.
static void check not null ( object o , string arg ) { if ( o == null ) { throw new illegal argument exception ( string . format ( str , arg ) ) ; } }	Throws IllegalArgumentException if the given object is null.
static void check argument ( boolean check , string msg , object ... args ) { if ( ! check ) { throw new illegal argument exception ( string . format ( msg , args ) ) ; } }	Throws IllegalArgumentException with the given message if the check is false.
static void check state ( boolean check , string msg , object ... args ) { if ( ! check ) { throw new illegal state exception ( string . format ( msg , args ) ) ; } }	Throws IllegalStateException with the given message if the check is false.
static string find jars dir ( string spark home , string scala version , boolean fail if not found ) {	Find the location of the Spark jars dir, depending on whether we're looking at a buildor a distribution directory.
public void point to ( object base object , long base offset , int size in bytes ) {	Update this UnsafeArrayData to point to different backing data.
private user group information get current ugi ( hive conf op config ) throws sql { try { return utils . get ugi ( ) ; } catch ( exception e ) { throw new sql ( str , e ) ; } }	Returns the current UGI on the stack.
private row set prepare from row ( list < object > rows , row set row set ) throws exception { for ( object row : rows ) { row set . add row ( ( object [ ] ) row ) ; } return row set ; }	already encoded to thrift-able object in ThriftFormatter.
private hive conf get config for operation ( ) throws sql { hive conf sql operation conf = get parent session ( ) . get hive conf ( ) ; if ( ! get conf overlay ( ) . is empty ( ) || should run async ( ) ) {	If there are query specific settings to overlay, then create a copy of configThere are two cases we need to clone the session config that's being passed to hive driver1.
private synchronized boolean should retry ( throwable e ) { boolean is io = e instanceof io || ( e . get cause ( ) != null && e . get cause ( ) instanceof io ) ; boolean has remaining retries = retry count < max retries ; return is io && has remaining retries ; }	Returns true if we should retry due a block fetch failure.
@ override public unsafe row append row ( object kbase , long koff , int klen , object vbase , long voff , int vlen ) {	Append a key value pair.It copies data into the backing MemoryBlock.Returns an UnsafeRow pointing to the value if succeeds, otherwise returns null.
public static ut from string ( string str ) { return str == null ? null : from bytes ( str . get bytes ( standard charsets . utf 8 ) ) ; }	Creates an UTF8String from String.
public static ut blank string ( int length ) { byte [ ] spaces = new byte [ length ] ; arrays . fill ( spaces , ( byte ) str ) ; return from bytes ( spaces ) ; }	Creates an UTF8String that contains `length` spaces.
public void write to memory ( object target , long target offset ) { platform . copy memory ( base , offset , target , target offset , num bytes ) ; }	Writes the content of this string into a memory address, identified by an object and an offset.The target memory address must already been allocated, and have enough space to hold all thebytes in this string.
public byte [ ] get bytes ( ) {	Returns the underline bytes, will be a copy of it if it's part of another array.
public boolean contains ( final ut substring ) { if ( substring . num bytes == num ) { return bool ; } byte first = substring . get byte ( num ) ; for ( int i = num ; i <= num bytes - substring . num bytes ; i ++ ) { if ( get byte ( i ) == first && match at ( substring , i ) ) { return bool ; } } return bool ; }	Returns whether this contains `substring` or not.
public ut to upper case ( ) { if ( num bytes == num ) { return empty ut ; } byte [ ] bytes = new byte [ num bytes ] ; bytes [ num ] = ( byte ) character . to title case ( get byte ( num ) ) ; for ( int i = num ; i < num bytes ; i ++ ) { byte b = get byte ( i ) ; if ( num bytes for first byte ( b ) != num ) {	Returns the upper case of this string.
public ut to lower case ( ) { if ( num bytes == num ) { return empty ut ; } byte [ ] bytes = new byte [ num bytes ] ; bytes [ num ] = ( byte ) character . to title case ( get byte ( num ) ) ; for ( int i = num ; i < num bytes ; i ++ ) { byte b = get byte ( i ) ; if ( num bytes for first byte ( b ) != num ) {	Returns the lower case of this string.
public ut to title case ( ) { if ( num bytes == num ) { return empty ut ; } byte [ ] bytes = new byte [ num bytes ] ; for ( int i = num ; i < num bytes ; i ++ ) { byte b = get byte ( i ) ; if ( i == num || get byte ( i - num ) == str ) { if ( num bytes for first byte ( b ) != num ) {	Returns the title case of this string, that could be used as title.
private ut copy ut ( int start , int end ) { int len = end - start + num ; byte [ ] new bytes = new byte [ len ] ; copy memory ( base , offset + start , new bytes , byte array offset , len ) ; return ut . from bytes ( new bytes ) ; }	Copy the bytes from the current UTF8String, and make a new UTF8String.
public ut trim ( ut trim string ) { if ( trim string != null ) { return trim left ( trim string ) . trim right ( trim string ) ; } else { return null ; } }	Based on the given trim string, trim this string starting from both endsThis method searches for each character in the source string, removes the character if it isfound in the trim string, stops at the first not found.
public ut trim left ( ut trim string ) { if ( trim string == null ) return null ;	Based on the given trim string, trim this string starting from left endThis method searches each character in the source string starting from the left end, removesthe character if it is in the trim string, stops at the first character which is not in thetrim string, returns the new string.
public ut trim right ( ut trim string ) { if ( trim string == null ) return null ; int char idx = num ;	Based on the given trim string, trim this string starting from right endThis method searches each character in the source string starting from the right end,removes the character if it is in the trim string, stops at the first character which is notin the trim string, returns the new string.
private int find ( ut str , int start ) { assert ( str . num bytes > num ) ; while ( start <= num bytes - str . num bytes ) { if ( byte array methods . array equals ( base , offset + start , str . base , str . offset , str . num bytes ) ) { return start ; } start += num ; } return - num ; }	Find the `str` from left to right.
public static ut concat ( ut ... inputs ) {	Concatenates input strings together into a single string.
public void register with shuffle server ( string host , int port , string exec id , executor shuffle info executor info ) throws io , interrupted exception { check init ( ) ; try ( transport client client = client factory . create unmanaged client ( host , port ) ) { byte buffer register message = new register executor ( app id , exec id , executor info ) . to byte buffer ( ) ; client . send rpc sync ( register message , registration timeout ms ) ; } }	Registers this executor with an external shuffle server.
@ override public long spill ( long size , memory consumer trigger ) throws io { if ( trigger != this ) { if ( reading iterator != null ) { return reading iterator . spill ( ) ; } return num ;	Sort and spill the current records in response to memory pressure.
private long get memory usage ( ) { long total page size = num ; for ( memory block page : allocated pages ) { total page size += page . size ( ) ; } return ( ( in mem sorter == null ) ? num : in mem sorter . get memory usage ( ) ) + total page size ; }	Return the total memory usage of this sorter, including the data pages and the sorter's pointerarray.
private long free memory ( ) { update peak memory used ( ) ; long memory freed = num ; for ( memory block block : allocated pages ) { memory freed += block . size ( ) ; free page ( block ) ; } allocated pages . clear ( ) ; current page = null ; page cursor = num ; return memory freed ; }	Free this sorter's data pages.
private void delete spill files ( ) { for ( unsafe sorter spill writer spill : spill writers ) { file file = spill . get file ( ) ; if ( file != null && file . exists ( ) ) { if ( ! file . delete ( ) ) { logger . error ( str , file . get absolute path ( ) ) ; } } } }	Deletes any spill files created by this sorter.
private void grow pointer array if necessary ( ) throws io { assert ( in mem sorter != null ) ; if ( ! in mem sorter . has space for another record ( ) ) { long used = in mem sorter . get memory usage ( ) ; long array array ; try {	Checks whether there is enough space to insert an additional record in to the sort pointerarray and grows the array if additional space is required.
private void acquire new page if necessary ( int required ) { if ( current page == null || page cursor + required > current page . get base offset ( ) + current page . size ( ) ) {	Allocates more memory in order to insert an additional record.
public void insert record ( object record base , long record offset , int length , long prefix , boolean prefix is null ) throws io { assert ( in mem sorter != null ) ; if ( in mem sorter . num records ( ) >= num elements for spill threshold ) { logger . info ( str + num elements for spill threshold ) ; spill ( ) ; } grow pointer array if necessary ( ) ; int uao size = unsafe aligned offset . get uao size ( ) ;	Write a record to the sorter.
public void merge ( unsafe external sorter other ) throws io { other . spill ( ) ; spill writers . add all ( other . spill writers ) ;	Merges another UnsafeExternalSorters into this one, the other one will be emptied.
public unsafe sorter iterator get iterator ( int start index ) throws io { if ( spill writers . is empty ( ) ) { assert ( in mem sorter != null ) ; unsafe sorter iterator iter = in mem sorter . get sorted iterator ( ) ; move over ( iter , start index ) ; return iter ; } else { linked list < unsafe sorter iterator > queue = new linked list < > ( ) ; int i = num ; for ( unsafe sorter spill writer spill writer : spill writers ) { if ( i + spill writer . records spilled ( ) > start index ) { unsafe sorter iterator iter = spill writer . get reader ( serializer manager ) ; move over ( iter , start index - i ) ; queue . add ( iter ) ; } i += spill writer . records spilled ( ) ; } if ( in mem sorter != null ) { unsafe sorter iterator iter = in mem sorter . get sorted iterator ( ) ; move over ( iter , start index - i ) ; queue . add ( iter ) ; } return new chained iterator ( queue ) ; } }	Returns an iterator starts from startIndex, which will return the rows in the order asinserted.It is the caller's responsibility to call `cleanupResources()`after consuming this iterator.TODO: support forced spilling.
void grow ( int needed size ) { if ( needed size < num ) { throw new illegal argument exception ( str + needed size + str ) ; } if ( needed size > array max - total size ( ) ) { throw new illegal argument exception ( str + needed size + str + str + array max ) ; } final int length = total size ( ) + needed size ; if ( buffer . length < length ) {	Grows the buffer by at least neededSize and points the row to the buffer.
public synchronized byte [ ] first token ( ) { if ( sasl client != null && sasl client . has initial response ( ) ) { try { return sasl client . evaluate challenge ( new byte [ num ] ) ; } catch ( sasl exception e ) { throw throwables . propagate ( e ) ; } } else { return new byte [ num ] ; } }	Used to initiate SASL handshake with server.
public synchronized byte [ ] response ( byte [ ] token ) { try { return sasl client != null ? sasl client . evaluate challenge ( token ) : new byte [ num ] ; } catch ( sasl exception e ) { throw throwables . propagate ( e ) ; } }	Respond to server's SASL token.
@ override public synchronized void dispose ( ) { if ( sasl client != null ) { try { sasl client . dispose ( ) ; } catch ( sasl exception e ) {	Disposes of any system resources or security-sensitive information theSaslClient might be using.
@ override public void get metrics ( metrics collector collector , boolean all ) { metrics record builder metrics record builder = collector . add record ( str ) ; for ( map . entry < string , metric > entry : metric set . get metrics ( ) . entry set ( ) ) { collect metric ( metrics record builder , entry . get key ( ) , entry . get value ( ) ) ; } }	Get metrics from the source.
public static properties to crypto conf ( string prefix , iterable < map . entry < string , string > > conf ) { properties props = new properties ( ) ; for ( map . entry < string , string > e : conf ) { string key = e . get key ( ) ; if ( key . starts with ( prefix ) ) { props . set property ( commons crypto config prefix + key . substring ( prefix . length ( ) ) , e . get value ( ) ) ; } } return props ; }	Extract the commons-crypto configuration embedded in a list of config values.
public boolean [ ] get booleans ( int row id , int count ) { boolean [ ] res = new boolean [ count ] ; for ( int i = num ; i < count ; i ++ ) { res [ i ] = get boolean ( row id + i ) ; } return res ; }	Gets boolean type values from [rowId, rowId + count).
public byte [ ] get bytes ( int row id , int count ) { byte [ ] res = new byte [ count ] ; for ( int i = num ; i < count ; i ++ ) { res [ i ] = get byte ( row id + i ) ; } return res ; }	Gets byte type values from [rowId, rowId + count).
public short [ ] get shorts ( int row id , int count ) { short [ ] res = new short [ count ] ; for ( int i = num ; i < count ; i ++ ) { res [ i ] = get short ( row id + i ) ; } return res ; }	Gets short type values from [rowId, rowId + count).
public int [ ] get ints ( int row id , int count ) { int [ ] res = new int [ count ] ; for ( int i = num ; i < count ; i ++ ) { res [ i ] = get int ( row id + i ) ; } return res ; }	Gets int type values from [rowId, rowId + count).
public long [ ] get longs ( int row id , int count ) { long [ ] res = new long [ count ] ; for ( int i = num ; i < count ; i ++ ) { res [ i ] = get long ( row id + i ) ; } return res ; }	Gets long type values from [rowId, rowId + count).
public float [ ] get floats ( int row id , int count ) { float [ ] res = new float [ count ] ; for ( int i = num ; i < count ; i ++ ) { res [ i ] = get float ( row id + i ) ; } return res ; }	Gets float type values from [rowId, rowId + count).
public double [ ] get doubles ( int row id , int count ) { double [ ] res = new double [ count ] ; for ( int i = num ; i < count ; i ++ ) { res [ i ] = get double ( row id + i ) ; } return res ; }	Gets double type values from [rowId, rowId + count).
public t get auth proc factory ( cli service ) throws login exception { if ( auth type str . equals ignore case ( auth types . kerberos . get auth name ( ) ) ) { return kerberos sasl helper . get kerberos processor factory ( sasl server , service ) ; } else { return plain sasl helper . get plain processor factory ( service ) ; } }	Returns the thrift processor factory for HiveServer2 running in binary mode.
public static void login from keytab ( hive conf hive conf ) throws io { string principal = hive conf . get var ( conf vars . hive serve kerberos principal ) ; string key tab file = hive conf . get var ( conf vars . hive serve kerberos keytab ) ; if ( principal . is empty ( ) || key tab file . is empty ( ) ) { throw new io ( str ) ; } else { user group information . login user from keytab ( security util . get server principal ( principal , str ) , key tab file ) ; } }	Perform kerberos login using the hadoop shim API if the configuration is available.
public static user group information login from spnego keytab and return ugi ( hive conf hive conf ) throws io { string principal = hive conf . get var ( conf vars . hive serve spnego principal ) ; string key tab file = hive conf . get var ( conf vars . hive serve spnego keytab ) ; if ( principal . is empty ( ) || key tab file . is empty ( ) ) { throw new io ( str ) ; } else { return user group information . login user from keytab and return ugi ( security util . get server principal ( principal , str ) , key tab file ) ; } }	Perform SPNEGO login using the hadoop shim API if the configuration is available.
public string get delegation token ( string owner , string renewer ) throws sql { if ( sasl server == null ) { throw new sql ( str , str ) ; } try { string token str = sasl server . get delegation token with service ( owner , renewer , h client token ) ; if ( token str == null || token str . is empty ( ) ) { throw new sql ( str + owner , str ) ; } return token str ; } catch ( io e ) { throw new sql ( str + owner , str , e ) ; } catch ( interrupted exception e ) { throw new sql ( str , str , e ) ; } }	retrieve delegation token for the given user.
public void cancel delegation token ( string delegation token ) throws sql { if ( sasl server == null ) { throw new sql ( str , str ) ; } try { sasl server . cancel delegation token ( delegation token ) ; } catch ( io e ) { throw new sql ( str + delegation token , str , e ) ; } }	cancel given delegation token.
public static string pattern to regex ( string pattern ) { if ( pattern == null ) { return str ; } else { string builder result = new string builder ( pattern . length ( ) ) ; boolean escaped = bool ; for ( int i = num , len = pattern . length ( ) ; i < len ; i ++ ) { char c = pattern . char at ( i ) ; if ( escaped ) { if ( c != search string escape ) { escaped = bool ; } result . append ( c ) ; } else { if ( c == search string escape ) { escaped = bool ; continue ; } else if ( c == str ) { result . append ( str ) ; } else if ( c == str ) { result . append ( str ) ; } else { result . append ( character . to lower case ( c ) ) ; } } } return result . to string ( ) ; } }	Convert a SQL search pattern into an equivalent Java Regex.
public static void ensure current state ( service . state state , service . state expected state ) { if ( state != expected state ) { throw new illegal state exception ( str + str + expected state + str + state ) ; } }	Verify that a service is in a given state.
public static void init ( service service , hive conf configuration ) { service . state state = service . get service state ( ) ; ensure current state ( state , service . state . notinited ) ; service . init ( configuration ) ; }	Initialize a service.The service state is checked before the operation begins.This process is not thread safe.
public static void start ( service service ) { service . state state = service . get service state ( ) ; ensure current state ( state , service . state . inited ) ; service . start ( ) ; }	Start a service.The service state is checked before the operation begins.This process is not thread safe.
public static void deploy ( service service , hive conf configuration ) { init ( service , configuration ) ; start ( service ) ; }	Initialize then start a service.The service state is checked before the operation begins.This process is not thread safe.
public static void stop ( service service ) { if ( service != null ) { service . state state = service . get service state ( ) ; if ( state == service . state . started ) { service . stop ( ) ; } } }	Stop a service.Do nothing if the service is null or not in a state in which it can be/needs to be stopped.The service state is checked before the operation begins.This process is not thread safe.
private boolean next ( ) throws io { if ( values read >= end of page value count ) { if ( values read >= total value count ) {	Advances to the next value.
void read batch ( int total , writable column vector column ) throws io { int row id = num ; writable column vector dictionary ids = null ; if ( dictionary != null ) {	Reads `total` values from this columnReader into column.
private schema column convert not supported exception construct convert not supported exception ( column descriptor descriptor , writable column vector column ) { return new schema column convert not supported exception ( arrays . to string ( descriptor . get path ( ) ) , descriptor . get primitive type ( ) . get primitive type name ( ) . to string ( ) , column . data type ( ) . catalog string ( ) ) ; }	Helper function to construct exception for parquet schema mismatch.
public static event loop group create event loop ( io mode , int num threads , string thread prefix ) { thread factory thread factory = create thread factory ( thread prefix ) ; switch ( mode ) { case nio : return new nio event loop group ( num threads , thread factory ) ; case epoll : return new epoll event loop group ( num threads , thread factory ) ; default : throw new illegal argument exception ( str + mode ) ; } }	Creates a Netty EventLoopGroup based on the IOMode.
public static class < ? extends server channel > get server channel class ( io mode ) { switch ( mode ) { case nio : return nio server socket channel . class ; case epoll : return epoll server socket channel . class ; default : throw new illegal argument exception ( str + mode ) ; } }	Returns the correct ServerSocketChannel class based on IOMode.
public static string get remote address ( channel channel ) { if ( channel != null && channel . remote address ( ) != null ) { return channel . remote address ( ) . to string ( ) ; } return str ; }	Returns the remote address on the channel or "&lt;unknown remote&gt;" if none exists.
public static int default num threads ( int num usable cores ) { final int available cores ; if ( num usable cores > num ) { available cores = num usable cores ; } else { available cores = runtime . get runtime ( ) . available processors ( ) ; } return math . min ( available cores , max default netty threads ) ; }	Returns the default number of threads for both the Netty client and server thread pools.If numUsableCores is 0, we will use Runtime get an approximate number of available cores.
public static synchronized pooled byte buf allocator get shared pooled byte buf allocator ( boolean allow direct bufs , boolean allow cache ) { final int index = allow cache ? num : num ; if ( shared pooled byte buf allocator [ index ] == null ) { shared pooled byte buf allocator [ index ] = create pooled byte buf allocator ( allow direct bufs , allow cache , default num threads ( num ) ) ; } return shared pooled byte buf allocator [ index ] ; }	Returns the lazily created shared pooled ByteBuf allocator for the specified allowCacheparameter value.
public static pooled byte buf allocator create pooled byte buf allocator ( boolean allow direct bufs , boolean allow cache , int num cores ) { if ( num cores == num ) { num cores = runtime . get runtime ( ) . available processors ( ) ; } return new pooled byte buf allocator ( allow direct bufs && platform dependent . direct buffer preferred ( ) , math . min ( pooled byte buf allocator . default num heap arena ( ) , num cores ) , math . min ( pooled byte buf allocator . default num direct arena ( ) , allow direct bufs ? num cores : num ) , pooled byte buf allocator . default page size ( ) , pooled byte buf allocator . default max order ( ) , allow cache ? pooled byte buf allocator . default tiny cache size ( ) : num , allow cache ? pooled byte buf allocator . default small cache size ( ) : num , allow cache ? pooled byte buf allocator . default normal cache size ( ) : num , allow cache ? pooled byte buf allocator . default use cache for all threads ( ) : bool ) ; }	Create a pooled ByteBuf allocator but disables the thread-local cache.
public static string create cookie token ( string client user name ) { string buffer sb = new string buffer ( ) ; sb . append ( cookie client user name ) . append ( cookie key value separator ) . append ( client user name ) . append ( cookie attr separator ) ; sb . append ( cookie client rand number ) . append ( cookie key value separator ) . append ( ( new random ( system . current time millis ( ) ) ) . next long ( ) ) ; return sb . to string ( ) ; }	Creates and returns a HS2 cookie token.
public static string get user name from cookie token ( string token str ) { map < string , string > map = split cookie token ( token str ) ; if ( ! map . key set ( ) . equals ( cookie attributes ) ) { log . error ( str + token str ) ; return null ; } return map . get ( cookie client user name ) ; }	Parses a cookie token to retrieve client user name.
private static map < string , string > split cookie token ( string token str ) { map < string , string > map = new hash map < string , string > ( ) ; string tokenizer st = new string tokenizer ( token str , cookie attr separator ) ; while ( st . has more tokens ( ) ) { string part = st . next token ( ) ; int separator = part . index of ( cookie key value separator ) ; if ( separator == - num ) { log . error ( str + token str ) ; return null ; } string key = part . substring ( num , separator ) ; string value = part . substring ( separator + num ) ; map . put ( key , value ) ; } return map ; }	Splits the cookie token into attributes pairs.
protected void validate fetch orientation ( fetch orientation orientation , enum set < fetch orientation > supported orientations ) throws sql { if ( ! supported orientations . contains ( orientation ) ) { throw new sql ( str + orientation . to string ( ) + str , str ) ; } }	Verify if the given fetch orientation is part of the supported orientation types.
public static byte buffer allocate direct buffer ( int size ) { try { if ( cleaner create method == null ) {	Allocate a DirectByteBuffer, potentially bypassing the JVM's MaxDirectMemorySize limit.
public static void write to memory ( byte [ ] src , object target , long target offset ) { platform . copy memory ( src , platform . byte array offset , target , target offset , src . length ) ; }	Writes the content of a byte array into a memory address, identified by an object and anoffset.
public static off heap column vector [ ] allocate columns ( int capacity , struct field [ ] fields ) { off heap column vector [ ] vectors = new off heap column vector [ fields . length ] ; for ( int i = num ; i < fields . length ; i ++ ) { vectors [ i ] = new off heap column vector ( capacity , fields [ i ] . data type ( ) ) ; } return vectors ; }	Allocates columns to store elements of each field off heap.Capacity is the initial capacity of the vector and it will grow as necessary.
@ override public int put byte array ( int row id , byte [ ] value , int offset , int length ) { int result = array data ( ) . append bytes ( length , value , offset ) ; platform . put int ( null , length data + num * row id , length ) ; platform . put int ( null , offset data + num * row id , result ) ; return result ; }	APIs dealing with ByteArrays.
@ override protected void reserve internal ( int new capacity ) { int old capacity = ( nulls == num ) ? num : capacity ; if ( is array ( ) || type instanceof map type ) { this . length data = platform . reallocate memory ( length data , old capacity * num , new capacity * num ) ; this . offset data = platform . reallocate memory ( offset data , old capacity * num , new capacity * num ) ; } else if ( type instanceof byte type || type instanceof boolean type ) { this . data = platform . reallocate memory ( data , old capacity , new capacity ) ; } else if ( type instanceof short type ) { this . data = platform . reallocate memory ( data , old capacity * num , new capacity * num ) ; } else if ( type instanceof integer type || type instanceof float type || type instanceof date type || decimal type . is32 bit decimal type ( type ) ) { this . data = platform . reallocate memory ( data , old capacity * num , new capacity * num ) ; } else if ( type instanceof long type || type instanceof double type || decimal type . is64 bit decimal type ( type ) || type instanceof timestamp type ) { this . data = platform . reallocate memory ( data , old capacity * num , new capacity * num ) ; } else if ( child columns != null ) {	Split out the slow path.
private void init ( int bit width ) { preconditions . check argument ( bit width >= num && bit width <= num , str ) ; this . bit width = bit width ; this . bytes width = bytes utils . padded byte count from bits ( bit width ) ; this . packer = packer . little endian . new byte packer ( bit width ) ; }	Initializes the internal state for decoding ints of `bitWidth`.
@ override public void read integers ( int total , writable column vector c , int row id ) { int left = total ; while ( left > num ) { if ( this . current count == num ) this . read next group ( ) ; int n = math . min ( left , this . current count ) ; switch ( mode ) { case rle : c . put ints ( row id , n , current value ) ; break ; case packed : c . put ints ( row id , n , current buffer , current buffer idx ) ; current buffer idx += n ; break ; } row id += n ; left -= n ; current count -= n ; } }	Since this is only used to decode dictionary IDs, only decoding integers is supported.
private int read unsigned var int ( ) throws io { int value = num ; int shift = num ; int b ; do { b = in . read ( ) ; value |= ( b & num ) << shift ; shift += num ; } while ( ( b & num ) != num ) ; return value ; }	Reads the next varint encoded int.
private int read int little endian padded on bit width ( ) throws io { switch ( bytes width ) { case num : return num ; case num : return in . read ( ) ; case num : { int ch2 = in . read ( ) ; int ch1 = in . read ( ) ; return ( ch1 << num ) + ch2 ; } case num : { int ch3 = in . read ( ) ; int ch2 = in . read ( ) ; int ch1 = in . read ( ) ; return ( ch1 << num ) + ( ch2 << num ) + ( ch3 << num ) ; } case num : { return read int little endian ( ) ; } } throw new runtime exception ( str ) ; }	Reads the next byteWidth little endian int.
private void read next group ( ) { try { int header = read unsigned var int ( ) ; this . mode = ( header & num ) == num ? mode . rle : mode . packed ; switch ( mode ) { case rle : this . current count = header > > > num ; this . current value = read int little endian padded on bit width ( ) ; return ; case packed : int num groups = header > > > num ; this . current count = num groups * num ; if ( this . current buffer . length < this . current count ) { this . current buffer = new int [ this . current count ] ; } current buffer idx = num ; int value index = num ; while ( value index < this . current count ) {	Reads the next group.
private void change state ( service . state new state ) { state = new state ;	Change to a new state and notify all listeners.This is a private method that is only invoked from synchronized methods,which avoid having to clone the listener list.
public long array allocate array ( long size ) { long required = size * num ; memory block page = task memory manager . allocate page ( required , this ) ; if ( page == null || page . size ( ) < required ) { throw oom ( page , required ) ; } used += required ; return new long array ( page ) ; }	Allocates a LongArray of `size`.
protected memory block allocate page ( long required ) { memory block page = task memory manager . allocate page ( math . max ( page size , required ) , this ) ; if ( page == null || page . size ( ) < required ) { throw oom ( page , required ) ; } used += page . size ( ) ; return page ; }	Allocate a memory block with at least `required` bytes.
@ override public long transfer to ( final writable byte channel target , final long position ) throws io { preconditions . check argument ( position == total bytes transferred , str ) ;	This code is more complicated than you would think because we might require multipletransferTo invocations in order to transfer a single MessageWithHeader to avoid busy waiting.The contract is that the caller will ensure position is properly set to the total numberof bytes transferred so far (i.e.
public void point to ( object base object , long base offset , int size in bytes ) { assert num fields >= num : str + num fields + str ; assert size in bytes % num == num : str + size in bytes + str ; this . base object = base object ; this . base offset = base offset ; this . size in bytes = size in bytes ; }	Update this UnsafeRow to point to different backing data.
@ override public void set decimal ( int ordinal , decimal value , int precision ) { assert index is valid ( ordinal ) ; if ( precision <= decimal . max long digits ( ) ) {	Updates the decimal column.Note: In order to support update a decimal with precision > 18, CAN NOT callsetNullAt() for this column.
@ override public unsafe row copy ( ) { unsafe row row copy = new unsafe row ( num fields ) ; final byte [ ] row data copy = new byte [ size in bytes ] ; platform . copy memory ( base object , base offset , row data copy , platform . byte array offset , size in bytes ) ; row copy . point to ( row data copy , platform . byte array offset , size in bytes ) ; return row copy ; }	Copies this row, returning a self-contained UnsafeRow that stores its data in an internalbyte array rather than referencing data stored in a data page.
public static unsafe row create from byte array ( int num bytes , int num fields ) { final unsafe row row = new unsafe row ( num fields ) ; row . point to ( new byte [ num bytes ] , num bytes ) ; return row ; }	Creates an empty UnsafeRow from a byte array with specified numBytes and numFields.The returned row is invalid until we call copyFrom on it.
public void write to stream ( output stream out , byte [ ] write buffer ) throws io { if ( base object instanceof byte [ ] ) { int offset in byte array = ( int ) ( base offset - platform . byte array offset ) ; out . write ( ( byte [ ] ) base object , offset in byte array , size in bytes ) ; }	Write this UnsafeRow's underlying bytes to the given OutputStream.
@ override public void do bootstrap ( transport client client , channel channel ) { spark sasl client sasl client = new spark sasl client ( app id , secret key holder , conf . sasl encryption ( ) ) ; try { byte [ ] payload = sasl client . first token ( ) ; while ( ! sasl client . is complete ( ) ) { sasl message msg = new sasl message ( app id , payload ) ; byte buf buf = unpooled . buffer ( msg . encoded length ( ) + ( int ) msg . body ( ) . size ( ) ) ; msg . encode ( buf ) ; buf . write bytes ( msg . body ( ) . nio byte buffer ( ) ) ; byte buffer response = client . send rpc sync ( buf . nio buffer ( ) , conf . auth rt ( ) ) ; payload = sasl client . response ( java utils . buffer to array ( response ) ) ; } client . set client id ( app id ) ; if ( conf . sasl encryption ( ) ) { if ( ! spark sasl server . qop auth conf . equals ( sasl client . get negotiated property ( sasl . qop ) ) ) { throw new runtime exception ( new sasl exception ( str ) ) ; } sasl encryption . add to channel ( channel , sasl client , conf . max sasl encrypted block size ( ) ) ; sasl client = null ; logger . debug ( str , client ) ; } } catch ( io ioe ) { throw new runtime exception ( ioe ) ; } finally { if ( sasl client != null ) { try {	Performs SASL authentication by sending a token, and then proceeding with the SASLchallenge-response tokens until we either successfully authenticate or throw an exceptiondue to mismatch.
session handle get session handle ( t req , t res ) throws sql , login exception , io { string user name = get user name ( req ) ; string ip address = get ip address ( ) ; t protocol = get min version ( cli . server version , req . get client protocol ( ) ) ; session handle session handle ; if ( cli service . get hive conf ( ) . get bool var ( conf vars . hive serve enable doas ) && ( user name != null ) ) { string delegation token str = get delegation token ( user name ) ; session handle = cli service . open session with impersonation ( protocol , user name , req . get password ( ) , ip address , req . get configuration ( ) , delegation token str ) ; } else { session handle = cli service . open session ( protocol , user name , req . get password ( ) , ip address , req . get configuration ( ) ) ; } res . set server protocol version ( protocol ) ; return session handle ; }	Create a session handle.
private string get proxy user ( string real user , map < string , string > session conf , string ip address ) throws sql { string proxy user = null ;	If the proxy user name is provided then check privileges to substitute the user.
public boolean get boolean ( string key , boolean default value ) { string value = get ( key ) ;	Returns the boolean value to which the specified key is mapped,or defaultValue if there is no mapping for the key.
public double get double ( string key , double default value ) { string value = get ( key ) ; return value == null ? default value : double . parse double ( value ) ; }	Returns the double value to which the specified key is mapped,or defaultValue if there is no mapping for the key.
boolean set ( long index ) { if ( ! get ( index ) ) { data [ ( int ) ( index > > > num ) ] |= ( num << index ) ; bit count ++ ; return bool ; } return bool ; }	Returns true if the bit changed value.
void put all ( bit array array ) { assert data . length == array . data . length : str ; long bit count = num ; for ( int i = num ; i < data . length ; i ++ ) { data [ i ] |= array . data [ i ] ; bit count += long . bit count ( data [ i ] ) ; } this . bit count = bit count ; }	Combines the two BitArrays using bitwise OR.
public rpc handler do bootstrap ( channel channel , rpc handler rpc handler ) { return new sasl rpc handler ( conf , channel , rpc handler , secret key holder ) ; }	Wrap the given application handler in a SaslRpcHandler that will handle the initial SASLnegotiation.
public shuffle index record get index ( int reduce id ) { long offset = offsets . get ( reduce id ) ; long next offset = offsets . get ( reduce id + num ) ; return new shuffle index record ( offset , next offset - offset ) ; }	Get index offset for a particular reducer.
private void grow ( int needed size ) { if ( needed size > array max - total size ( ) ) { throw new unsupported operation exception ( str + needed size + str + str + array max ) ; } final int length = total size ( ) + needed size ; if ( buffer . length < length ) { int new length = length < array max / num ? length * num : array max ; final byte [ ] tmp = new byte [ new length ] ; platform . copy memory ( buffer , platform . byte array offset , tmp , platform . byte array offset , total size ( ) ) ; buffer = tmp ; } }	Grows the buffer by at least `neededSize`.
public int connection timeout ms ( ) { long default network timeout s = java utils . time string as sec ( conf . get ( str , str ) ) ; long default timeout ms = java utils . time string as sec ( conf . get ( spark network io connectiontimeout key , default network timeout s + str ) ) * num ; return ( int ) default timeout ms ; }	Connect timeout in milliseconds.
list < string > build java command ( string extra class path ) throws io { list < string > cmd = new array list < > ( ) ; string [ ] candidate java homes = new string [ ] { java home , child env . get ( str ) , system . getenv ( str ) , system . get property ( str ) } ; for ( string java home : candidate java homes ) { if ( java home != null ) { cmd . add ( join ( file . separator , java home , str , str ) ) ; break ; } }	Builds a list of arguments to run java.This method finds the java executable to use and appends JVM-specific options for running aclass with Spark in the classpath.
private void add to class path ( set < string > cp , string entries ) { if ( is empty ( entries ) ) { return ; } string [ ] split = entries . split ( pattern . quote ( file . path separator ) ) ; for ( string entry : split ) { if ( ! is empty ( entry ) ) { if ( new file ( entry ) . is directory ( ) && ! entry . ends with ( file . separator ) ) { entry += file . separator ; } cp . add ( entry ) ; } } }	Adds entries to the classpath.
private properties load properties file ( ) throws io { properties props = new properties ( ) ; file props file ; if ( properties file != null ) { props file = new file ( properties file ) ; check argument ( props file . is file ( ) , str , properties file ) ; } else { props file = new file ( get conf dir ( ) , default properties file ) ; } if ( props file . is file ( ) ) { try ( input stream reader isr = new input stream reader ( new file input stream ( props file ) , standard charsets . utf 8 ) ) { props . load ( isr ) ; for ( map . entry < object , object > e : props . entry set ( ) ) { e . set value ( e . get value ( ) . to string ( ) . trim ( ) ) ; } } } return props ; }	Loads the configuration file for the application, if it exists.
@ override public void initialize ( input split input split , task attempt context task attempt context ) throws io { file split file split = ( file split ) input split ; configuration conf = task attempt context . get configuration ( ) ; reader reader = orc file . create reader ( file split . get path ( ) , orc file . reader options ( conf ) . max length ( orc conf . max file length . get long ( conf ) ) . filesystem ( file split . get path ( ) . get file system ( conf ) ) ) ; reader . options options = orc input format . build options ( conf , reader , file split . get start ( ) , file split . get length ( ) ) ; record reader = reader . rows ( options ) ; }	Initialize ORC file reader and batch record reader.Please note that `initBatch` is needed to be called after this.
public void init batch ( type description orc schema , struct field [ ] required fields , int [ ] requested data col ids , int [ ] requested partition col ids , internal row partition values ) { wrap = new vectorized row batch wrap ( orc schema . create row batch ( capacity ) ) ; assert ( ! wrap . batch ( ) . selected in use ) ;	Initialize columnar batch by setting required schema and partition information.With this information, this creates ColumnarBatch with the full schema.
private boolean next batch ( ) throws io { record reader . next batch ( wrap . batch ( ) ) ; int batch size = wrap . batch ( ) . size ; if ( batch size == num ) { return bool ; } columnar batch . set num rows ( batch size ) ; for ( int i = num ; i < required fields . length ; i ++ ) { if ( requested data col ids [ i ] != - num ) { ( ( orc column vector ) orc vector wrappers [ i ] ) . set batch size ( batch size ) ; } } return bool ; }	Return true if there exists more data in the next batch.
public static long parse second nano ( string second nano ) throws illegal argument exception { string [ ] parts = second nano . split ( str ) ; if ( parts . length == num ) { return to long with range ( str , parts [ num ] , long . min value / micros per second , long . max value / micros per second ) * micros per second ; } else if ( parts . length == num ) { long seconds = parts [ num ] . equals ( str ) ? num : to long with range ( str , parts [ num ] , long . min value / micros per second , long . max value / micros per second ) ; long nanos = to long with range ( str , parts [ num ] , num , num ) ; return seconds * micros per second + nanos / num ; } else { throw new illegal argument exception ( str ) ; } }	Parse second_nano string in ss.nnnnnnnnn format to microseconds.
public void add to channel ( channel ch ) throws io { ch . pipeline ( ) . add first ( encryption handler name , new encryption handler ( this ) ) . add first ( decryption handler name , new decryption handler ( this ) ) ; }	Add handlers to channel.
@ override public void close ( ) {	Close all connections in the connection pool, and shutdown the worker thread pool.
@ override public int write ( byte buffer src ) { int to transfer = math . min ( src . remaining ( ) , data . length - offset ) ; src . get ( data , offset , to transfer ) ; offset += to transfer ; return to transfer ; }	Reads from the given buffer into the internal byte array.
protected synchronized void release ( boolean user access ) { session state . detach session ( ) ; if ( thread with garbage cleanup . current thread ( ) instanceof thread with garbage cleanup ) { thread with garbage cleanup current thread = ( thread with garbage cleanup ) thread with garbage cleanup . current thread ( ) ; current thread . cache thread local raw store ( ) ; } if ( user access ) { last access time = system . current time millis ( ) ; } if ( op handle set . is empty ( ) ) { last idle time = system . current time millis ( ) ; } else { last idle time = num ; } }	1. We'll remove the ThreadLocal SessionState as this thread might now serveother requests.2. We'll cache the ThreadLocal RawStore object for this background thread for an orderly cleanupwhen this thread is garbage collected later.
private string get user from token ( hive auth factory auth factory , string token str ) throws sql { return auth factory . get user from token ( token str ) ; }	extract the real user from the given token string.
public void set session ugi ( string owner ) throws sql { if ( owner == null ) { throw new sql ( str ) ; } if ( user group information . is security enabled ( ) ) { try { session ugi = user group information . create proxy user ( owner , user group information . get login user ( ) ) ; } catch ( io e ) { throw new sql ( str , e ) ; } } else { session ugi = user group information . create remote user ( owner ) ; } }	setup appropriate UGI for the session.
@ override public void close ( ) throws sql { try { acquire ( bool ) ; cancel delegation token ( ) ; } finally { try { super . close ( ) ; } finally { try { file system . close all for ugi ( session ugi ) ; } catch ( io ioe ) { throw new sql ( str + session ugi , ioe ) ; } } } }	Close the file systems for the session and remove it from the FileSystem cache.Cancel the session's delegation token and close the metastore connection.
private void set delegation token ( string delegation token str ) throws sql { this . delegation token str = delegation token str ; if ( delegation token str != null ) { get hive conf ( ) . set ( str , h token ) ; try { utils . set token str ( session ugi , delegation token str , h token ) ; } catch ( io e ) { throw new sql ( str , e ) ; } } }	Enable delegation token for the sessionsave the token string and set the token.signature in hive conf.
private void cancel delegation token ( ) throws sql { if ( delegation token str != null ) { try { hive . get ( get hive conf ( ) ) . cancel delegation token ( delegation token str ) ; } catch ( hive exception e ) { throw new sql ( str , e ) ; }	If the session has a delegation token obtained from the metastore, then cancel it.
public void release execution memory ( long size , memory consumer consumer ) { logger . debug ( str , task attempt id , utils . bytes to string ( size ) , consumer ) ; memory manager . release execution memory ( size , task attempt id , consumer . get mode ( ) ) ; }	Release N bytes of execution memory for a MemoryConsumer.
public void show memory usage ( ) { logger . info ( str + task attempt id ) ; synchronized ( this ) { long memory accounted for by consumers = num ; for ( memory consumer c : consumers ) { long total mem usage = c . get used ( ) ; memory accounted for by consumers += total mem usage ; if ( total mem usage > num ) { logger . info ( str + c + str + utils . bytes to string ( total mem usage ) ) ; } } long memory not accounted for = memory manager . get execution memory usage for task ( task attempt id ) - memory accounted for by consumers ; logger . info ( str , memory not accounted for , task attempt id ) ; logger . info ( str , memory manager . execution memory used ( ) , memory manager . storage memory used ( ) ) ; } }	Dump the memory usage of all consumers.
public memory block allocate page ( long size , memory consumer consumer ) { assert ( consumer != null ) ; assert ( consumer . get mode ( ) == tungsten memory mode ) ; if ( size > maximum page size bytes ) { throw new too large page exception ( size ) ; } long acquired = acquire execution memory ( size , consumer ) ; if ( acquired <= num ) { return null ; } final int page number ; synchronized ( this ) { page number = allocated pages . next clear bit ( num ) ; if ( page number >= page table size ) { release execution memory ( acquired , consumer ) ; throw new illegal state exception ( str + page table size + str ) ; } allocated pages . set ( page number ) ; } memory block page = null ; try { page = memory manager . tungsten memory allocator ( ) . allocate ( acquired ) ; } catch ( out of memory error e ) { logger . warn ( str , acquired ) ;	Allocate a block of memory that will be tracked in the MemoryManager's page table; this isintended for allocating large blocks of Tungsten memory that will be shared between operators.Returns `null` if there was not enough memory to allocate the page.
public long encode page number and offset ( memory block page , long offset in page ) { if ( tungsten memory mode == memory mode . off heap ) {	Given a memory page and offset within that page, encode this address into a 64-bit long.This address will remain valid as long as the corresponding page has not been freed.
public long clean up all allocated memory ( ) { synchronized ( this ) { for ( memory consumer c : consumers ) { if ( c != null && c . get used ( ) > num ) {	Clean up all allocated memory and pages.
public synchronized byte [ ] response ( byte [ ] token ) { try { return sasl server != null ? sasl server . evaluate response ( token ) : new byte [ num ] ; } catch ( sasl exception e ) { throw throwables . propagate ( e ) ; } }	Used to respond to server SASL tokens.
@ override public synchronized void dispose ( ) { if ( sasl server != null ) { try { sasl server . dispose ( ) ; } catch ( sasl exception e ) {	Disposes of any system resources or security-sensitive information theSaslServer might be using.
private static string get base64 encoded string ( string str ) { byte buf byte buf = null ; byte buf encoded byte buf = null ; try { byte buf = unpooled . wrapped buffer ( str . get bytes ( standard charsets . utf 8 ) ) ; encoded byte buf = base64 . encode ( byte buf ) ; return encoded byte buf . to string ( standard charsets . utf 8 ) ; } finally {	Return a Base64-encoded string.
public static long pack pointer ( long record pointer , int partition id ) { assert ( partition id <= maximum partition id ) ;	Pack a record address and partition id into a single word.
synchronized void dispose ( ) { if ( ! is disposed ( ) ) {	Mark the handle as disposed, and set it as LOST in case the current state is not final.This method should be called only when there's a reasonable expectation that the communicationwith the child application is not needed anymore, either because the code managing the handlehas said so, or because the child application is finished.
public synchronized string get delegation token from meta store ( string owner ) throws sql , unsupported operation exception , login exception , io { if ( ! hive conf . get bool var ( hive conf . conf vars . metastore use thrift sasl ) || ! hive conf . get bool var ( hive conf . conf vars . hive serve enable doas ) ) { throw new unsupported operation exception ( str ) ; } try { hive . close current ( ) ; return hive . get ( hive conf ) . get delegation token ( owner , owner ) ; } catch ( hive exception e ) { if ( e . get cause ( ) instanceof unsupported operation exception ) { throw ( unsupported operation exception ) e . get cause ( ) ; } else { throw new sql ( str , e ) ; } } }	obtain delegation token for the give user from metastore.
public integer get decimal digits ( ) { switch ( this . type ) { case boolean type : case tinyint type : case smallint type : case int type : case bigint type : return num ; case float type : return num ; case double type : return num ; case decimal type : return type qualifiers . get scale ( ) ; case timestamp type : return num ; default : return null ; } }	The number of fractional digits for this type.Null is returned for data types where this is not applicable.
public static < out > iterator < out > collect ( data stream < out > stream ) throws io { type serializer < out > serializer = stream . get type ( ) . create serializer ( stream . get execution environment ( ) . get config ( ) ) ; socket stream iterator < out > iter = new socket stream iterator < out > ( serializer ) ;	Returns an iterator to iterate over the elements of the DataStream.
public command line get command line ( options command line options ) throws exception { final list < string > args = new array list < > ( ) ; properties . as map ( ) . for each ( ( k , v ) -> {	Parses the given command line options from the deployment properties.
public static deployment entry merge ( deployment entry deployment1 , deployment entry deployment2 ) { final map < string , string > merged properties = new hash map < > ( deployment1 . as map ( ) ) ; merged properties . put all ( deployment2 . as map ( ) ) ; final descriptor properties properties = new descriptor properties ( bool ) ; properties . put properties ( merged properties ) ; return new deployment entry ( properties ) ; }	Merges two deployments entries.
public static rpc service create rpc service ( final configuration configuration , final high availability services ha services ) throws exception { check not null ( configuration ) ; check not null ( ha services ) ; final string task manager address = determine task manager bind address ( configuration , ha services ) ; final string port range definition = configuration . get string ( task manager options . rpc port ) ; return akka rpc service utils . create rpc service ( task manager address , port range definition , configuration ) ; }	Create a RPC service for the task manager.
long refresh and get total ( ) { long total = num ; for ( result subpartition part : partition . get all partitions ( ) ) { total += part . unsynchronized get number of queued buffers ( ) ; } return total ; }	Iterates over all sub-partitions and collects the total number of queued buffers in abest-effort way.
int refresh and get min ( ) { int min = integer . max value ; result subpartition [ ] all partitions = partition . get all partitions ( ) ; if ( all partitions . length == num ) {	Iterates over all sub-partitions and collects the minimum number of queued buffers in asub-partition in a best-effort way.
int refresh and get max ( ) { int max = num ; for ( result subpartition part : partition . get all partitions ( ) ) { int size = part . unsynchronized get number of queued buffers ( ) ; max = math . max ( max , size ) ; } return max ; }	Iterates over all sub-partitions and collects the maximum number of queued buffers in asub-partition in a best-effort way.
float refresh and get avg ( ) { long total = num ; result subpartition [ ] all partitions = partition . get all partitions ( ) ; for ( result subpartition part : all partitions ) { int size = part . unsynchronized get number of queued buffers ( ) ; total += size ; } return total / ( float ) all partitions . length ; }	Iterates over all sub-partitions and collects the average number of queued buffers in asub-partition in a best-effort way.
public synchronized url add file ( file local file , string remote file ) throws io , url { return add path ( new path ( local file . to uri ( ) ) , new path ( remote file ) ) ; }	Adds a file to the artifact server.
public synchronized url add path ( path path , path remote file ) throws io , url { if ( paths . contains key ( remote file ) ) { throw new illegal argument exception ( str ) ; } if ( remote file . is absolute ( ) ) { throw new illegal argument exception ( str ) ; } url file url = new url ( base url , remote file . to string ( ) ) ; router . add any ( file url . get path ( ) , new virtual file server handler ( path ) ) ; paths . put ( remote file , file url ) ; return file url ; }	Adds a path to the artifact server.
public synchronized void stop ( ) throws exception { if ( this . server channel != null ) { this . server channel . close ( ) . await uninterruptibly ( ) ; this . server channel = null ; } if ( bootstrap != null ) { if ( bootstrap . group ( ) != null ) { bootstrap . group ( ) . shutdown gracefully ( ) ; } bootstrap = null ; } }	Stops the artifact server.
private object invoke rpc ( method method , object [ ] args ) throws exception { string method name = method . get name ( ) ; class < ? > [ ] parameter types = method . get parameter types ( ) ; annotation [ ] [ ] parameter annotations = method . get parameter annotations ( ) ; time future timeout = extract rpc timeout ( parameter annotations , args , timeout ) ; final rpc invocation rpc invocation = create rpc invocation message ( method name , parameter types , args ) ; class < ? > return type = method . get return type ( ) ; final object result ; if ( objects . equals ( return type , void . type ) ) { tell ( rpc invocation ) ; result = null ; } else {	Invokes a RPC method by sending the RPC invocation details to the rpc endpoint.
protected rpc invocation create rpc invocation message ( final string method name , final class < ? > [ ] parameter types , final object [ ] args ) throws io { final rpc invocation rpc invocation ; if ( is local ) { rpc invocation = new local rpc invocation ( method name , parameter types , args ) ; } else { try { remote rpc invocation remote rpc invocation = new remote rpc invocation ( method name , parameter types , args ) ; if ( remote rpc invocation . get size ( ) > maximum framesize ) { throw new io ( str ) ; } else { rpc invocation = remote rpc invocation ; } } catch ( io e ) { log . warn ( str , e ) ; throw e ; } } return rpc invocation ; }	Create the RpcInvocation message for the given RPC.
protected completable future < ? > ask ( object message , time timeout ) { return future utils . to java ( patterns . ask ( rpc endpoint , message , timeout . to milliseconds ( ) ) ) ; }	Sends the message to the RPC endpoint and returns a future containingits response.
public result unregister reference ( shared state registry key registration key ) { preconditions . check not null ( registration key ) ; final result result ; final stream state handle scheduled state deletion ; shared state registry . shared state entry entry ; synchronized ( registered states ) { entry = registered states . get ( registration key ) ; preconditions . check state ( entry != null , str ) ; entry . decrease reference count ( ) ;	Releases one reference to the given shared state in the registry.
public void register all ( iterable < ? extends composite state handle > state handles ) { if ( state handles == null ) { return ; } synchronized ( registered states ) { for ( composite state handle state handle : state handles ) { state handle . register shared states ( this ) ; } } }	Register given shared states in the registry.
public boolean set canceller handle ( scheduled future < ? > canceller handle ) { synchronized ( lock ) { if ( this . canceller handle == null ) { if ( ! discarded ) { this . canceller handle = canceller handle ; return bool ; } else { return bool ; } } else { throw new illegal state exception ( str ) ; } } }	Sets the handle for the canceller to this pending checkpoint.
public task acknowledge result acknowledge task ( id execution attempt id , task state snapshot operator subtask states , checkpoint metrics metrics ) { synchronized ( lock ) { if ( discarded ) { return task acknowledge result . discarded ; } final execution vertex vertex = not yet acknowledged tasks . remove ( execution attempt id ) ; if ( vertex == null ) { if ( acknowledged tasks . contains ( execution attempt id ) ) { return task acknowledge result . duplicate ; } else { return task acknowledge result . unknown ; } } else { acknowledged tasks . add ( execution attempt id ) ; } list < id > operator i = vertex . get job vertex ( ) . get operator i ( ) ; int subtask index = vertex . get parallel subtask index ( ) ; long ack timestamp = system . current time millis ( ) ; long state size = num ; if ( operator subtask states != null ) { for ( id operator id : operator i ) { operator subtask state operator subtask state = operator subtask states . get subtask state by operator id ( operator id ) ;	Acknowledges the task with the given execution attempt id and the given subtask state.
public void abort ( checkpoint failure reason reason , throwable cause ) { try { checkpoint exception exception = new checkpoint exception ( reason , cause ) ; on completion promise . complete exceptionally ( exception ) ; report failed checkpoint ( exception ) ; assert abort subsumed forced ( reason ) ; } finally { dispose ( bool ) ; } }	Aborts a checkpoint with reason and cause.
private void report failed checkpoint ( exception cause ) {	Reports a failed checkpoint with the given optional cause.
static type information schema to type info ( type description schema ) { switch ( schema . get category ( ) ) { case boolean : return basic type info . boolean type info ; case byte : return basic type info . byte type info ; case short : return basic type info . short type info ; case int : return basic type info . int type info ; case long : return basic type info . long type info ; case float : return basic type info . float type info ; case double : return basic type info . double type info ; case decimal : return basic type info . big dec type info ; case string : case char : case varchar : return basic type info . string type info ; case date : return sql time type info . date ; case timestamp : return sql time type info . timestamp ; case binary : return primitive array type info . byte primitive array type info ; case struct : list < type description > field schemas = schema . get children ( ) ; type information [ ] field types = new type information [ field schemas . size ( ) ] ; for ( int i = num ; i < field schemas . size ( ) ; i ++ ) { field types [ i ] = schema to type info ( field schemas . get ( i ) ) ; } string [ ] field names = schema . get field names ( ) . to array ( new string [ ] { } ) ; return new row type info ( field types , field names ) ; case list : type description element schema = schema . get children ( ) . get ( num ) ; type information < ? > element type = schema to type info ( element schema ) ;	Converts an ORC schema to a Flink TypeInformation.
static int fill rows ( row [ ] rows , type description schema , vectorized row batch batch , int [ ] selected fields ) { int rows to read = math . min ( ( int ) batch . count ( ) , rows . length ) ; list < type description > field types = schema . get children ( ) ;	Fills an ORC batch into an array of Row.
private static void fill column with repeating value ( object [ ] vals , int field idx , object repeating value , int child count ) { if ( field idx == - num ) {	Sets a repeating value to all objects or row fields of the passed vals array.
public static string lpad ( string base , int len , string pad ) { if ( len < num || str . equals ( pad ) ) { return null ; } else if ( len == num ) { return str ; } char [ ] data = new char [ len ] ; char [ ] base chars = base . to char array ( ) ; char [ ] pad chars = pad . to char array ( ) ;	Returns the string str left-padded with the string pad to a length of len characters.If str is longer than len, the return value is shortened to len characters.
public static string rpad ( string base , int len , string pad ) { if ( len < num || str . equals ( pad ) ) { return null ; } else if ( len == num ) { return str ; } char [ ] data = new char [ len ] ; char [ ] base chars = base . to char array ( ) ; char [ ] pad chars = pad . to char array ( ) ; int pos = num ;	Returns the string str right-padded with the string pad to a length of len characters.If str is longer than len, the return value is shortened to len characters.
public static string replace ( string str , string old str , string replacement ) { return str . replace ( old str , replacement ) ; }	Replaces all the old strings with the replacement string.
public static string regexp replace ( string str , string regex , string replacement ) { if ( regex . is empty ( ) ) { return str ; } try {	Returns a string resulting from replacing all substrings that match the regularexpression with replacement.
public static string regexp extract ( string str , string regex , int extract index ) { if ( extract index < num ) { return null ; } try { matcher m = regexp pattern cache . get ( regex ) . matcher ( str ) ; if ( m . find ( ) ) { match result mr = m . to match result ( ) ; return mr . group ( extract index ) ; } return null ; } catch ( exception e ) { log . error ( string . format ( str , str , regex , extract index ) , e ) ; return null ; } }	Returns a string extracted with a specified regular expression and a regexmatch group index.
public static string hash ( string algorithm , string str , string charset name ) { try { byte [ ] digest = message digest . get instance ( algorithm ) . digest ( str to bytes with charset ( str , charset name ) ) ; return encoding utils . hex ( digest ) ; } catch ( no such algorithm exception e ) { throw new illegal argument exception ( str + algorithm , e ) ; } }	Calculate the hash value of a given string.
public static string parse url ( string url str , string part to extract ) { url url ; try { url = url cache . get ( url str ) ; } catch ( exception e ) { log . error ( str + url str , e ) ; return null ; } if ( str . equals ( part to extract ) ) { return url . get host ( ) ; } if ( str . equals ( part to extract ) ) { return url . get path ( ) ; } if ( str . equals ( part to extract ) ) { return url . get query ( ) ; } if ( str . equals ( part to extract ) ) { return url . get ref ( ) ; } if ( str . equals ( part to extract ) ) { return url . get protocol ( ) ; } if ( str . equals ( part to extract ) ) { return url . get file ( ) ; } if ( str . equals ( part to extract ) ) { return url . get authority ( ) ; } if ( str . equals ( part to extract ) ) { return url . get user info ( ) ; } return null ; }	Parse url and return various components of the URL.If accept any null arguments, return null.
public static string parse url ( string url str , string part to extract , string key ) { if ( ! str . equals ( part to extract ) ) { return null ; } string query = parse url ( url str , part to extract ) ; if ( query == null ) { return null ; } pattern p = pattern . compile ( str + pattern . quote ( key ) + str ) ; matcher m = p . matcher ( query ) ; if ( m . find ( ) ) { return m . group ( num ) ; } return null ; }	Parse url and return various parameter of the URL.If accept any null arguments, return null.
public static string hex ( string x ) { return encoding utils . hex ( x . get bytes ( standard charsets . utf 8 ) ) . to upper case ( ) ; }	Returns the hex string of a string argument.
public static map < string , string > str to map ( string text , string list delimiter , string key value delimiter ) { if ( string utils . is empty ( text ) ) { return empty map ; } string [ ] key value pairs = text . split ( list delimiter ) ; map < string , string > ret = new hash map < > ( key value pairs . length ) ; for ( string key value pair : key value pairs ) { string [ ] key value = key value pair . split ( key value delimiter , num ) ; if ( key value . length < num ) { ret . put ( key value pair , null ) ; } else { ret . put ( key value [ num ] , key value [ num ] ) ; } } return ret ; }	Creates a map by parsing text.
@ nonnull public consumer records < byte [ ] , byte [ ] > poll next ( ) throws exception { synchronized ( lock ) { while ( next == null && error == null ) { lock . wait ( ) ; } consumer records < byte [ ] , byte [ ] > n = next ; if ( n != null ) { next = null ; lock . notify all ( ) ; return n ; } else { exception utils . rethrow exception ( error , error . get message ( ) ) ;	Polls the next element from the Handover, possibly blocking until the next element isavailable.
public boolean is compatible with ( dewey number other ) { if ( length ( ) > other . length ( ) ) {	Checks whether this dewey number is compatible to the other dewey number.
public dewey number increase ( int times ) { int [ ] new dewey number = arrays . copy of ( dewey number , dewey number . length ) ; new dewey number [ dewey number . length - num ] += times ; return new dewey number ( new dewey number ) ; }	Creates a new dewey number from this such that its last digit is increased by the suppliednumber.
public dewey number add stage ( ) { int [ ] new dewey number = arrays . copy of ( dewey number , dewey number . length + num ) ; return new dewey number ( new dewey number ) ; }	Creates a new dewey number from this such that a 0 is appended as new last digit.
public static dewey number from string ( final string dewey number string ) { string [ ] splits = dewey number string . split ( str ) ; if ( splits . length == num ) { return new dewey number ( integer . parse int ( dewey number string ) ) ; } else { int [ ] dewey number = new int [ splits . length ] ; for ( int i = num ; i < splits . length ; i ++ ) { dewey number [ i ] = integer . parse int ( splits [ i ] ) ; } return new dewey number ( dewey number ) ; } }	Creates a dewey number from a string representation.
public completable future < job details info > get job details ( id job id ) { final job details headers details headers = job details headers . get instance ( ) ; final job message parameters params = new job message parameters ( ) ; params . job path parameter . resolve ( job id ) ; return send request ( details headers , params ) ; }	Requests the job details.
job submission result finalize execute ( ) throws program invocation exception { return client . run ( detached plan , jar files to attach , classpaths to attach , user code class loader , savepoint settings ) ; }	Finishes this Context Environment's execution by explicitly running the plan constructed.
public static void discard state future ( runnable future < ? extends state object > state future ) throws exception { if ( null != state future ) { if ( ! state future . cancel ( bool ) ) { try {	Discards the given state future by first trying to cancel it.
public static < t > t find ( class < t > factory class , descriptor descriptor ) { preconditions . check not null ( descriptor ) ; return find internal ( factory class , descriptor . to properties ( ) , optional . empty ( ) ) ; }	Finds a table factory of the given class and descriptor.
public static < t > t find ( class < t > factory class , descriptor descriptor , class loader class loader ) { preconditions . check not null ( descriptor ) ; preconditions . check not null ( class loader ) ; return find internal ( factory class , descriptor . to properties ( ) , optional . of ( class loader ) ) ; }	Finds a table factory of the given class, descriptor, and classloader.
public static < t > t find ( class < t > factory class , map < string , string > property map ) { return find internal ( factory class , property map , optional . empty ( ) ) ; }	Finds a table factory of the given class and property map.
private static list < table factory > discover factories ( optional < class loader > class loader ) { try { list < table factory > result = new linked list < > ( ) ; if ( class loader . is present ( ) ) { service loader . load ( table factory . class , class loader . get ( ) ) . iterator ( ) . for each remaining ( result :: add ) ; } else { default loader . iterator ( ) . for each remaining ( result :: add ) ; } return result ; } catch ( service configuration error e ) { log . error ( str , e ) ; throw new table exception ( str , e ) ; } }	Searches for factories using Java service providers.
private static < t > list < table factory > filter by factory class ( class < t > factory class , map < string , string > properties , list < table factory > found factories ) { list < table factory > class factories = found factories . stream ( ) . filter ( p -> factory class . is assignable from ( p . get class ( ) ) ) . collect ( collectors . to list ( ) ) ; if ( class factories . is empty ( ) ) { throw new no matching table factory exception ( string . format ( str , factory class . get canonical name ( ) ) , factory class , found factories , properties ) ; } return class factories ; }	Filters factories with matching context by factory class.
private static < t > list < table factory > filter by context ( class < t > factory class , map < string , string > properties , list < table factory > found factories , list < table factory > class factories ) { list < table factory > matching factories = class factories . stream ( ) . filter ( factory -> { map < string , string > requested context = normalize context ( factory ) ; map < string , string > plain context = new hash map < > ( requested context ) ;	Filters for factories with matching context.
private static map < string , string > normalize context ( table factory factory ) { map < string , string > required context = factory . required context ( ) ; if ( required context == null ) { throw new table exception ( string . format ( str , factory . get class ( ) . get name ( ) ) ) ; } return required context . key set ( ) . stream ( ) . collect ( collectors . to map ( key -> key . to lower case ( ) , key -> required context . get ( key ) ) ) ; }	Prepares the properties of a context to be used for match operations.
private static < t > t filter by supported properties ( class < t > factory class , map < string , string > properties , list < table factory > found factories , list < table factory > class factories ) { final list < string > plain given keys = new linked list < > ( ) ; properties . key set ( ) . for each ( k -> {	Filters the matching class factories by supported properties.
private static tuple2 < list < string > , list < string > > normalize supported properties ( table factory factory ) { list < string > supported properties = factory . supported properties ( ) ; if ( supported properties == null ) { throw new table exception ( string . format ( str , factory . get class ( ) . get name ( ) ) ) ; } list < string > supported keys = supported properties . stream ( ) . map ( p -> p . to lower case ( ) ) . collect ( collectors . to list ( ) ) ;	Prepares the supported properties of a factory to be used for match operations.
@ override public void insert or replace record ( t record ) throws io { if ( closed ) { return ; } t match = prober . get match for ( record , reuse ) ; if ( match == null ) { prober . insert after no match ( record ) ; } else { prober . update match ( record ) ; } }	Searches the hash table for a record with the given key.If it is found, then it is overridden with the specified record.Otherwise, the specified record is inserted.
private void rebuild ( long new num bucket segments ) throws io {	Same as above, but the number of bucket segments of the new table can be specified.
public static stringified accumulator result [ ] stringify accumulator results ( map < string , optional failure < accumulator < ? , ? > > > accs ) { if ( accs == null || accs . is empty ( ) ) { return new stringified accumulator result [ num ] ; } else { stringified accumulator result [ ] results = new stringified accumulator result [ accs . size ( ) ] ; int i = num ; for ( map . entry < string , optional failure < accumulator < ? , ? > > > entry : accs . entry set ( ) ) { results [ i ++ ] = stringify accumulator result ( entry . get key ( ) , entry . get value ( ) ) ; } return results ; } }	Flatten a map of accumulator names to Accumulator instances into an array of StringifiedAccumulatorResult values.
public boolean is matching ( resource profile required ) { if ( required == unknown ) { return bool ; } if ( cpu cores >= required . get cpu cores ( ) && heap memory in mb >= required . get heap memory in mb ( ) && direct memory in mb >= required . get direct memory in mb ( ) && native memory in mb >= required . get native memory in mb ( ) && network memory in mb >= required . get network memory in mb ( ) ) { for ( map . entry < string , resource > resource : required . extended resources . entry set ( ) ) { if ( ! extended resources . contains key ( resource . get key ( ) ) || ! extended resources . get ( resource . get key ( ) ) . get resource aggregate type ( ) . equals ( resource . get value ( ) . get resource aggregate type ( ) ) || extended resources . get ( resource . get key ( ) ) . get value ( ) < resource . get value ( ) . get value ( ) ) { return bool ; } } return bool ; } return bool ; }	Check whether required resource profile can be matched.
private void grow ( int min capacity ) { int old capacity = segment . size ( ) ; int new capacity = old capacity + ( old capacity > > num ) ; if ( new capacity - min capacity < num ) { new capacity = min capacity ; } segment = memory segment factory . wrap ( arrays . copy of ( segment . get array ( ) , new capacity ) ) ; after grow ( ) ; }	Increases the capacity to ensure that it can hold at least theminimum capacity argument.
public void release partitions ( collection < id > partition ids ) { for ( id partition id : partition ids ) { result partition manager . release partition ( partition id , null ) ; } }	Batch release intermediate result partitions.
public static restart strategy configuration fixed delay restart ( int restart attempts , long delay between attempts ) { return fixed delay restart ( restart attempts , time . of ( delay between attempts , time unit . milliseconds ) ) ; }	Generates a FixedDelayRestartStrategyConfiguration.
public static failure rate restart strategy configuration failure rate restart ( int failure rate , time failure interval , time delay interval ) { return new failure rate restart strategy configuration ( failure rate , failure interval , delay interval ) ; }	Generates a FailureRateRestartStrategyConfiguration.
private < t > void deploy job ( execution context < t > context , job graph job graph , result < t > result ) {	Deploys a job. Depending on the deployment creates a new job cluster. It saves the cluster id inthe result and blocks until job completion.
void store initial hash table ( ) throws io { if ( spilled ) { return ;	This method stores the initial hash table's contents on disk if hash join needs the memoryfor further partition processing.The initial hash table is rebuild before a new secondary input is opened.For the sake of simplicity we iterate over all in-memory elements and store them in one file.The file is hashed into memory upon opening a new probe input.
protected boolean check next index offset ( ) { if ( this . current sort index offset > this . last index entry offset ) { memory segment return segment = next memory segment ( ) ; if ( return segment != null ) { this . current sort index segment = return segment ; this . sort index . add ( this . current sort index segment ) ; this . current sort index offset = num ; } else { return bool ; } } return bool ; }	check if we need request next index memory.
protected void write index and normalized key ( base row record , long curr offset ) {	Write of index and normalizedKey.
@ suppress warnings ( { str , str } ) public void collect buffer ( collector < out > c , int buffer size ) throws io { file buffer . position ( num ) ; while ( file buffer . position ( ) < buffer size ) { c . collect ( deserializer . deserialize ( ) ) ; } }	Reads a buffer of the given size from the memory-mapped file, and collects all records contained.
@ deprecated public void init ( map < event id , lockable < v > > events , map < node id , lockable < shared buffer node > > entries ) throws exception { events buffer . put all ( events ) ; this . entries . put all ( entries ) ; map < long , integer > max ids = events . key set ( ) . stream ( ) . collect ( collectors . to map ( event id :: get timestamp , event id :: get id , math :: max ) ) ; events count . put all ( max ids ) ; }	Initializes underlying state with given map of events and entries.
public boolean is empty ( ) throws exception { return iterables . is empty ( events buffer cache . key set ( ) ) && iterables . is empty ( events buffer . keys ( ) ) ; }	Checks if there is no elements in the buffer.
void upsert event ( event id event id , lockable < v > event ) { this . events buffer cache . put ( event id , event ) ; }	Inserts or updates an event in cache.
void upsert entry ( node id node id , lockable < shared buffer node > entry ) { this . entry cache . put ( node id , entry ) ; }	Inserts or updates a shareBufferNode in cache.
void remove event ( event id event id ) throws exception { this . events buffer cache . remove ( event id ) ; this . events buffer . remove ( event id ) ; }	Removes an event from cache and state.
void remove entry ( node id node id ) throws exception { this . entry cache . remove ( node id ) ; this . entries . remove ( node id ) ; }	Removes a ShareBufferNode from cache and state.
lockable < shared buffer node > get entry ( node id node id ) { return entry cache . compute if absent ( node id , id -> { try { return entries . get ( id ) ; } catch ( exception ex ) { throw new wrapping runtime exception ( ex ) ; } } ) ; }	It always returns node either from state or cache.
lockable < v > get event ( event id event id ) { return events buffer cache . compute if absent ( event id , id -> { try { return events buffer . get ( id ) ; } catch ( exception ex ) { throw new wrapping runtime exception ( ex ) ; } } ) ; }	It always returns event either from state or cache.
void flush cache ( ) throws exception { if ( ! entry cache . is empty ( ) ) { entries . put all ( entry cache ) ; entry cache . clear ( ) ; } if ( ! events buffer cache . is empty ( ) ) { events buffer . put all ( events buffer cache ) ; events buffer cache . clear ( ) ; } }	Flush the event and node from cache to state.
public time window cover ( time window other ) { return new time window ( math . min ( start , other . start ) , math . max ( end , other . end ) ) ; }	Returns the minimal window covers both this window and the given window.
@ override protected void compute operator specific default estimates ( data statistics statistics ) { this . estimated num records = get predecessor node ( ) . get estimated num records ( ) ; this . estimated output size = get predecessor node ( ) . get estimated output size ( ) ; }	Computes the estimated outputs for the data sink.
public list < kafka topic partition > discover partitions ( ) throws wakeup exception , closed exception { if ( ! closed && ! wakeup ) { try { list < kafka topic partition > new discovered partitions ;	Execute a partition discovery attempt for this subtask.This method lets the partition discoverer update what partitions it has discovered so far.
public void shutdown ( job status job status ) throws exception { synchronized ( lock ) { if ( ! shutdown ) { shutdown = bool ; log . info ( str , job ) ; periodic scheduling = bool ; trigger request queued = bool ;	Shuts down the checkpoint coordinator.
public completable future < completed checkpoint > trigger savepoint ( final long timestamp , @ nullable final string target location ) { final checkpoint properties properties = checkpoint properties . for savepoint ( ) ; return trigger savepoint internal ( timestamp , properties , bool , target location ) ; }	Triggers a savepoint with the given savepoint directory as a target.
public completable future < completed checkpoint > trigger synchronous savepoint ( final long timestamp , final boolean advance to end of event time , @ nullable final string target location ) { final checkpoint properties properties = checkpoint properties . for sync savepoint ( ) ; return trigger savepoint internal ( timestamp , properties , advance to end of event time , target location ) ; }	Triggers a synchronous savepoint with the given savepoint directory as a target.
public boolean trigger checkpoint ( long timestamp , boolean is periodic ) { try { trigger checkpoint ( timestamp , checkpoint properties , null , is periodic , bool ) ; return bool ; } catch ( checkpoint exception e ) { return bool ; } }	Triggers a new standard checkpoint and uses the given timestamp as the checkpointtimestamp.
private void complete pending checkpoint ( pending checkpoint pending checkpoint ) throws checkpoint exception { final long checkpoint id = pending checkpoint . get checkpoint id ( ) ; final completed checkpoint completed checkpoint ;	Try to complete the given pending checkpoint.
public void fail unacknowledged pending checkpoints for ( id execution attempt id , throwable cause ) { synchronized ( lock ) { iterator < pending checkpoint > pending checkpoint iterator = pending checkpoints . values ( ) . iterator ( ) ; while ( pending checkpoint iterator . has next ( ) ) { final pending checkpoint pending checkpoint = pending checkpoint iterator . next ( ) ; if ( ! pending checkpoint . is acknowledged by ( execution attempt id ) ) { pending checkpoint iterator . remove ( ) ; discard checkpoint ( pending checkpoint , cause ) ; } } } }	Fails all pending checkpoints which have not been acknowledged by the given executionattempt id.
private void trigger queued requests ( ) { if ( trigger request queued ) { trigger request queued = bool ;	Triggers the queued request, if there is one. NOTE: The caller of this method must hold the lock when invoking the method!.
public boolean restore savepoint ( string savepoint pointer , boolean allow non restored , map < id , execution job vertex > tasks , class loader user class loader ) throws exception { preconditions . check not null ( savepoint pointer , str ) ; log . info ( str , job , savepoint pointer , ( allow non restored ? str : str ) ) ; final completed checkpoint storage location checkpoint location = checkpoint storage . resolve checkpoint ( savepoint pointer ) ;	Restore the state with given savepoint.
public void abort pending checkpoints ( checkpoint exception exception ) { synchronized ( lock ) { for ( pending checkpoint p : pending checkpoints . values ( ) ) { p . abort ( exception . get checkpoint failure reason ( ) ) ; } pending checkpoints . clear ( ) ; } }	Aborts all the pending checkpoints due to en exception.
private void discard subtask state ( final id job id , final id execution attempt id , final long checkpoint id , final task state snapshot subtask state ) { if ( subtask state != null ) { executor . execute ( new runnable ( ) { @ override public void run ( ) { try { subtask state . discard state ( ) ; } catch ( throwable t2 ) { log . warn ( str + str , checkpoint id , execution attempt id , job id , t2 ) ; } } } ) ; } }	Discards the given state object asynchronously belonging to the given job, execution attemptid and checkpoint id.
public resource spec merge ( resource spec other ) { resource spec target = new resource spec ( math . max ( this . cpu cores , other . cpu cores ) , this . heap memory in mb + other . heap memory in mb , this . direct memory in mb + other . direct memory in mb , this . native memory in mb + other . native memory in mb , this . state size in mb + other . state size in mb ) ; target . extended resources . put all ( extended resources ) ; for ( resource resource : other . extended resources . values ( ) ) { target . extended resources . merge ( resource . get name ( ) , resource , ( v1 , v2 ) -> v1 . merge ( v2 ) ) ; } return target ; }	Used by system internally to merge the other resources of chained operatorswhen generating the job graph or merge the resource consumed by state backend.
public boolean is valid ( ) { if ( this . cpu cores >= num && this . heap memory in mb >= num && this . direct memory in mb >= num && this . native memory in mb >= num && this . state size in mb >= num ) { for ( resource resource : extended resources . values ( ) ) { if ( resource . get value ( ) < num ) { return bool ; } } return bool ; } else { return bool ; } }	Check whether all the field values are valid.
public void add unique field ( field set unique field set ) { if ( this . unique fields == null ) { this . unique fields = new hash set < field set > ( ) ; } this . unique fields . add ( unique field set ) ; }	Adds a FieldSet to be unique.
public void add unique field ( int field ) { if ( this . unique fields == null ) { this . unique fields = new hash set < field set > ( ) ; } this . unique fields . add ( new field set ( field ) ) ; }	Adds a field as having only unique values.
public void add unique fields ( set < field set > unique field sets ) { if ( this . unique fields == null ) { this . unique fields = new hash set < field set > ( ) ; } this . unique fields . add all ( unique field sets ) ; }	Adds multiple FieldSets to be unique.
public static descriptor properties normalize yaml ( map < string , object > yaml map ) { final map < string , string > normalized = new hash map < > ( ) ; yaml map . for each ( ( k , v ) -> normalize yaml object ( normalized , k , v ) ) ; final descriptor properties properties = new descriptor properties ( bool ) ; properties . put properties ( normalized ) ; return properties ; }	Normalizes key-value properties from Yaml in the normalized format of the Table API.
void retrigger subpartition request ( timer timer , final int subpartition index ) { synchronized ( request lock ) { check state ( subpartition view == null , str ) ; timer . schedule ( new timer task ( ) { @ override public void run ( ) { try { request subpartition ( subpartition index ) ; } catch ( throwable t ) { set error ( t ) ; } } } , get current backoff ( ) ) ; } }	Retriggers a subpartition request.
public static string format address ( int address ) { int b1 = ( address > > > num ) & num ; int b2 = ( address > > > num ) & num ; int b3 = ( address > > > num ) & num ; int b4 = address & num ; return str + b1 + str + b2 + str + b3 + str + b4 ; }	Util method to create a string representation of a 32 bit integer representingan IPv4 address.
public python data stream from collection ( iterator < object > iter ) throws exception { return new python data stream < > ( env . add source ( new python iterator function ( iter ) , type extractor . get for class ( object . class ) ) . map ( new adapter map < > ( ) ) ) ; }	Creates a python data stream from the given iterator.
public void seek ( long block , long record in block ) throws io { list < block meta data > block meta data = reader . get row groups ( ) ; if ( block == - num && record in block == - num ) {	Moves the reading position to the given block and seeks to and reads the given record.
public tuple2 < long , long > get current read position ( ) {	Returns the current read position in the split, i.e., the current block andthe number of records that were returned from that block.
public boolean reach end ( ) throws io {	Checks if the record reader returned all records.This method must be called before a record can be returned.
private boolean read next record ( ) throws io { boolean record found = bool ; while ( ! record found ) {	Reads the next record.
public void shutdown ( ) { synchronized ( global lock ) { for ( instance i : all instances ) { i . remove slot listener ( ) ; i . cancel and release all slots ( ) ; } all instances . clear ( ) ; all instances by host . clear ( ) ; instances with available resources . clear ( ) ; task queue . clear ( ) ; } }	Shuts the scheduler down.
public string format ( description description ) { for ( block element block element : description . get blocks ( ) ) { block element . format ( this ) ; } return finalize formatting ( ) ; }	Formats the description into a String using format specific tags.
@ override protected shard consumer create shard consumer ( integer subscribed shard state index , stream shard handle handle , sequence number last seq num , shard metrics reporter shard metrics reporter ) { return new shard consumer ( this , subscribed shard state index , handle , last seq num , db . create ( get consumer configuration ( ) ) , shard metrics reporter ) ; }	Create a new DynamoDB streams shard consumer.
private void open cli ( session context context , executor executor ) { cli client cli = null ; try { cli = new cli client ( context , executor ) ;	Opens the CLI client for executing SQL statements.
public static amazon kinesis create kinesis client ( properties config props , client configuration aws client config ) {	Creates an Amazon Kinesis Client.
private static aws get credentials provider ( final properties config props , final string config prefix ) { credential provider credential provider type ; if ( ! config props . contains key ( config prefix ) ) { if ( config props . contains key ( aws . access key id ( config prefix ) ) && config props . contains key ( aws . secret key ( config prefix ) ) ) {	If the provider is ASSUME_ROLE, then the credentials for assuming this role are determinedrecursively.
public static boolean is valid region ( string region ) { try { regions . from name ( region . to lower case ( ) ) ; } catch ( illegal argument exception e ) { return bool ; } return bool ; }	Checks whether or not a region ID is valid.
public static long next power of two ( long x ) { if ( x == num ) { return num ; } else { -- x ; x |= x > > num ; x |= x > > num ; x |= x > > num ; x |= x > > num ; x |= x > > num ; return ( x | x > > num ) + num ; } }	Return the least power of two greater than or equal to the specified value.
public static int max fill ( int n , float f ) { return math . min ( ( int ) math . ceil ( ( double ) ( ( float ) n * f ) ) , n - num ) ; }	Returns the maximum number of entries that can be filled before rehashing.
@ override public void recover ( ) throws exception { log . info ( str ) ;	Gets the latest checkpoint from ZooKeeper and removes all others.
@ override public void add checkpoint ( final completed checkpoint checkpoint ) throws exception { check not null ( checkpoint , str ) ; final string path = checkpoint id to path ( checkpoint . get checkpoint id ( ) ) ;	Synchronously writes the new checkpoints to ZooKeeper and asynchronously removes older ones.
public static long path to checkpoint id ( string path ) { try { string number string ;	Converts a path to the checkpoint id.
private void check and propagate async error ( ) throws exception { if ( thrown exception != null ) { string error messages = str ; if ( thrown exception instanceof user record failed exception ) { list < attempt > attempts = ( ( user record failed exception ) thrown exception ) . get result ( ) . get attempts ( ) ; for ( attempt attempt : attempts ) { if ( attempt . get error message ( ) != null ) { error messages += attempt . get error message ( ) + str ; } } } if ( fail on error ) { throw new runtime exception ( str + error messages , thrown exception ) ; } else { log . warn ( str , thrown exception , error messages ) ;	Check if there are any asynchronous exceptions.
public static queryable state configuration disabled ( ) { final iterator < integer > proxy ports = net utils . get port range from string ( queryable state options . proxy port range . default value ( ) ) ; final iterator < integer > server ports = net utils . get port range from string ( queryable state options . server port range . default value ( ) ) ; return new queryable state configuration ( proxy ports , server ports , num , num , num , num ) ; }	Gets the configuration describing the queryable state as deactivated.
public void add broadcast set for scatter function ( string name , data set < ? > data ) { this . bc vars scatter . add ( new tuple2 < > ( name , data ) ) ; }	Adds a data set as a broadcast set to the scatter function.
public void add broadcast set for gather function ( string name , data set < ? > data ) { this . bc vars gather . add ( new tuple2 < > ( name , data ) ) ; }	Adds a data set as a broadcast set to the gather function.
public list < map < string , list < event id > > > extract patterns ( final node id node id , final dewey number version ) { list < map < string , list < event id > > > result = new array list < > ( ) ;	Returns all elements from the previous relation starting at the given entry.
public map < string , list < v > > materialize match ( map < string , list < event id > > match ) { map < string , list < v > > materialized match = new linked hash map < > ( match . size ( ) ) ; for ( map . entry < string , list < event id > > pattern : match . entry set ( ) ) { list < v > events = new array list < > ( pattern . get value ( ) . size ( ) ) ; for ( event id event id : pattern . get value ( ) ) { try { v event = shared buffer . get event ( event id ) . get element ( ) ; events . add ( event ) ; } catch ( exception ex ) { throw new wrapping runtime exception ( ex ) ; } } materialized match . put ( pattern . get key ( ) , events ) ; } return materialized match ; }	Extracts the real event from the sharedBuffer with pre-extracted eventId.
public void lock node ( final node id node ) { lockable < shared buffer node > shared buffer node = shared buffer . get entry ( node ) ; if ( shared buffer node != null ) { shared buffer node . lock ( ) ; shared buffer . upsert entry ( node , shared buffer node ) ; } }	Increases the reference counter for the given entry so that it is notaccidentally removed.
public void release node ( final node id node ) throws exception { lockable < shared buffer node > shared buffer node = shared buffer . get entry ( node ) ; if ( shared buffer node != null ) { if ( shared buffer node . release ( ) ) { remove node ( node , shared buffer node . get element ( ) ) ; } else { shared buffer . upsert entry ( node , shared buffer node ) ; } } }	Decreases the reference counter for the given entry so that it can beremoved once the reference counter reaches 0.
private void lock event ( event id event id ) { lockable < v > event wrapper = shared buffer . get event ( event id ) ; check state ( event wrapper != null , str , event id ) ; event wrapper . lock ( ) ; shared buffer . upsert event ( event id , event wrapper ) ; }	Increases the reference counter for the given event so that it is notaccidentally removed.
public void release event ( event id event id ) throws exception { lockable < v > event wrapper = shared buffer . get event ( event id ) ; if ( event wrapper != null ) { if ( event wrapper . release ( ) ) { shared buffer . remove event ( event id ) ; } else { shared buffer . upsert event ( event id , event wrapper ) ; } } }	Decreases the reference counter for the given event so that it can beremoved once the reference counter reaches 0.
protected void stop ( string [ ] args ) throws exception { log . info ( str ) ; final options command options = cli frontend parser . get stop command options ( ) ; final options command line options = cli frontend parser . merge options ( command options , custom command line options ) ; final command line command line = cli frontend parser . parse ( command line options , args , bool ) ; final stop options stop options = new stop options ( command line ) ; if ( stop options . is print help ( ) ) { cli frontend parser . print help for stop ( custom command lines ) ; return ; } final string [ ] cleaned args = stop options . get args ( ) ; final string target directory = stop options . has savepoint flag ( ) && cleaned args . length > num ? stop options . get target directory ( ) : null ;	Executes the STOP action.
protected void cancel ( string [ ] args ) throws exception { log . info ( str ) ; final options command options = cli frontend parser . get cancel command options ( ) ; final options command line options = cli frontend parser . merge options ( command options , custom command line options ) ; final command line command line = cli frontend parser . parse ( command line options , args , bool ) ; cancel options cancel options = new cancel options ( command line ) ;	Executes the CANCEL action.
protected void savepoint ( string [ ] args ) throws exception { log . info ( str ) ; final options command options = cli frontend parser . get savepoint command options ( ) ; final options command line options = cli frontend parser . merge options ( command options , custom command line options ) ; final command line command line = cli frontend parser . parse ( command line options , args , bool ) ; final savepoint options savepoint options = new savepoint options ( command line ) ;	Executes the SAVEPOINT action.
private string trigger savepoint ( cluster client < ? > cluster client , id job id , string savepoint directory ) throws flink exception { log and sysout ( str + job id + str ) ; completable future < string > savepoint path future = cluster client . trigger savepoint ( job id , savepoint directory ) ; log and sysout ( str ) ; final string savepoint path ; try { savepoint path = savepoint path future . get ( ) ; } catch ( exception e ) { throwable cause = exception utils . strip execution exception ( e ) ; throw new flink exception ( str + job id + str , cause ) ; } log and sysout ( str + savepoint path ) ; log and sysout ( str ) ; return savepoint path ; }	Sends a SavepointTriggerMessage to the job manager.
private void dispose savepoint ( cluster client < ? > cluster client , string savepoint path ) throws flink exception { preconditions . check not null ( savepoint path , str + str ) ; log and sysout ( str + savepoint path + str ) ; final completable future < acknowledge > dispose future = cluster client . dispose savepoint ( savepoint path ) ; log and sysout ( str ) ; try { dispose future . get ( client timeout . to millis ( ) , time unit . milliseconds ) ; } catch ( exception e ) { throw new flink exception ( str + savepoint path + str , e ) ; } log and sysout ( str + savepoint path + str ) ; }	Sends a SavepointDisposalRequest to the job manager.
packaged program build program ( program options options ) throws file not found exception , program invocation exception { string [ ] program args = options . get program args ( ) ; string jar file path = options . get jar file path ( ) ; list < url > classpaths = options . get classpaths ( ) ; if ( jar file path == null ) { throw new illegal argument exception ( str ) ; } file jar file = new file ( jar file path ) ;	Creates a Packaged program from the given command line options.
private static int handle parametrization exception ( program parametrization exception e ) { log . error ( str , e ) ; system . err . println ( e . get message ( ) ) ; return num ; }	Displays an optional exception message for incorrect program parametrization.
private static int handle error ( throwable t ) { log . error ( str , t ) ; system . err . println ( ) ; system . err . println ( str ) ; system . err . println ( str ) ; system . err . println ( ) ; if ( t . get cause ( ) instanceof invalid program exception ) { system . err . println ( t . get cause ( ) . get message ( ) ) ; stack trace element [ ] trace = t . get cause ( ) . get stack trace ( ) ; for ( stack trace element ele : trace ) { system . err . println ( str + ele ) ; if ( ele . get method name ( ) . equals ( str ) ) { break ; } } } else { t . print stack trace ( ) ; } return num ; }	Displays an exception message.
public static void main ( final string [ ] args ) { environment information . log environment info ( log , str , args ) ;	Submits the job based on the arguments.
static void set job manager address in config ( configuration config , inet socket address address ) { config . set string ( job manager options . address , address . get host string ( ) ) ; config . set integer ( job manager options . port , address . get port ( ) ) ; config . set string ( rest options . address , address . get host string ( ) ) ; config . set integer ( rest options . port , address . get port ( ) ) ; }	Writes the given job manager address to the associated configuration object.
public custom command line < ? > get active custom command line ( command line command line ) { for ( custom command line < ? > cli : custom command lines ) { if ( cli . is active ( command line ) ) { return cli ; } } throw new illegal state exception ( str ) ; }	Gets the custom command-line for the arguments.
private static custom command line < ? > load custom command line ( string class name , object ... params ) throws illegal access exception , invocation target exception , instantiation exception , class not found exception , no such method exception { class < ? extends custom command line > custom cli class = class . for name ( class name ) . as subclass ( custom command line . class ) ;	Loads a class from the classpath that implements the CustomCommandLine interface.
private static job vertex back pressure info . vertex back pressure level get back pressure level ( double back pressure ratio ) { if ( back pressure ratio <= num ) { return job vertex back pressure info . vertex back pressure level . ok ; } else if ( back pressure ratio <= num ) { return job vertex back pressure info . vertex back pressure level . low ; } else { return job vertex back pressure info . vertex back pressure level . high ; } }	Returns the back pressure level as a String.
public boolean schema equals ( object obj ) { return equals ( obj ) && arrays . equals ( field names , ( ( row type info ) obj ) . field names ) ; }	Tests whether an other object describes the same, schema-equivalent row information.
@ override public void discard state ( ) throws exception { file system fs = get file system ( ) ; fs . delete ( file path , bool ) ; }	Discard the state by deleting the file that stores the state.
public final void set new vertex value ( vv new value ) { if ( set new vertex value called ) { throw new illegal state exception ( str ) ; } set new vertex value called = bool ; out vertex . f1 = new value ; out . collect ( either . left ( out vertex ) ) ; }	Sets the new value of this vertex.
public void set new vertex value ( vv new value ) { if ( set new vertex value called ) { throw new illegal state exception ( str ) ; } set new vertex value called = bool ; if ( is opt degrees ( ) ) { out val with degrees . f1 . f0 = new value ; out with degrees . collect ( out val with degrees ) ; } else { out val . set value ( new value ) ; out . collect ( out val ) ; } }	Sets the new value of this vertex.
private static path validate path ( path path ) { final uri uri = path . to uri ( ) ; final string scheme = uri . get scheme ( ) ; final string path part = uri . get path ( ) ;	Checks the validity of the path's scheme and path.
public void transfer all state data to directory ( incremental remote keyed state handle restore state handle , path dest , closeable registry closeable registry ) throws exception { final map < id , stream state handle > sst files = restore state handle . get shared state ( ) ; final map < id , stream state handle > misc files = restore state handle . get private state ( ) ; download data for all state handles ( sst files , dest , closeable registry ) ; download data for all state handles ( misc files , dest , closeable registry ) ; }	Transfer all state data to the target directory using specified number of threads.
private void download data for state handle ( path restore file path , stream state handle remote file handle , closeable registry closeable registry ) throws io { fs input stream = null ; fs output stream = null ; try { file system restore file system = restore file path . get file system ( ) ; input stream = remote file handle . open input stream ( ) ; closeable registry . register closeable ( input stream ) ; output stream = restore file system . create ( restore file path , file system . write mode . overwrite ) ; closeable registry . register closeable ( output stream ) ; byte [ ] buffer = new byte [ num * num ] ; while ( bool ) { int num bytes = input stream . read ( buffer ) ; if ( num bytes == - num ) { break ; } output stream . write ( buffer , num , num bytes ) ; } } finally { if ( closeable registry . unregister closeable ( input stream ) ) { input stream . close ( ) ; } if ( closeable registry . unregister closeable ( output stream ) ) { output stream . close ( ) ; } } }	Copies the file from a single state handle to the given path.
public static < out > void check collection ( collection < out > elements , class < out > viewed as ) { for ( out elem : elements ) { if ( elem == null ) { throw new illegal argument exception ( str ) ; } if ( ! viewed as . is assignable from ( elem . get class ( ) ) ) { throw new illegal argument exception ( str + viewed as . get canonical name ( ) ) ; } } }	Verifies that all elements in the collection are non-null, and are of the given class, ora subclass thereof.
public void initialize cache ( object key ) throws exception { this . sorted windows = cached sorted windows . get ( key ) ; if ( sorted windows == null ) { this . sorted windows = new tree set < > ( ) ; iterator < map . entry < w , w > > key values = mapping . iterator ( ) ; if ( key values != null ) { while ( key values . has next ( ) ) { map . entry < w , w > key value = key values . next ( ) ; this . sorted windows . add ( key value . get key ( ) ) ; } } cached sorted windows . put ( key , sorted windows ) ; } }	Set current key context of this window set. Notes: {.
public final boolean is resolved ( ) { return get path parameters ( ) . stream ( ) . filter ( message parameter :: is mandatory ) . all match ( message parameter :: is resolved ) && get query parameters ( ) . stream ( ) . filter ( message parameter :: is mandatory ) . all match ( message parameter :: is resolved ) ; }	Returns whether all mandatory parameters have been resolved.
public static database create hive database ( string db name , catalog database db ) { map < string , string > props = db . get properties ( ) ; return new database ( db name , db . get description ( ) . is present ( ) ? db . get description ( ) . get ( ) : null , null , props ) ; }	Creates a Hive database from CatalogDatabase.
private static int index of name ( list < unresolved reference expression > input field references , string target name ) { int i ; for ( i = num ; i < input field references . size ( ) ; ++ i ) { if ( input field references . get ( i ) . get name ( ) . equals ( target name ) ) { break ; } } return i == input field references . size ( ) ? - num : i ; }	Find the index of targetName in the list.
private static boolean check begin ( binary string pattern , memory segment [ ] segments , int start , int len ) { int len sub = pattern . get size in bytes ( ) ; return len >= len sub && segments util . equals ( pattern . get segments ( ) , num , segments , start , len sub ) ; }	Matches the beginning of each string to a pattern.
private static int index middle ( binary string pattern , memory segment [ ] segments , int start , int len ) { return segments util . find ( segments , start , len , pattern . get segments ( ) , pattern . get offset ( ) , pattern . get size in bytes ( ) ) ; }	Matches the middle of each string to its pattern.
public < c extends rpc gateway > c get self gateway ( class < c > self gateway type ) { if ( self gateway type . is instance ( rpc server ) ) { @ suppress warnings ( str ) c self gateway = ( ( c ) rpc server ) ; return self gateway ; } else { throw new runtime exception ( str + self gateway type + str ) ; } }	Returns a self gateway of the specified type which can be used to issue asynchronouscalls against the RpcEndpoint.
@ internal public static void close safety net and guarded resources for thread ( ) { safety net closeable registry registry = registries . get ( ) ; if ( null != registry ) { registries . remove ( ) ; io . close quietly ( registry ) ; } }	Closes the safety net for a thread.
public void add heuristic network cost ( double cost ) { if ( cost <= num ) { throw new illegal argument exception ( str ) ; } this . heuristic network cost += cost ;	Adds the heuristic costs for network to the current heuristic network costsfor this Costs object.
public void add heuristic disk cost ( double cost ) { if ( cost <= num ) { throw new illegal argument exception ( str ) ; } this . heuristic disk cost += cost ;	Adds the heuristic costs for disk to the current heuristic disk costsfor this Costs object.
public void add heuristic cpu cost ( double cost ) { if ( cost <= num ) { throw new illegal argument exception ( str ) ; } this . heuristic cpu cost += cost ;	Adds the given heuristic CPU cost to the current heuristic CPU cost for this Costs object.
public void subtract costs ( costs other ) { if ( this . network cost != unknown && other . network cost != unknown ) { this . network cost -= other . network cost ; if ( this . network cost < num ) { throw new illegal argument exception ( str ) ; } } if ( this . disk cost != unknown && other . disk cost != unknown ) { this . disk cost -= other . disk cost ; if ( this . disk cost < num ) { throw new illegal argument exception ( str ) ; } } if ( this . cpu cost != unknown && other . cpu cost != unknown ) { this . cpu cost -= other . cpu cost ; if ( this . cpu cost < num ) { throw new illegal argument exception ( str ) ; } }	Subtracts the given costs from these costs.
public join operator < i1 , i2 , out > with partitioner ( partitioner < ? > partitioner ) { if ( partitioner != null ) { keys1 . validate custom partitioner ( partitioner , null ) ; keys2 . validate custom partitioner ( partitioner , null ) ; } this . custom partitioner = get input1 ( ) . clean ( partitioner ) ; return this ; }	Sets a custom partitioner for this join.
public binary row append ( lookup info info , binary row value ) throws io { try { if ( num elements >= growth threshold ) { grow and rehash ( ) ;	Append an value into the hash map's record area.
public void reset ( ) { int num buckets = bucket segments . size ( ) * num buckets per segment ; this . log2 num buckets = math utils . log2strict ( num buckets ) ; this . num buckets mask = ( num << math utils . log2strict ( num buckets ) ) - num ; this . num buckets mask2 = ( num << math utils . log2strict ( num buckets > > num ) ) - num ; this . growth threshold = ( int ) ( num buckets * load factor ) ;	reset the map's record and bucket area's memory segments for reusing.
static tuple2 < path , local resource > setup local resource ( file system fs , string app id , path local src path , path homedir , string relative target path ) throws io { file local file = new file ( local src path . to uri ( ) . get path ( ) ) ; if ( local file . is directory ( ) ) { throw new illegal argument exception ( str + local src path ) ; }	Copy a local file to a remote file system.
private static local resource register local resource ( path remote rsrc path , long resource size , long resource modification time ) { local resource local resource = records . new record ( local resource . class ) ; local resource . set resource ( converter utils . get yarn url from uri ( remote rsrc path . to uri ( ) ) ) ; local resource . set size ( resource size ) ; local resource . set timestamp ( resource modification time ) ; local resource . set type ( local resource type . file ) ; local resource . set visibility ( local resource visibility . application ) ; return local resource ; }	Creates a YARN resource for the remote object at the given location.
private static void obtain token for h ( credentials credentials , configuration conf ) throws io { if ( user group information . is security enabled ( ) ) { log . info ( str ) ; try {	Obtain Kerberos security token for HBase.
public static map < string , string > get environment variables ( string env prefix , org . apache . flink . configuration . configuration flink configuration ) { map < string , string > result = new hash map < > ( ) ; for ( map . entry < string , string > entry : flink configuration . to map ( ) . entry set ( ) ) { if ( entry . get key ( ) . starts with ( env prefix ) && entry . get key ( ) . length ( ) > env prefix . length ( ) ) {	Method to extract environment variables from the flinkConfiguration based on the given prefix String.
static void require ( boolean condition , string message , object ... values ) { if ( ! condition ) { throw new runtime exception ( string . format ( message , values ) ) ; } }	Validates a condition, throwing a RuntimeException if the condition is violated.
public query scope info get query service metric info ( character filter filter ) { if ( query service scope info == null ) { query service scope info = create query service metric info ( filter ) ; } return query service scope info ; }	Returns the metric query service scope for this group.
protected void add metric ( string name , metric metric ) { if ( metric == null ) { log . warn ( str , name ) ; return ; }	Adds the given metric to the group and registers it at the registry, if the groupis not yet closed, and if no metric with the same name has been registered before.
private static calendar value as calendar ( object value ) { date date = ( date ) value ; calendar cal = calendar . get instance ( ) ; cal . set time ( date ) ; return cal ; }	Convert a Date value to a Calendar.
public static boolean is function of type ( expression expr , function definition . type type ) { return expr instanceof call expression && ( ( call expression ) expr ) . get function definition ( ) . get type ( ) == type ; }	Checks if the expression is a function call of given type.
private static string strip hostname ( final string original hostname ) {	Looks for a domain suffix in a FQDN and strips it if present.
private void on barrier ( int channel index ) throws io { if ( ! blocked channels [ channel index ] ) { blocked channels [ channel index ] = bool ; num barriers received ++ ; if ( log . is debug enabled ( ) ) { log . debug ( str , input gate . get owning task name ( ) , channel index ) ; } } else { throw new io ( str + channel index ) ; } }	Blocks the given channel index, from which a barrier has been received.
private void release blocks and reset barriers ( ) throws io { log . debug ( str , input gate . get owning task name ( ) ) ; for ( int i = num ; i < blocked channels . length ; i ++ ) { blocked channels [ i ] = bool ; } if ( current buffered == null ) {	Releases the blocks on all channels and resets the barrier count.Makes sure the just written data is the next to be consumed.
public static void init defaults from configuration ( configuration configuration ) { final boolean overwrite = configuration . get boolean ( core options . filesytem default override ) ; default write mode = overwrite ? write mode . overwrite : write mode . no overwrite ; final boolean always create directory = configuration . get boolean ( core options . filesystem output always create directory ) ; default output directory mode = always create directory ? output directory mode . always : output directory mode . paronly ; }	Initialize defaults for output format.
@ override public void initialize global ( int parallelism ) throws io { final path path = get output file path ( ) ; final file system fs = path . get file system ( ) ;	Initialization of the distributed file system if it is used.
public static < t extends throwable > optional < t > find throwable ( throwable throwable , class < t > search type ) { if ( throwable == null || search type == null ) { return optional . empty ( ) ; } throwable t = throwable ; while ( t != null ) { if ( search type . is assignable from ( t . get class ( ) ) ) { return optional . of ( search type . cast ( t ) ) ; } else { t = t . get cause ( ) ; } } return optional . empty ( ) ; }	Checks whether a throwable chain contains a specific type of exception and returns it.
public static optional < throwable > find throwable ( throwable throwable , predicate < throwable > predicate ) { if ( throwable == null || predicate == null ) { return optional . empty ( ) ; } throwable t = throwable ; while ( t != null ) { if ( predicate . test ( t ) ) { return optional . of ( t ) ; } else { t = t . get cause ( ) ; } } return optional . empty ( ) ; }	Checks whether a throwable chain contains an exception matching a predicate and returns it.
public static optional < throwable > find throwable with message ( throwable throwable , string search message ) { if ( throwable == null || search message == null ) { return optional . empty ( ) ; } throwable t = throwable ; while ( t != null ) { if ( t . get message ( ) != null && t . get message ( ) . contains ( search message ) ) { return optional . of ( t ) ; } else { t = t . get cause ( ) ; } } return optional . empty ( ) ; }	Checks whether a throwable chain contains a specific error message and returns the corresponding throwable.
public static int optimal num of bits ( long input entries , double fpp ) { int num bits = ( int ) ( - input entries * math . log ( fpp ) / ( math . log ( num ) * math . log ( num ) ) ) ; return num bits ; }	Compute optimal bits number with given input entries and expected false positive probability.
static int optimal num of hash functions ( long expect entries , long bit size ) { return math . max ( num , ( int ) math . round ( ( double ) bit size / expect entries * math . log ( num ) ) ) ; }	compute the optimal hash function number with given input entries and bits size, which wouldmake the false positive probability lowest.
@ suppress warnings ( str ) public list < record writer < serialization delegate < t > > > get writers ( ) { return collections . unmodifiable list ( arrays . as list ( this . writers ) ) ; }	List of writers that are associated with this output collector.
@ override public boolean try assign payload ( payload payload ) { preconditions . check not null ( payload ) ;	Atomically sets the executed vertex, if no vertex has been assigned to this slot so far.
public static scope formats from config ( configuration config ) { string jm format = config . get string ( metric options . scope naming jm ) ; string jm job format = config . get string ( metric options . scope naming jm job ) ; string tm format = config . get string ( metric options . scope naming tm ) ; string tm job format = config . get string ( metric options . scope naming tm job ) ; string task format = config . get string ( metric options . scope naming task ) ; string operator format = config . get string ( metric options . scope naming operator ) ; return new scope formats ( jm format , jm job format , tm format , tm job format , task format , operator format ) ; }	Creates the scope formats as defined in the given configuration.
private static void init defaults from configuration ( configuration configuration ) { final long to = configuration . get long ( config constants . fs stream opening timeout key , config constants . default fs stream opening timeout ) ; if ( to < num ) { log . error ( str + to + str + config constants . default fs stream opening timeout ) ; default opening timeout = config constants . default fs stream opening timeout ; } else if ( to == num ) { default opening timeout = num ;	Initialize defaults for input format.
public path [ ] get file paths ( ) { if ( supports multi paths ( ) ) { if ( this . file paths == null ) { return new path [ num ] ; } return this . file paths ; } else { if ( this . file path == null ) { return new path [ num ] ; } return new path [ ] { file path } ; } }	Returns the paths of all files to be read by the FileInputFormat.
private long add files in dir ( path path , list < file status > files , boolean log excluded files ) throws io { final file system fs = path . get file system ( ) ; long length = num ; for ( file status dir : fs . list status ( path ) ) { if ( dir . is dir ( ) ) { if ( accept file ( dir ) && enumerate nested files ) { length += add files in dir ( dir . get path ( ) , files , log excluded files ) ; } else { if ( log excluded files && log . is debug enabled ( ) ) { log . debug ( str + dir . get path ( ) . to string ( ) + str ) ; } } } else { if ( accept file ( dir ) ) { files . add ( dir ) ; length += dir . get len ( ) ; test for unsplittable ( dir ) ; } else { if ( log excluded files && log . is debug enabled ( ) ) { log . debug ( str + dir . get path ( ) . to string ( ) + str ) ; } } } } return length ; }	Enumerate all files in the directory and recursive if enumerateNestedFiles is true.
public synchronized void add open channels ( list < io > to open ) { check argument ( ! closed ) ; for ( io channel : to open ) { open channels . add ( channel ) ; channels . remove ( channel . get channel id ( ) ) ; } }	Open File channels.
public void open ( context < k , w > ctx ) throws exception { this . ctx = ctx ; this . window assigner . open ( ctx ) ; }	Initialization method for the function.
public single output stream operator < t > sum ( int position to sum ) { return aggregate ( new sum aggregator < > ( position to sum , input . get type ( ) , input . get execution config ( ) ) ) ; }	Applies an aggregation that sums every window of the data stream at thegiven position.
public single output stream operator < t > sum ( string field ) { return aggregate ( new sum aggregator < > ( field , input . get type ( ) , input . get execution config ( ) ) ) ; }	Applies an aggregation that sums every window of the pojo data stream atthe given field for every window.
public single output stream operator < t > min ( int position to min ) { return aggregate ( new comparable aggregator < > ( position to min , input . get type ( ) , aggregation function . aggregation type . min , input . get execution config ( ) ) ) ; }	Applies an aggregation that that gives the minimum value of every windowof the data stream at the given position.
public single output stream operator < t > min ( string field ) { return aggregate ( new comparable aggregator < > ( field , input . get type ( ) , aggregation function . aggregation type . min , bool , input . get execution config ( ) ) ) ; }	Applies an aggregation that that gives the minimum value of the pojo datastream at the given field expression for every window. A field expression is either the name of a public field or a getter method withparentheses of the {.
public single output stream operator < t > max ( int position to max ) { return aggregate ( new comparable aggregator < > ( position to max , input . get type ( ) , aggregation function . aggregation type . max , input . get execution config ( ) ) ) ; }	Applies an aggregation that gives the maximum value of every window ofthe data stream at the given position.
public sorted grouping < t > with partitioner ( partitioner < ? > partitioner ) { preconditions . check not null ( partitioner ) ; get keys ( ) . validate custom partitioner ( partitioner , null ) ; this . custom partitioner = partitioner ; return this ; }	Uses a custom partitioner for the grouping.
public static byte [ ] encode ut ( string str ) { byte [ ] bytes = allocate reuse bytes ( str . length ( ) * max bytes per char ) ; int len = encode ut ( str , bytes ) ; return arrays . copy of ( bytes , len ) ; }	This method must have the same result with JDK's String.getBytes.
public methodless router < t > add route ( string path pattern , t target ) { path pattern p = new path pattern ( path pattern ) ; if ( routes . contains key ( p ) ) { return this ; } routes . put ( p , target ) ; return this ; }	This method does nothing if the path pattern has already been added.A path pattern can only point to one target.
public boolean any matched ( string [ ] request path tokens ) { map < string , string > path params = new hash map < > ( ) ; for ( path pattern pattern : routes . key set ( ) ) { if ( pattern . match ( request path tokens , path params ) ) { return bool ; }	Checks if there's any matching route.
private void initialize ( string scheme , string authority , string path ) { try { this . uri = new uri ( scheme , authority , normalize path ( path ) , null , null ) . normalize ( ) ; } catch ( uri e ) { throw new illegal argument exception ( e ) ; } }	Initializes a path object given the scheme, authority and path string.
public boolean is absolute ( ) { final int start = has windows drive ( uri . get path ( ) , bool ) ? num : num ; return uri . get path ( ) . starts with ( separator , start ) ; }	Checks if the directory of this path is absolute.
public int depth ( ) { string path = uri . get path ( ) ; int depth = num ; int slash = path . length ( ) == num && path . char at ( num ) == str ? - num : num ; while ( slash != - num ) { depth ++ ; slash = path . index of ( separator , slash + num ) ; } return depth ; }	Returns the number of elements in this path.
public avro record class ( class < ? extends specific record > record class ) { preconditions . check not null ( record class ) ; this . record class = record class ; return this ; }	Sets the class of the Avro specific record.
public static void read fully ( final input stream in , final byte [ ] buf , int off , final int len ) throws io { int to read = len ; while ( to read > num ) { final int ret = in . read ( buf , off , to read ) ; if ( ret < num ) { throw new io ( str ) ; } to read -= ret ; off += ret ; } }	Reads len bytes in a loop.
private static string get algorithms listing ( ) { str builder str builder = new str builder ( ) ; str builder . append new line ( ) . appendln ( str ) . append new line ( ) . appendln ( str ) ; for ( driver algorithm : driver factory ) { str builder . append ( str ) . append fixed width pad right ( algorithm . get name ( ) , num , str ) . append ( algorithm . get short description ( ) ) . append new line ( ) ; } return str builder . to string ( ) ; }	List available algorithms. This is displayed to the user when no validalgorithm is given in the program parameterization.
private static string get algorithm usage ( string algorithm name ) { str builder str builder = new str builder ( ) ; driver algorithm = driver factory . get ( algorithm name ) ; str builder . append new line ( ) . append new line ( ) . appendln ( algorithm . get long description ( ) ) . append new line ( ) . append ( str ) . append ( algorithm name ) . append ( str ) . append new line ( ) . append new line ( ) . appendln ( str ) ; for ( input input : input factory ) { str builder . append ( str ) . append ( input . get name ( ) ) . append ( str ) . appendln ( input . get usage ( ) ) ; } string algorithm parameterization = algorithm . get usage ( ) ; if ( algorithm parameterization . length ( ) > num ) { str builder . append new line ( ) . appendln ( str ) . append ( str ) . appendln ( algorithm . get usage ( ) ) ; } str builder . append new line ( ) . appendln ( str ) ; for ( output output : output factory ) { str builder . append ( str ) . append ( output . get name ( ) ) . append ( str ) . appendln ( output . get usage ( ) ) ; } return str builder . append new line ( ) . to string ( ) ; }	Display the usage for the given algorithm.
private void execute ( ) throws exception { if ( result == null ) { env . execute ( execution name ) ; } else { output . write ( execution name . to string ( ) , system . out , result ) ; } system . out . println ( ) ; algorithm . print analytics ( system . out ) ; if ( job details path . get value ( ) != null ) { write job details ( env , job details path . get value ( ) ) ; } }	Execute the Flink job.
protected void start threads ( ) { if ( this . read thread != null ) { this . read thread . start ( ) ; } if ( this . sort thread != null ) { this . sort thread . start ( ) ; } if ( this . spill thread != null ) { this . spill thread . start ( ) ; } }	Starts all the threads that are used by this sort-merger.
protected final void set result iterator exception ( io ioex ) { synchronized ( this . iterator lock ) { if ( this . iterator exception == null ) { this . iterator exception = ioex ; this . iterator lock . notify all ( ) ; } } }	Reports an exception to all threads that are waiting for the result iterator.
protected static < t > circular element < t > end marker ( ) { @ suppress warnings ( str ) circular element < t > c = ( circular element < t > ) eof marker ; return c ; }	Gets the element that is passed as marker for the end of data.
protected static < t > circular element < t > spilling marker ( ) { @ suppress warnings ( str ) circular element < t > c = ( circular element < t > ) spilling marker ; return c ; }	Gets the element that is passed as marker for signal beginning of spilling.
@ override public void create resource ( ) throws exception { cluster = builder . get cluster ( ) ; session = cluster . connect ( ) ; session . execute ( string . format ( str , key space ) ) ; session . execute ( string . format ( str , key space , table ) ) ; try { session . close ( ) ; } catch ( exception e ) { log . error ( str , e ) ; } try { cluster . close ( ) ; } catch ( exception e ) { log . error ( str , e ) ; } }	Generates the necessary tables to store information.
public void jar dir ( file dir or file2 jar , file dest jar ) throws io { if ( dir or file2 jar == null || dest jar == null ) { throw new illegal argument exception ( ) ; } m dest jar name = dest jar . get canonical path ( ) ; file output stream fout = new file output stream ( dest jar ) ; jar output stream jout = new jar output stream ( fout ) ;	Jars a given directory or single file into a JarOutputStream.
public void unjar dir ( file jar file , file dest dir ) throws io { buffered output stream dest = null ; file input stream fis = new file input stream ( jar file ) ; unjar ( fis , dest dir ) ; }	Unjars a given jar file into a given directory.
public void unjar ( input stream in , file dest dir ) throws io { buffered output stream dest = null ; jar input stream jis = new jar input stream ( in ) ; jar entry entry ; while ( ( entry = jis . get next jar entry ( ) ) != null ) { if ( entry . is directory ( ) ) { file dir = new file ( dest dir , entry . get name ( ) ) ; dir . mkdir ( ) ; if ( entry . get time ( ) != - num ) { dir . set last modified ( entry . get time ( ) ) ; } continue ; } int count ; byte [ ] data = new byte [ buffer size ] ; file dest file = new file ( dest dir , entry . get name ( ) ) ; if ( m verbose ) { system . out . println ( str + dest file + str + entry . get name ( ) ) ; } file output stream fos = new file output stream ( dest file ) ; dest = new buffered output stream ( fos , buffer size ) ; try { while ( ( count = jis . read ( data , num , buffer size ) ) != - num ) { dest . write ( data , num , count ) ; } dest . flush ( ) ; } finally { dest . close ( ) ; } if ( entry . get time ( ) != - num ) { dest file . set last modified ( entry . get time ( ) ) ; } } jis . close ( ) ; }	Given an InputStream on a jar file, unjars the contents into the givendirectory.
private boolean ensure batch ( ) throws io { if ( next row >= rows in batch ) {	Checks if there is at least one row left in the batch to return.If no more row are available, it reads another batch of rows.
public map < string , optional failure < object > > get accumulators ( id job id ) throws exception { return get accumulators ( job id , class loader . get system class loader ( ) ) ; }	Requests and returns the accumulators for the given job identifier.
private static optimized plan get optimized plan ( optimizer compiler , job with jars prog , int parallelism ) throws compiler exception , program invocation exception { return get optimized plan ( compiler , prog . get plan ( ) , parallelism ) ; }	Creates the optimized plan for a given program, using this client's compiler.
public static string generate runtime name ( class < ? > clazz , string [ ] fields ) { return table connector utils . generate runtime name ( clazz , fields ) ; }	Returns the table connector name used for log and web UI.
public channel future request subpartition ( final id partition id , final int subpartition index , final remote input channel input channel , int delay ms ) throws io { check not closed ( ) ; log . debug ( str , subpartition index , partition id , delay ms ) ; client handler . add input channel ( input channel ) ; final partition request request = new partition request ( partition id , subpartition index , input channel . get input channel id ( ) , input channel . get initial credit ( ) ) ; final channel future listener listener = new channel future listener ( ) { @ override public void operation complete ( channel future future ) throws exception { if ( ! future . is success ( ) ) { client handler . remove input channel ( input channel ) ; socket address remote addr = future . channel ( ) . remote address ( ) ; input channel . on error ( new local transport exception ( string . format ( str , remote addr ) , future . channel ( ) . local address ( ) , future . cause ( ) ) ) ; } } } ; if ( delay ms == num ) { channel future f = tcp channel . write and flush ( request ) ; f . add listener ( listener ) ; return f ; } else { final channel future [ ] f = new channel future [ num ] ; tcp channel . event loop ( ) . schedule ( new runnable ( ) { @ override public void run ( ) { f [ num ] = tcp channel . write and flush ( request ) ; f [ num ] . add listener ( listener ) ; } } , delay ms , time unit . milliseconds ) ; return f [ num ] ; } }	Requests a remote intermediate result partition queue.
public void select fields ( string [ ] field names ) { check not null ( field names , str ) ; this . field names = field names ; row type info row type info = ( row type info ) parquet schema converter . from parquet type ( expected file schema ) ; type information [ ] select field types = new type information [ field names . length ] ; for ( int i = num ; i < field names . length ; i ++ ) { try { select field types [ i ] = row type info . get type at ( field names [ i ] ) ; } catch ( index out of bounds exception e ) { throw new illegal argument exception ( string . format ( str + str , field names [ i ] ) , e ) ; } } this . field types = select field types ; }	Configures the fields to be read and returned by the ParquetInputFormat.
private message type get read schema ( message type file schema , path file path ) { row type info file type info = ( row type info ) parquet schema converter . from parquet type ( file schema ) ; list < type > types = new array list < > ( ) ; for ( int i = num ; i < field names . length ; ++ i ) { string read field name = field names [ i ] ; type information < ? > read field type = field types [ i ] ; if ( file type info . get field index ( read field name ) < num ) { if ( ! skip wrong schema file split ) { throw new illegal argument exception ( str + read field name + str + str + file path + str ) ; } else { this . skip this split = bool ; return file schema ; } } if ( ! read field type . equals ( file type info . get type at ( read field name ) ) ) { if ( ! skip wrong schema file split ) { throw new illegal argument exception ( str + read field type + str + read field name + str + file type info . get type at ( read field name ) + str + file path + str ) ; } else { this . skip this split = bool ; return file schema ; } } types . add ( file schema . get type ( read field name ) ) ; } return new message type ( file schema . get name ( ) , types ) ; }	Generates and returns the read schema based on the projected fields for a given file.
private long cal expiration time ( long operator time , long relative size ) { if ( operator time < long . max value ) { return operator time - relative size - allowed lateness - num ; } else {	Calculate the expiration time with the given operator time and relative window size.
private void register clean up timer ( context ctx , long row time , boolean left row ) throws io { if ( left row ) { long clean up time = row time + left relative size + min clean up interval + allowed lateness + num ; register timer ( ctx , clean up time ) ; right timer state . update ( clean up time ) ; } else { long clean up time = row time + right relative size + min clean up interval + allowed lateness + num ; register timer ( ctx , clean up time ) ; left timer state . update ( clean up time ) ; } }	Register a timer for cleaning up rows in a specified time.
private void remove expired rows ( collector < base row > collector , long expiration time , map state < long , list < tuple2 < base row , boolean > > > row cache , value state < long > timer state , on timer context ctx , boolean remove left ) throws exception { iterator < map . entry < long , list < tuple2 < base row , boolean > > > > iterator = row cache . iterator ( ) ; long earliest timestamp = - num ;	Remove the expired rows.
public < t > dynamic result < t > create result ( environment env , table schema schema , execution config config ) { final row type info output type = new row type info ( schema . get field types ( ) , schema . get field names ( ) ) ; if ( env . get execution ( ) . is streaming execution ( ) ) {	Creates a result. Might start threads or opens sockets so every created result must be closed.
@ override public void lazy destroy ( ) {	Destroy is called after the produce or consume phase of a task finishes.
public static < kt , kb , vvt , vvb , ev > bipartite graph < kt , kb , vvt , vvb , ev > from data set ( data set < vertex < kt , vvt > > top vertices , data set < vertex < kb , vvb > > bottom vertices , data set < bipartite edge < kt , kb , ev > > edges , execution environment context ) { return new bipartite graph < > ( top vertices , bottom vertices , edges , context ) ; }	Create bipartite graph from datasets.
public graph < kt , vvt , tuple2 < ev , ev > > projection top simple ( ) { data set < edge < kt , tuple2 < ev , ev > > > new edges = edges . join ( edges ) . where ( num ) . equal to ( num ) . with ( new projection top simple < > ( ) ) . name ( str ) ; return graph . from data set ( top vertices , new edges , context ) ; }	Convert a bipartite graph into an undirected graph that contains only top vertices.
public graph < kb , vvb , tuple2 < ev , ev > > projection bottom simple ( ) { data set < edge < kb , tuple2 < ev , ev > > > new edges = edges . join ( edges ) . where ( num ) . equal to ( num ) . with ( new projection bottom simple < > ( ) ) . name ( str ) ; return graph . from data set ( bottom vertices , new edges , context ) ; }	Convert a bipartite graph into an undirected graph that contains only bottom vertices.
public graph < kt , vvt , projection < kb , vvb , vvt , ev > > projection top full ( ) { data set < tuple5 < kt , kb , ev , vvt , vvb > > edges with vertices = join edge with vertices ( ) ; data set < edge < kt , projection < kb , vvb , vvt , ev > > > new edges = edges with vertices . join ( edges with vertices ) . where ( num ) . equal to ( num ) . with ( new projection top full < > ( ) ) . name ( str ) ; return graph . from data set ( top vertices , new edges , context ) ; }	Convert a bipartite graph into a graph that contains only top vertices.
public graph < kb , vvb , projection < kt , vvt , vvb , ev > > projection bottom full ( ) { data set < tuple5 < kt , kb , ev , vvt , vvb > > edges with vertices = join edge with vertices ( ) ; data set < edge < kb , projection < kt , vvt , vvb , ev > > > new edges = edges with vertices . join ( edges with vertices ) . where ( num ) . equal to ( num ) . with ( new projection bottom full < > ( ) ) . name ( str ) ; return graph . from data set ( bottom vertices , new edges , context ) ; }	Convert a bipartite graph into a graph that contains only bottom vertices.
@ override public job execution result execute ( string job name ) throws exception {	Executes the JobGraph of the on a mini cluster of CLusterUtil with a userspecified name.
public static kv state service from configuration ( task manager services configuration task manager services configuration ) { kv state registry kv state registry = new kv state registry ( ) ; queryable state configuration qs config = task manager services configuration . get queryable state config ( ) ; kv state client proxy kv client proxy = null ; kv state server kv state server = null ; if ( qs config != null ) { int num proxy server network threads = qs config . num proxy server threads ( ) == num ? task manager services configuration . get number of slots ( ) : qs config . num proxy server threads ( ) ; int num proxy server query threads = qs config . num proxy query threads ( ) == num ? task manager services configuration . get number of slots ( ) : qs config . num proxy query threads ( ) ; kv client proxy = queryable state utils . create kv state client proxy ( task manager services configuration . get task manager address ( ) , qs config . get proxy port range ( ) , num proxy server network threads , num proxy server query threads , new disabled kv state request stats ( ) ) ; int num state server network threads = qs config . num state server threads ( ) == num ? task manager services configuration . get number of slots ( ) : qs config . num state server threads ( ) ; int num state server query threads = qs config . num state query threads ( ) == num ? task manager services configuration . get number of slots ( ) : qs config . num state query threads ( ) ; kv state server = queryable state utils . create kv state server ( task manager services configuration . get task manager address ( ) , qs config . get state server port range ( ) , num state server network threads , num state server query threads , kv state registry , new disabled kv state request stats ( ) ) ; } return new kv state service ( kv state registry , kv state server , kv client proxy ) ; }	Creates and returns the KvState service.
public compensated sum add ( compensated sum other ) { double corrected sum = other . value ( ) + ( delta + other . delta ( ) ) ; double updated value = value + corrected sum ; double updated delta = corrected sum - ( updated value - value ) ; return new compensated sum ( updated value , updated delta ) ; }	Increments the Kahan sum by adding two sums, and updating the correction term for reducing numeric errors.
public sort partition operator < t > sort partition ( int field , order order ) { if ( use key selector ) { throw new invalid program exception ( str ) ; } ensure sortable key ( field ) ; keys . add ( new keys . expression keys < > ( field , get type ( ) ) ) ; orders . add ( order ) ; return this ; }	Appends an additional sort order with the specified field in the specified order to thelocal partition sorting of the DataSet.
public option < long > get number of allocated bytes ( ) throws no such field exception , illegal access exception { if ( direct arenas != null ) { long num chunks = num ; for ( object arena : direct arenas ) { num chunks += get number of allocated chunks ( arena , str ) ; num chunks += get number of allocated chunks ( arena , str ) ; num chunks += get number of allocated chunks ( arena , str ) ; num chunks += get number of allocated chunks ( arena , str ) ; num chunks += get number of allocated chunks ( arena , str ) ; num chunks += get number of allocated chunks ( arena , str ) ; } long allocated bytes = num chunks * chunk size ; return option . apply ( allocated bytes ) ; } else { return option . empty ( ) ; } }	Returns the number of currently allocated bytes.
private long get number of allocated chunks ( object arena , string chunk list field name ) throws no such field exception , illegal access exception {	Returns the number of allocated bytes of the given arena and chunk list.
private void validate key types ( int [ ] key field indices ) { final type information < ? > [ ] types = get field types ( ) ; for ( int key field index : key field indices ) { final type information < ? > type = types [ key field index ] ; if ( ! type check utils . is simple string representation ( type ) ) { throw new validation exception ( str + str + type ) ; } } }	Validate the types that are used for conversion to string.
protected void add path recursively ( final file source path , final path target path , final container specification env ) throws io { final java . nio . file . path source root = source path . to path ( ) . get parent ( ) ; files . walk file tree ( source path . to path ( ) , new simple file visitor < java . nio . file . path > ( ) { @ override public file visit result visit file ( java . nio . file . path file , basic file attributes attrs ) throws io { java . nio . file . path relative path = source root . relativize ( file ) ; container specification . artifact . builder artifact = container specification . artifact . new builder ( ) . set source ( new path ( file . to uri ( ) ) ) . set dest ( new path ( target path , relative path . to string ( ) ) ) . set executable ( files . is executable ( file ) ) . set cachable ( bool ) . set extract ( bool ) ; env . get artifacts ( ) . add ( artifact . build ( ) ) ; return super . visit file ( file , attrs ) ; } } ) ; }	Add a path recursively to the container specification.If the path is a directory, the directory itself (not just its contents) is added to the target path.The execute bit is preserved; permissions aren't.
public void subscribe to event ( id partition id , event listener < task event > event listener , class < ? extends task event > event type ) { check not null ( partition id ) ; check not null ( event listener ) ; check not null ( event type ) ; task event handler task event handler ; synchronized ( registered handlers ) { task event handler = registered handlers . get ( partition id ) ; } if ( task event handler == null ) { throw new illegal state exception ( str + partition id + str ) ; } task event handler . subscribe ( event listener , event type ) ; }	Subscribes a listener to this dispatcher for events on a partition.
public static py object adapt ( object o ) { if ( o instanceof py object ) { return ( py object ) o ; } return py . java2py ( o ) ; }	Convert java object to its corresponding PyObject representation.
public static void reset ( final collection < master trigger restore hook < ? > > hooks , final logger log ) throws flink exception { for ( master trigger restore hook < ? > hook : hooks ) { final string id = hook . get identifier ( ) ; try { hook . reset ( ) ; } catch ( throwable t ) { exception utils . rethrow if fatal error or oom ( t ) ; throw new flink exception ( str + id + str , t ) ; } } }	Resets the master hooks.
public static void close ( final collection < master trigger restore hook < ? > > hooks , final logger log ) throws flink exception { for ( master trigger restore hook < ? > hook : hooks ) { try { hook . close ( ) ; } catch ( throwable t ) { log . warn ( str + hook . get identifier ( ) + str , t ) ; } } }	Closes the master hooks.
public static list < master state > trigger master hooks ( collection < master trigger restore hook < ? > > hooks , long checkpoint id , long timestamp , executor executor , time timeout ) throws flink exception { final array list < master state > states = new array list < > ( hooks . size ( ) ) ; for ( master trigger restore hook < ? > hook : hooks ) { master state state = trigger hook ( hook , checkpoint id , timestamp , executor , timeout ) ; if ( state != null ) { states . add ( state ) ; } } states . trim to size ( ) ; return states ; }	Triggers all given master hooks and returns state objects for each hook thatproduced a state.
public static < t > master trigger restore hook < t > wrap hook ( master trigger restore hook < t > hook , class loader user class loader ) { return new wrapped master hook < > ( hook , user class loader ) ; }	Wraps a hook such that the user-code classloader is applied when the hook is invoked.
@ suppress warnings ( str ) public static < x > x deserialize function ( runtime context context , byte [ ] ser fun ) throws flink exception { if ( ! jython initialized ) {	Deserialize the given python function.
public static void init and exec python script ( python environment factory factory , java . nio . file . path script directory , string script name , string [ ] args ) { string [ ] full args = new string [ args . length + num ] ; full args [ num ] = script directory . resolve ( script name ) . to string ( ) ; system . arraycopy ( args , num , full args , num , args . length ) ; python interpreter python interpreter = init python interpreter ( full args , script directory . to uri ( ) . get path ( ) , script name ) ; python interpreter . set ( str , factory ) ; python interpreter . exec ( script name + str ) ; }	Initializes the Jython interpreter and executes a python script.
private static void set required properties ( properties zk props ) {	Sets required properties to reasonable defaults and logs it.
public static string generate runtime name ( class < ? > clazz , string [ ] fields ) { string class name = clazz . get simple name ( ) ; if ( null == fields ) { return class name + str ; } else { return class name + str + string . join ( str , fields ) + str ; } }	Returns the table connector name used for logging and web UI.
public mesos configuration with framework info ( protos . framework info . builder framework info ) { return new mesos configuration ( master url , framework info , credential ) ; }	Revise the configuration with updated framework info.
public set < string > roles ( ) { return framework info . has role ( ) && ! str . equals ( framework info . get role ( ) ) ? collections . singleton ( framework info . get role ( ) ) : collections . empty set ( ) ; }	Gets the roles associated with the framework.
public scheduler driver create driver ( scheduler scheduler , boolean implicit acknowledgements ) { mesos scheduler driver scheduler driver ; if ( this . credential ( ) . is defined ( ) ) { scheduler driver = new mesos scheduler driver ( scheduler , framework info . build ( ) , this . master url ( ) , implicit acknowledgements , this . credential ( ) . get ( ) . build ( ) ) ; } else { scheduler driver = new mesos scheduler driver ( scheduler , framework info . build ( ) , this . master url ( ) , implicit acknowledgements ) ; } return scheduler driver ; }	Create the Mesos scheduler driver based on this configuration.
public static thread add shutdown hook ( final auto closeable service , final string service name , final logger logger ) { check not null ( service ) ; check not null ( logger ) ; final thread shutdown hook = new thread ( ( ) -> { try { service . close ( ) ; } catch ( throwable t ) { logger . error ( str , service name , t ) ; } } , service name + str ) ; return add shutdown hook thread ( shutdown hook , service name , logger ) ? shutdown hook : null ; }	Adds a shutdown hook to the JVM and returns the Thread, which has been registered.
public static boolean add shutdown hook thread ( final thread shutdown hook , final string service name , final logger logger ) { check not null ( shutdown hook ) ; check not null ( logger ) ; try {	Adds a shutdown hook to the JVM.
public static void remove shutdown hook ( final thread shutdown hook , final string service name , final logger logger ) {	Removes a shutdown hook from the JVM.
public void start ( final string initial owner address , final rpc service initial rpc service , final high availability services initial high availability services , final job leader listener initial job leader listener ) { if ( job leader service . state . created != state ) { throw new illegal state exception ( str ) ; } else { log . info ( str ) ; this . owner address = preconditions . check not null ( initial owner address ) ; this . rpc service = preconditions . check not null ( initial rpc service ) ; this . high availability services = preconditions . check not null ( initial high availability services ) ; this . job leader listener = preconditions . check not null ( initial job leader listener ) ; state = job leader service . state . started ; } }	Start the job leader service with the given services.
public void stop ( ) throws exception { log . info ( str ) ; if ( job leader service . state . started == state ) { for ( tuple2 < leader retrieval service , job leader service . job manager leader listener > leader retrieval service entry : job leader services . values ( ) ) { leader retrieval service leader retrieval service = leader retrieval service entry . f0 ; job leader service . job manager leader listener job manager leader listener = leader retrieval service entry . f1 ; job manager leader listener . stop ( ) ; leader retrieval service . stop ( ) ; } job leader services . clear ( ) ; } state = job leader service . state . stopped ; }	Stop the job leader services.
public void remove job ( id job id ) throws exception { preconditions . check state ( job leader service . state . started == state , str ) ; tuple2 < leader retrieval service , job leader service . job manager leader listener > entry = job leader services . remove ( job id ) ; if ( entry != null ) { log . info ( str , job id ) ; leader retrieval service leader retrieval service = entry . f0 ; job leader service . job manager leader listener job manager leader listener = entry . f1 ; leader retrieval service . stop ( ) ; job manager leader listener . stop ( ) ; } }	Remove the given job from being monitored by the job leader service.
public void add job ( final id job id , final string default target address ) throws exception { preconditions . check state ( job leader service . state . started == state , str ) ; log . info ( str , job id ) ; final leader retrieval service leader retrieval service = high availability services . get job manager leader retriever ( job id , default target address ) ; job leader service . job manager leader listener job manager leader listener = new job manager leader listener ( job id ) ; final tuple2 < leader retrieval service , job manager leader listener > old entry = job leader services . put ( job id , tuple2 . of ( leader retrieval service , job manager leader listener ) ) ; if ( old entry != null ) { old entry . f0 . stop ( ) ; old entry . f1 . stop ( ) ; } leader retrieval service . start ( job manager leader listener ) ; }	Add the given job to be monitored.
public void reconnect ( final id job id ) { preconditions . check not null ( job id , str ) ; final tuple2 < leader retrieval service , job manager leader listener > job leader service = job leader services . get ( job id ) ; if ( job leader service != null ) { job leader service . f1 . reconnect ( ) ; } else { log . info ( str , job id ) ; } }	Triggers reconnection to the last known leader of the given job.
@ visible for testing public boolean contains job ( id job id ) { preconditions . check state ( job leader service . state . started == state , str ) ; return job leader services . contains key ( job id ) ; }	Check whether the service monitors the given job.
private void save handle in state ( final long checkpoint id , final long timestamp ) throws exception {	Called when a checkpoint barrier arrives.
public statistics column stats ( string column name , column stats column stats ) { map < string , string > map = normalize column stats ( column stats ) ; this . column stats . put ( column name , map ) ; return this ; }	Sets statistics for a column.
public statistics column distinct count ( string column name , long ndv ) { this . column stats . compute if absent ( column name , column -> new hash map < > ( ) ) . put ( distinct count , string . value of ( ndv ) ) ; return this ; }	Sets the number of distinct values statistic for the given column.
public statistics column null count ( string column name , long null count ) { this . column stats . compute if absent ( column name , column -> new hash map < > ( ) ) . put ( null count , string . value of ( null count ) ) ; return this ; }	Sets the number of null values statistic for the given column.
public statistics column avg length ( string column name , double avg len ) { this . column stats . compute if absent ( column name , column -> new hash map < > ( ) ) . put ( avg length , string . value of ( avg len ) ) ; return this ; }	Sets the average length statistic for the given column.
public statistics column max length ( string column name , integer max len ) { this . column stats . compute if absent ( column name , column -> new hash map < > ( ) ) . put ( max length , string . value of ( max len ) ) ; return this ; }	Sets the maximum length statistic for the given column.
public statistics column max value ( string column name , number max ) { this . column stats . compute if absent ( column name , column -> new hash map < > ( ) ) . put ( max value , string . value of ( max ) ) ; return this ; }	Sets the maximum value statistic for the given column.
public statistics column min value ( string column name , number min ) { this . column stats . compute if absent ( column name , column -> new hash map < > ( ) ) . put ( min value , string . value of ( min ) ) ; return this ; }	Sets the minimum value statistic for the given column.
@ override public int size ( ) { if ( all elements in cache ) { return ordered cache . size ( ) ; } else { int count = num ; try ( final rocks bytes iterator iterator = ordered bytes iterator ( ) ) { while ( iterator . has next ( ) ) { iterator . next ( ) ; ++ count ; } } return count ; } }	This implementation comes at a relatively high cost per invocation.
private void generate all failover region ( list < execution job vertex > new job vertices topological ) { final identity hash map < execution vertex , array list < execution vertex > > vertex to region = new identity hash map < > ( ) ;	Generate all the FailoverRegion from the new added job vertexes.
@ override public void reset ( ) { this . cursor = fixed size ; for ( int i = num ; i < null bits size in bytes ; i += num ) { segment . put long ( i , num ) ; } this . segment . put int ( num , num elements ) ; }	First, reset.
private static inet address find address using strategy ( address detection state strategy , inet socket address target address , boolean logging ) throws io {	Try to find a local address which allows as to connect to the targetAddress using the givenstrategy.
@ override public void add ( buffer or event boe ) throws io { try { byte buffer contents ; if ( boe . is buffer ( ) ) { buffer buf = boe . get buffer ( ) ; contents = buf . get nio buffer readable ( ) ; } else { contents = event serializer . to serialized event ( boe . get event ( ) ) ; } head buffer . clear ( ) ; head buffer . put int ( boe . get channel index ( ) ) ; head buffer . put int ( contents . remaining ( ) ) ; head buffer . put ( ( byte ) ( boe . is buffer ( ) ? num : num ) ) ; head buffer . flip ( ) ; bytes written += ( head buffer . remaining ( ) + contents . remaining ( ) ) ; file utils . write completely ( current channel , head buffer ) ; file utils . write completely ( current channel , contents ) ; } finally { if ( boe . is buffer ( ) ) { boe . get buffer ( ) . recycle buffer ( ) ; } } }	Adds a buffer or event to the sequence of spilled buffers and events.
public static base row key selector get base row selector ( int [ ] key fields , base row type info row type ) { if ( key fields . length > num ) { internal type [ ] input field types = row type . get internal types ( ) ; string [ ] input field names = row type . get field names ( ) ; internal type [ ] key field types = new internal type [ key fields . length ] ; string [ ] key field names = new string [ key fields . length ] ; for ( int i = num ; i < key fields . length ; ++ i ) { key field types [ i ] = input field types [ key fields [ i ] ] ; key field names [ i ] = input field names [ key fields [ i ] ] ; } row type return type = new row type ( key field types , key field names ) ; row type input type = new row type ( input field types , row type . get field names ( ) ) ; generated projection generated projection = projection code generator . generate projection ( code generator context . apply ( new table config ( ) ) , str , input type , return type , key fields ) ; base row type info key row type = return type . to type info ( ) ;	Create a BaseRowKeySelector to extract keys from DataStream which type is BaseRowTypeInfo.
public void close ( ) { synchronized ( this ) { if ( this . closed ) { return ; } this . closed = bool ; } this . num records in buffer = num ; this . num records returned = num ;	This method closes the iterator and releases all resources.
private map < path , file status > list eligible files ( file system file system , path path ) throws io { final file status [ ] statuses ; try { statuses = file system . list status ( path ) ; } catch ( io e ) {	Returns the paths of the files not yet processed.
private time window merge window ( time window cur window , time window other , collection < time window > merged window ) { if ( cur window . intersects ( other ) ) { merged window . add ( other ) ; return cur window . cover ( other ) ; } else { return cur window ; } }	Merge curWindow and other, return a new window which covers curWindow and otherif they are overlapped.
protected configuration apply command line options to configuration ( command line command line ) throws flink exception { final configuration resulting configuration = new configuration ( configuration ) ; if ( command line . has option ( address option . get opt ( ) ) ) { string address with port = command line . get option value ( address option . get opt ( ) ) ; inet socket address job manager address = client utils . parse host port address ( address with port ) ; set job manager address in config ( resulting configuration , job manager address ) ; } if ( command line . has option ( zookeeper namespace option . get opt ( ) ) ) { string zk namespace = command line . get option value ( zookeeper namespace option . get opt ( ) ) ; resulting configuration . set string ( high availability options . ha cluster id , zk namespace ) ; } return resulting configuration ; }	Override configuration settings by specified command line options.
private string get internal ( string key ) { preconditions . check argument ( configured options . contains key ( key ) , str + key + str ) ; return configured options . get ( key ) ; }	Returns the value in string format with the given key.
protected boolean increase backoff ( ) {	Increases the current backoff and returns whether the operation was successful.
public static final boolean delimiter next ( byte [ ] bytes , int start pos , byte [ ] delim ) { for ( int pos = num ; pos < delim . length ; pos ++ ) {	Checks if the delimiter starts at the given start position of the byte array.Attention: This method assumes that enough characters follow the start position for the delimiter check!.
public static final boolean ends with delimiter ( byte [ ] bytes , int end pos , byte [ ] delim ) { if ( end pos < delim . length - num ) { return bool ; } for ( int pos = num ; pos < delim . length ; ++ pos ) { if ( delim [ pos ] != bytes [ end pos - delim . length + num + pos ] ) { return bool ; } } return bool ; }	Checks if the given bytes ends with the delimiter at the given end position.
protected final int next string end pos ( byte [ ] bytes , int start pos , int limit , byte [ ] delimiter ) { int end pos = start pos ; final int delim limit = limit - delimiter . length + num ; while ( end pos < limit ) { if ( end pos < delim limit && delimiter next ( bytes , end pos , delimiter ) ) { break ; } end pos ++ ; } if ( end pos == start pos ) { set error state ( parse error state . empty column ) ; return - num ; } return end pos ; }	Returns the end position of a string.
protected static final int next string length ( byte [ ] bytes , int start pos , int length , char delimiter ) { if ( length <= num ) { throw new illegal argument exception ( str ) ; } int limited length = num ; final byte del byte = ( byte ) delimiter ; while ( limited length < length && bytes [ start pos + limited length ] != del byte ) { limited length ++ ; } return limited length ; }	Returns the length of a string.
public static < t > class < field parser < t > > get parser for type ( class < t > type ) { class < ? extends field parser < ? > > parser = parsers . get ( type ) ; if ( parser == null ) { return null ; } else { @ suppress warnings ( str ) class < field parser < t > > typed parser = ( class < field parser < t > > ) parser ; return typed parser ; } }	Gets the parser for the type specified by the given class.
protected void report all element key groups ( ) { preconditions . check state ( partitioning source . length >= number of elements ) ; for ( int i = num ; i < number of elements ; ++ i ) { int key group = key group range assignment . assign to key group ( key extractor function . extract key from element ( partitioning source [ i ] ) , total key groups ) ; report key group of element at index ( i , key group ) ; } }	This method iterates over the input data and reports the key-group for each element.
protected void report key group of element at index ( int index , int key group ) { final int key group index = key group - first key group ; element key groups [ index ] = key group index ; ++ counter histogram [ key group index ] ; }	This method reports in the bookkeeping data that the element at the given index belongs to the given key-group.
@ override public void open ( runtime context runtime context ) { this . runtime context = runtime context ; local rate bytes per second = global rate bytes per second / runtime context . get number of parallel subtasks ( ) ; this . rate limiter = rate limiter . create ( local rate bytes per second ) ; }	Creates a rate limiter with the runtime context provided.
protected static void validate zoo keeper config ( properties props ) { if ( props . get property ( str ) == null ) { throw new illegal argument exception ( str ) ; } if ( props . get property ( consumer config . group id config ) == null ) { throw new illegal argument exception ( str + consumer config . group id config + str ) ; } try {	Validate the ZK configuration, checking for required parameters.
private static void validate auto offset reset value ( properties config ) { final string val = config . get property ( consumer config . auto offset reset config , str ) ; if ( ! ( val . equals ( str ) || val . equals ( str ) || val . equals ( str ) || val . equals ( str ) ) ) {	Check for invalid "auto.offset.reset" values.
public boolean is canceled or failed ( ) { return execution state == execution state . canceling || execution state == execution state . canceled || execution state == execution state . failed ; }	Checks whether the task has failed, is canceled, or is being canceled at the moment.
private void release network resources ( ) { log . debug ( str , task name with subtask , get execution state ( ) ) ; for ( result partition partition : produced partitions ) { task event dispatcher . unregister partition ( partition . get partition id ( ) ) ; if ( is canceled or failed ( ) ) { partition . fail ( get failure cause ( ) ) ; } } close network resources ( ) ; }	Releases network resources before task exits.
private boolean transition state ( execution state current state , execution state new state , throwable cause ) { if ( state updater . compare and set ( this , current state , new state ) ) { if ( cause == null ) { log . info ( str , task name with subtask , execution id , current state , new state ) ; } else { log . info ( str , task name with subtask , execution id , current state , new state , cause ) ; } return bool ; } else { return bool ; } }	Try to transition the execution state from the current state to the new state.
public void trigger checkpoint barrier ( final long checkpoint id , final long checkpoint timestamp , final checkpoint options checkpoint options , final boolean advance to end of event time ) { final abstract invokable invokable = this . invokable ; final checkpoint meta data checkpoint meta data = new checkpoint meta data ( checkpoint id , checkpoint timestamp ) ; if ( execution state == execution state . running && invokable != null ) {	Calls the invokable to trigger a checkpoint.
private void execute async call runnable ( runnable runnable , string call name , boolean blocking ) {	Utility method to dispatch an asynchronous call on the invokable.
public static < k , v > merge result < k , v > merge right into left ( linked optional map < k , v > left , linked optional map < k , v > right ) { linked optional map < k , v > merged = new linked optional map < > ( left ) ; merged . put all ( right ) ; return new merge result < > ( merged , is left prefix of right ( left , right ) ) ; }	Tries to merges the keys and the values of .
public set < string > absent keys or values ( ) { return underlying map . entry set ( ) . stream ( ) . filter ( linked optional map :: key or value is absent ) . map ( entry :: get key ) . collect ( collectors . to collection ( linked hash set :: new ) ) ; }	Returns the key names of any keys or values that are absent.
public boolean has absent keys or values ( ) { for ( entry < string , key value < k , v > > entry : underlying map . entry set ( ) ) { if ( key or value is absent ( entry ) ) { return bool ; } } return bool ; }	Checks whether there are entries with absent keys or values.
public void add discovered partitions ( list < kafka topic partition > new partitions ) throws io , class not found exception { list < kafka topic partition state < kph > > new partition states = create partition state holders ( new partitions , kafka topic partition state sentinel . earliest offset , timestamp watermark mode , watermarks periodic , watermarks punctuated , user code class loader ) ; if ( use metrics ) { register offset metrics ( consumer metric group , new partition states ) ; } for ( kafka topic partition state < kph > new partition state : new partition states ) {	Adds a list of newly discovered partitions to the fetcher for consuming. This method creates the partition state holder for each new partition, using{.
public hash map < kafka topic partition , long > snapshot current state ( ) {	Takes a snapshot of the partition offsets.
protected void emit record ( t record , kafka topic partition state < kph > partition state , long offset ) throws exception { if ( record != null ) { if ( timestamp watermark mode == no timestamps watermarks ) {	Emits a record without attaching an existing timestamp to it.
protected void emit record with timestamp ( t record , kafka topic partition state < kph > partition state , long offset , long timestamp ) throws exception { if ( record != null ) { if ( timestamp watermark mode == no timestamps watermarks ) {	Emits a record attaching a timestamp to it.
private void emit record with timestamp and periodic watermark ( t record , kafka topic partition state < kph > partition state , long offset , long kafka event timestamp ) { @ suppress warnings ( str ) final kafka topic partition state with periodic watermarks < t , kph > with watermarks state = ( kafka topic partition state with periodic watermarks < t , kph > ) partition state ;	Record emission, if a timestamp will be attached from an assigner that isalso a periodic watermark generator.
private void emit record with timestamp and punctuated watermark ( t record , kafka topic partition state < kph > partition state , long offset , long kafka event timestamp ) { @ suppress warnings ( str ) final kafka topic partition state with punctuated watermarks < t , kph > with watermarks state = ( kafka topic partition state with punctuated watermarks < t , kph > ) partition state ;	Record emission, if a timestamp will be attached from an assigner that isalso a punctuated watermark generator.
private void update min punctuated watermark ( watermark next watermark ) { if ( next watermark . get timestamp ( ) > max watermark so far ) { long new min = long . max value ; for ( kafka topic partition state < ? > state : subscribed partition states ) { @ suppress warnings ( str ) final kafka topic partition state with punctuated watermarks < t , kph > with watermarks state = ( kafka topic partition state with punctuated watermarks < t , kph > ) state ; new min = math . min ( new min , with watermarks state . get current partition watermark ( ) ) ; }	Checks whether a new per-partition watermark is also a new cross-partition watermark.
public elasticsearch host ( string hostname , int port , string protocol ) { final host host = new host ( preconditions . check not null ( hostname ) , port , preconditions . check not null ( protocol ) ) ; hosts . add ( host ) ; return this ; }	Adds an Elasticsearch host to connect to.
public elasticsearch failure handler custom ( class < ? extends action request failure handler > failure handler class ) { internal properties . put string ( connector failure handler , elasticsearch validator . connector failure handler value custom ) ; internal properties . put class ( connector failure handler class , failure handler class ) ; return this ; }	Configures a failure handling strategy in case a request to Elasticsearch fails. This strategy allows for custom failure handling using a {.
public elasticsearch bulk flush max size ( string max size ) { internal properties . put memory size ( connector bulk flush max size , memory size . parse ( max size , memory size . memory unit . bytes ) ) ; return this ; }	Configures how to buffer elements before sending them in bulk to the cluster for efficiency. Sets the maximum size of buffered actions per bulk request (using the syntax of {.
public set < string > generate ids to abort ( ) { set < string > ids to abort = new hash set < > ( ) ; for ( int i = num ; i < safe scale down factor ; i ++ ) { ids to abort . add all ( generate ids to use ( i * pool size * total number of subtasks ) ) ; } return ids to abort ; }	If we have to abort previous transactional id in case of restart after a failure BEFORE first checkpointcompleted, we don't know what was the parallelism used in previous attempt.
@ override public void register table source ( string name ) { preconditions . check not null ( name ) ; table source < ? > table source = table factory util . find and create table source ( this ) ; table env . register table source ( name , table source ) ; }	Searches for the specified table source, configures it accordingly, and registers it asa table under the given name.
@ override public void register table sink ( string name ) { preconditions . check not null ( name ) ; table sink < ? > table sink = table factory util . find and create table sink ( this ) ; table env . register table sink ( name , table sink ) ; }	Searches for the specified table sink, configures it accordingly, and registers it asa table under the given name.
@ override public d with format ( format descriptor format ) { format descriptor = optional . of ( preconditions . check not null ( format ) ) ; return ( d ) this ; }	Specifies the format that defines how to read data from a connector.
@ override public d with schema ( schema schema ) { schema descriptor = optional . of ( preconditions . check not null ( schema ) ) ; return ( d ) this ; }	Specifies the resulting table schema.
private list < map < stream state handle , operator state handle > > init merge map list ( list < list < operator state handle > > parallel subtask states ) { int parallelism = parallel subtask states . size ( ) ; final list < map < stream state handle , operator state handle > > merge map list = new array list < > ( parallelism ) ; for ( list < operator state handle > previous parallel subtask state : parallel subtask states ) { merge map list . add ( previous parallel subtask state . stream ( ) . collect ( collectors . to map ( operator state handle :: get delegate state handle , function . identity ( ) ) ) ) ; } return merge map list ; }	Init the the list of StreamStateHandle -> OperatorStateHandle map with given parallelSubtaskStates when parallelism not changed.
private map < string , list < tuple2 < stream state handle , operator state handle . state meta info > > > collect union states ( list < list < operator state handle > > parallel subtask states ) { map < string , list < tuple2 < stream state handle , operator state handle . state meta info > > > union states = new hash map < > ( parallel subtask states . size ( ) ) ; for ( list < operator state handle > sub task state : parallel subtask states ) { for ( operator state handle operator state handle : sub task state ) { if ( operator state handle == null ) { continue ; } final set < map . entry < string , operator state handle . state meta info > > partition offset entries = operator state handle . get state name to partition offsets ( ) . entry set ( ) ; partition offset entries . stream ( ) . filter ( entry -> entry . get value ( ) . get distribution mode ( ) . equals ( operator state handle . mode . union ) ) . for each ( entry -> { list < tuple2 < stream state handle , operator state handle . state meta info > > state locations = union states . compute if absent ( entry . get key ( ) , k -> new array list < > ( parallel subtask states . size ( ) * partition offset entries . size ( ) ) ) ; state locations . add ( tuple2 . of ( operator state handle . get delegate state handle ( ) , entry . get value ( ) ) ) ; } ) ; } } return union states ; }	Collect union states from given parallelSubtaskStates.
@ suppress warnings ( str ) private group by state name results group by state mode ( list < list < operator state handle > > previous parallel subtask states ) {	Group by the different named states.
private list < map < stream state handle , operator state handle > > repartition ( group by state name results name to state by mode , int new parallelism ) {	Repartition all named states.
private void repartition split state ( map < string , list < tuple2 < stream state handle , operator state handle . state meta info > > > name to distribute state , int new parallelism , list < map < stream state handle , operator state handle > > merge map list ) { int start parallel op = num ;	Repartition SPLIT_DISTRIBUTE state.
private void repartition union state ( map < string , list < tuple2 < stream state handle , operator state handle . state meta info > > > union state , list < map < stream state handle , operator state handle > > merge map list ) { for ( map < stream state handle , operator state handle > merge map : merge map list ) { for ( map . entry < string , list < tuple2 < stream state handle , operator state handle . state meta info > > > e : union state . entry set ( ) ) { for ( tuple2 < stream state handle , operator state handle . state meta info > handle with meta info : e . get value ( ) ) { operator state handle operator state handle = merge map . get ( handle with meta info . f0 ) ; if ( operator state handle == null ) { operator state handle = new operator stream state handle ( new hash map < > ( union state . size ( ) ) , handle with meta info . f0 ) ; merge map . put ( handle with meta info . f0 , operator state handle ) ; } operator state handle . get state name to partition offsets ( ) . put ( e . get key ( ) , handle with meta info . f1 ) ; } } } }	Repartition UNION state.
private void repartition broadcast state ( map < string , list < tuple2 < stream state handle , operator state handle . state meta info > > > broadcast state , list < map < stream state handle , operator state handle > > merge map list ) { int new parallelism = merge map list . size ( ) ; for ( int i = num ; i < new parallelism ; ++ i ) { final map < stream state handle , operator state handle > merge map = merge map list . get ( i ) ;	Repartition BROADCAST state.
private int binary search ( t record ) { int low = num ; int high = this . boundaries . length - num ; type comparator . extract keys ( record , keys , num ) ; while ( low <= high ) { final int mid = ( low + high ) > > > num ; final int result = compare keys ( flat comparators , keys , this . boundaries [ mid ] ) ; if ( result > num ) { low = mid + num ; } else if ( result < num ) { high = mid - num ; } else { return mid ; } }	Search the range index of input record.
public void serialize to pages without length ( binary row record , abstract paged output view out ) throws io { int remain size = record . get size in bytes ( ) ; int pos in seg of record = record . get offset ( ) ; int segment size = record . get segments ( ) [ num ] . size ( ) ; for ( memory segment seg of record : record . get segments ( ) ) { int n write = math . min ( segment size - pos in seg of record , remain size ) ; assert n write > num ; out . write ( seg of record , pos in seg of record , n write ) ;	Serialize row to pages without row length.
public void copy from pages to view ( abstract paged input view source , data output view target ) throws io { check skip read for fix length part ( source ) ; int length = source . read int ( ) ; target . write int ( length ) ; target . write ( source , length ) ; }	Copy a binaryRow which stored in paged input view to output view.
public void set driver key info ( field list keys , int id ) { this . set driver key info ( keys , get true array ( keys . size ( ) ) , id ) ; }	Sets the key field indexes for the specified driver comparator.
public void set driver key info ( field list keys , boolean [ ] sort order , int id ) { if ( id < num || id >= driver keys . length ) { throw new compiler exception ( str + super . get driver strategy ( ) . get num required comparators ( ) + str ) ; } this . driver keys [ id ] = keys ; this . driver sort orders [ id ] = sort order ; }	Sets the key field information for the specified driver comparator.
private void add contender ( embedded leader election service service , leader contender contender ) { synchronized ( lock ) { check state ( ! shutdown , str ) ; check state ( ! service . running , str ) ; try { if ( ! all leader contenders . add ( service ) ) { throw new illegal state exception ( str ) ; } service . contender = contender ; service . running = bool ; update leader ( ) . when complete ( ( a void , throwable ) -> { if ( throwable != null ) { fatal error ( throwable ) ; } } ) ; } catch ( throwable t ) { fatal error ( t ) ; } } }	Callback from leader contenders when they start their service.
private void remove contender ( embedded leader election service service ) { synchronized ( lock ) {	Callback from leader contenders when they stop their service.
private void confirm leader ( final embedded leader election service service , final uuid leader session id ) { synchronized ( lock ) {	Callback from leader contenders when they confirm a leader grant.
private static collection < string > get available metrics ( collection < ? extends metric store . component metric store > stores ) { set < string > unique metrics = new hash set < > ( num ) ; for ( metric store . component metric store store : stores ) { unique metrics . add all ( store . metrics . key set ( ) ) ; } return unique metrics ; }	Returns a JSON string containing a list of all available metrics in the given stores.
private aggregated metrics response body get aggregated metric values ( collection < ? extends metric store . component metric store > stores , list < string > requested metrics , metric accumulator factory requested aggregations factories ) { collection < aggregated metric > aggregated metrics = new array list < > ( requested metrics . size ( ) ) ; for ( string requested metric : requested metrics ) { final collection < double > values = new array list < > ( stores . size ( ) ) ; try { for ( metric store . component metric store store : stores ) { string string value = store . metrics . get ( requested metric ) ; if ( string value != null ) { values . add ( double . value of ( string value ) ) ; } } } catch ( number format exception nfe ) { log . warn ( str , requested metric , nfe ) ;	Extracts and aggregates all requested metrics from the given metric stores, and maps the result to a JSON string.
private static void set deserializer ( properties props ) { final string de ser name = byte array deserializer . class . get name ( ) ; object key de ser = props . get ( consumer config . key deserializer class config ) ; object val de ser = props . get ( consumer config . value deserializer class config ) ; if ( key de ser != null && ! key de ser . equals ( de ser name ) ) { log . warn ( str , consumer config . key deserializer class config ) ; } if ( val de ser != null && ! val de ser . equals ( de ser name ) ) { log . warn ( str , consumer config . value deserializer class config ) ; } props . put ( consumer config . key deserializer class config , de ser name ) ; props . put ( consumer config . value deserializer class config , de ser name ) ; }	Makes sure that the ByteArrayDeserializer is registered in the Kafka properties.
public void persist ( ) throws exception { if ( ! mapping . equals ( initial mapping ) ) { state . clear ( ) ; for ( map . entry < w , w > window : mapping . entry set ( ) ) { state . add ( new tuple2 < > ( window . get key ( ) , window . get value ( ) ) ) ; } } }	Persist the updated mapping to the given state if the mapping changed sinceinitialization.
private org . apache . hadoop . conf . configuration load hadoop config from flink ( ) { org . apache . hadoop . conf . configuration hadoop config = new org . apache . hadoop . conf . configuration ( ) ; for ( string key : flink config . key set ( ) ) { for ( string prefix : flink config prefixes ) { if ( key . starts with ( prefix ) ) { string new key = hadoop config prefix + key . substring ( prefix . length ( ) ) ; string new value = fix hadoop config ( key , flink config . get string ( key , null ) ) ; hadoop config . set ( new key , new value ) ; log . debug ( str , key , new key ) ; } } } return hadoop config ; }	add additional config entries from the Flink config to the Hadoop config.
private static boolean wait until lease is revoked ( final file system fs , final path path ) throws io { preconditions . check state ( fs instanceof distributed file system ) ; final distributed file system dfs = ( distributed file system ) fs ; dfs . recover lease ( path ) ; final deadline deadline = deadline . now ( ) . plus ( duration . of millis ( lease timeout ) ) ; final stop watch sw = new stop watch ( ) ; sw . start ( ) ; boolean is closed = dfs . is file closed ( path ) ; while ( ! is closed && deadline . has time left ( ) ) { try { thread . sleep ( num ) ; } catch ( interrupted exception e1 ) { throw new io ( str , e1 ) ; } is closed = dfs . is file closed ( path ) ; } return is closed ; }	Called when resuming execution after a failure and waits until the leaseof the file we are resuming is free.
@ override public completable future < void > on stop ( ) { log . info ( str , get address ( ) ) ; throwable throwable = null ; if ( resource manager connection != null ) { resource manager connection . close ( ) ; } for ( job manager connection job manager connection : job manager connections . values ( ) ) { try { disassociate from job manager ( job manager connection , new flink exception ( str ) ) ; } catch ( throwable t ) { throwable = exception utils . first or suppressed ( t , throwable ) ; } } job manager heartbeat manager . stop ( ) ; resource manager heartbeat manager . stop ( ) ; try { stop task executor services ( ) ; } catch ( exception e ) { throwable = exception utils . first or suppressed ( e , throwable ) ; } if ( throwable != null ) { return future utils . completed exceptionally ( new flink exception ( str , throwable ) ) ; } else { log . info ( str , get address ( ) ) ; return completable future . completed future ( null ) ; } }	Called to shut down the TaskManager.
public optional < operator back pressure stats > get operator back pressure stats ( execution job vertex vertex ) { synchronized ( lock ) { final operator back pressure stats stats = operator stats cache . get if present ( vertex ) ; if ( stats == null || back pressure stats refresh interval <= system . current time millis ( ) - stats . get end timestamp ( ) ) { trigger stack trace sample internal ( vertex ) ; } return optional . of nullable ( stats ) ; } }	Returns back pressure statistics for a operator.
private boolean trigger stack trace sample internal ( final execution job vertex vertex ) { assert ( thread . holds lock ( lock ) ) ; if ( shut down ) { return bool ; } if ( ! pending stats . contains ( vertex ) && ! vertex . get graph ( ) . get state ( ) . is globally terminal state ( ) ) { executor executor = vertex . get graph ( ) . get future executor ( ) ;	Triggers a stack trace sample for a operator to gather the back pressurestatistics.
private static string [ ] get cldb ( string authority ) throws io {	Retrieves the CLDB locations for the given MapR cluster name.
@ suppress warnings ( str ) @ public evolving public static < x > primitive array type info < x > get info for ( class < x > type ) { if ( ! type . is array ( ) ) { throw new invalid types exception ( str ) ; }	Tries to get the PrimitiveArrayTypeInfo for an array.
static void adjust auto commit config ( properties properties , offset commit mode offset commit mode ) { if ( offset commit mode == offset commit mode . on checkpoints || offset commit mode == offset commit mode . disabled ) { properties . set property ( consumer config . enable auto commit config , str ) ; } }	Make sure that auto commit is disabled when our offset commit mode is ON_CHECKPOINTS.This overwrites whatever setting the user configured in the properties.
protected flink kafka consumer base < t > set start from timestamp ( long startup offsets timestamp ) { check argument ( startup offsets timestamp >= num , str ) ; long current timestamp = system . current time millis ( ) ; check argument ( startup offsets timestamp <= current timestamp , str , startup offsets timestamp , current timestamp ) ; this . startup mode = startup mode . timestamp ; this . startup offsets timestamp = startup offsets timestamp ; this . specific startup offsets = null ; return this ; }	Version-specific subclasses which can expose the functionality should override and allow public access.
@ visible for testing public shared state registry key create shared state registry key from file name ( id sh id ) { return new shared state registry key ( string . value of ( backend identifier ) + str + key group range , sh id ) ; }	Create a unique key to register one of our shared state handles.
public static < l , r > either < l , r > left ( l value ) { return new left < l , r > ( value ) ; }	Create a Left value of Either.
public static < l , r > either < l , r > right ( r value ) { return new right < l , r > ( value ) ; }	Create a Right value of Either.
public throwable unwrap ( ) { throwable cause = get cause ( ) ; return ( cause instanceof wrapping runtime exception ) ? ( ( wrapping runtime exception ) cause ) . unwrap ( ) : cause ; }	Recursively unwraps this WrappingRuntimeException and its causes, getting the firstnon wrapping exception.
private kryo get kryo instance ( ) { try {	Returns the Chill Kryo Serializer which is implicitly added to the classpath via flink-runtime.Falls back to the default Kryo serializer if it can't be found.
private static linked hash map < string , kryo registration > build kryo registrations ( class < ? > serialized type , linked hash set < class < ? > > registered types , linked hash map < class < ? > , class < ? extends serializer < ? > > > registered types with serializer classes , linked hash map < class < ? > , execution config . serializable serializer < ? > > registered types with serializers ) { final linked hash map < string , kryo registration > kryo registrations = new linked hash map < > ( ) ; kryo registrations . put ( serialized type . get name ( ) , new kryo registration ( serialized type ) ) ; for ( class < ? > registered type : check not null ( registered types ) ) { kryo registrations . put ( registered type . get name ( ) , new kryo registration ( registered type ) ) ; } for ( map . entry < class < ? > , class < ? extends serializer < ? > > > registered type with serializer class entry : check not null ( registered types with serializer classes ) . entry set ( ) ) { kryo registrations . put ( registered type with serializer class entry . get key ( ) . get name ( ) , new kryo registration ( registered type with serializer class entry . get key ( ) , registered type with serializer class entry . get value ( ) ) ) ; } for ( map . entry < class < ? > , execution config . serializable serializer < ? > > registered type with serializer entry : check not null ( registered types with serializers ) . entry set ( ) ) { kryo registrations . put ( registered type with serializer entry . get key ( ) . get name ( ) , new kryo registration ( registered type with serializer entry . get key ( ) , registered type with serializer entry . get value ( ) ) ) ; }	Utility method that takes lists of registered types and their serializers, and resolvethem into a single list such that the result will resemble the final registrationresult in Kryo.
@ suppress warnings ( str ) public < k , vv , ev > graph < k , vv , ev > types ( class < k > vertex key , class < vv > vertex value , class < ev > edge value ) { if ( edge reader == null ) { throw new runtime exception ( str ) ; } data set < tuple3 < k , k , ev > > edges = edge reader . types ( vertex key , vertex key , edge value ) ;	Creates a Graph from CSV input with vertex values and edge values.The vertex values are specified through a vertices input file or a user-defined map function.
public < k , ev > graph < k , null value , ev > edge types ( class < k > vertex key , class < ev > edge value ) { if ( edge reader == null ) { throw new runtime exception ( str ) ; } data set < tuple3 < k , k , ev > > edges = edge reader . types ( vertex key , vertex key , edge value ) . name ( graph csv reader . class . get name ( ) ) ; return graph . from tuple data set ( edges , execution context ) ; }	Creates a Graph from CSV input with edge values, but without vertex values.
public < k > graph < k , null value , null value > key type ( class < k > vertex key ) { if ( edge reader == null ) { throw new runtime exception ( str ) ; } data set < edge < k , null value > > edges = edge reader . types ( vertex key , vertex key ) . name ( graph csv reader . class . get name ( ) ) . map ( new tuple2 to edge map < > ( ) ) . name ( str ) ; return graph . from data set ( edges , execution context ) ; }	Creates a Graph from CSV input without vertex values or edge values.
@ suppress warnings ( { str , str } ) public < k , vv > graph < k , vv , null value > vertex types ( class < k > vertex key , class < vv > vertex value ) { if ( edge reader == null ) { throw new runtime exception ( str ) ; } data set < edge < k , null value > > edges = edge reader . types ( vertex key , vertex key ) . name ( graph csv reader . class . get name ( ) ) . map ( new tuple2 to edge map < > ( ) ) . name ( str ) ;	Creates a Graph from CSV input without edge values.The vertex values are specified through a vertices input file or a user-defined map function.If no vertices input file is provided, the vertex IDs are automatically created from the edgesinput file.
public static void merge hadoop conf ( job conf job conf ) {	Merge HadoopConfiguration into JobConf.
@ suppress warnings ( str ) public completable future < stack trace sample > trigger stack trace sample ( execution vertex [ ] tasks to sample , int num samples , time delay between samples , int max stack trace depth ) { check not null ( tasks to sample , str ) ; check argument ( tasks to sample . length >= num , str ) ; check argument ( num samples >= num , str ) ; check argument ( max stack trace depth >= num , str ) ;	Triggers a stack trace sample to all tasks.
public void cancel stack trace sample ( int sample id , throwable cause ) { synchronized ( lock ) { if ( is shut down ) { return ; } pending stack trace sample sample = pending samples . remove ( sample id ) ; if ( sample != null ) { if ( cause != null ) { log . info ( str + sample id , cause ) ; } else { log . info ( str , sample id ) ; } sample . discard ( cause ) ; remember recent sample id ( sample id ) ; } } }	Cancels a pending sample.
public void shut down ( ) { synchronized ( lock ) { if ( ! is shut down ) { log . info ( str ) ; for ( pending stack trace sample pending : pending samples . values ( ) ) { pending . discard ( new runtime exception ( str ) ) ; } pending samples . clear ( ) ; is shut down = bool ; } } }	Shuts down the coordinator.
public void collect stack traces ( int sample id , id execution id , list < stack trace element [ ] > stack traces ) { synchronized ( lock ) { if ( is shut down ) { return ; } if ( log . is debug enabled ( ) ) { log . debug ( str , sample id , execution id ) ; } pending stack trace sample pending = pending samples . get ( sample id ) ; if ( pending != null ) { pending . collect stack traces ( execution id , stack traces ) ;	Collects stack traces of a task.
public < out > data stream source < out > from collection ( collection < out > data , type information < out > type info ) { preconditions . check not null ( data , str ) ;	Creates a data stream from the given non-empty collection.
private < out > data stream source < out > from parallel collection ( splittable iterator < out > iterator , type information < out > type info , string operator name ) { return add source ( new from splittable iterator function < > ( iterator ) , operator name , type info ) ; }	private helper for passing different names.
public static < t > data set < long value > count ( data set < t > input ) { return input . map ( new map to < > ( new long value ( num ) ) ) . returns ( long value type info ) . name ( str ) . reduce ( new add long value ( ) ) . name ( str ) ; }	Count the number of elements in a DataSet.
public static void install ( security configuration config ) throws exception {	Installs a process-wide security configuration.
private static void write header ( final byte buf buf , final message type message type ) { buf . write int ( version ) ; buf . write int ( message type . ordinal ( ) ) ; }	Helper for serializing the header.
private static byte buf write payload ( final byte buf allocator alloc , final long request id , final message type message type , final byte [ ] payload ) { final int frame length = header length + request id size + payload . length ; final byte buf buf = alloc . io buffer ( frame length + integer . bytes ) ; buf . write int ( frame length ) ; write header ( buf , message type ) ; buf . write long ( request id ) ; buf . write bytes ( payload ) ; return buf ; }	Helper for serializing the messages.
byte [ ] [ ] get family keys ( ) { charset c = charset . for name ( charset ) ; byte [ ] [ ] family keys = new byte [ this . family map . size ( ) ] [ ] ; int i = num ; for ( string name : this . family map . key set ( ) ) { family keys [ i ++ ] = name . get bytes ( c ) ; } return family keys ; }	Returns the HBase identifiers of all registered column families.
string [ ] get qualifier names ( string family ) { map < string , type information < ? > > qualifier map = family map . get ( family ) ; if ( qualifier map == null ) { throw new illegal argument exception ( str + family + str ) ; } string [ ] qualifier names = new string [ qualifier map . size ( ) ] ; int i = num ; for ( string qualifier : qualifier map . key set ( ) ) { qualifier names [ i ] = qualifier ; i ++ ; } return qualifier names ; }	Returns the names of all registered column qualifiers of a specific column family.
byte [ ] [ ] get qualifier keys ( string family ) { map < string , type information < ? > > qualifier map = family map . get ( family ) ; if ( qualifier map == null ) { throw new illegal argument exception ( str + family + str ) ; } charset c = charset . for name ( charset ) ; byte [ ] [ ] qualifier keys = new byte [ qualifier map . size ( ) ] [ ] ; int i = num ; for ( string name : qualifier map . key set ( ) ) { qualifier keys [ i ++ ] = name . get bytes ( c ) ; } return qualifier keys ; }	Returns the HBase identifiers of all registered column qualifiers for a specific column family.
type information < ? > [ ] get qualifier types ( string family ) { map < string , type information < ? > > qualifier map = family map . get ( family ) ; if ( qualifier map == null ) { throw new illegal argument exception ( str + family + str ) ; } type information < ? > [ ] type information = new type information [ qualifier map . size ( ) ] ; int i = num ; for ( type information < ? > type info : qualifier map . values ( ) ) { type information [ i ] = type info ; i ++ ; } return type information ; }	Returns the types of all registered column qualifiers of a specific column family.
@ public evolving public string get string ( config option < string > config option , string override default ) { object o = get raw value from option ( config option ) ; return o == null ? override default : o . to string ( ) ; }	Returns the value associated with the given config option as a string.If no value is mapped under any key of the option, it returns the specifieddefault instead of the option's default value.
@ public evolving public int get integer ( config option < integer > config option ) { object o = get value or default from option ( config option ) ; return convert to int ( o , config option . default value ( ) ) ; }	Returns the value associated with the given config option as an integer.
@ public evolving public int get integer ( config option < integer > config option , int override default ) { object o = get raw value from option ( config option ) ; if ( o == null ) { return override default ; } return convert to int ( o , config option . default value ( ) ) ; }	Returns the value associated with the given config option as an integer.If no value is mapped under any key of the option, it returns the specifieddefault instead of the option's default value.
@ public evolving public long get long ( config option < long > config option ) { object o = get value or default from option ( config option ) ; return convert to long ( o , config option . default value ( ) ) ; }	Returns the value associated with the given config option as a long integer.
@ public evolving public long get long ( config option < long > config option , long override default ) { object o = get raw value from option ( config option ) ; if ( o == null ) { return override default ; } return convert to long ( o , config option . default value ( ) ) ; }	Returns the value associated with the given config option as a long integer.If no value is mapped under any key of the option, it returns the specifieddefault instead of the option's default value.
@ public evolving public boolean get boolean ( config option < boolean > config option ) { object o = get value or default from option ( config option ) ; return convert to boolean ( o ) ; }	Returns the value associated with the given config option as a boolean.
@ public evolving public boolean get boolean ( config option < boolean > config option , boolean override default ) { object o = get raw value from option ( config option ) ; if ( o == null ) { return override default ; } return convert to boolean ( o ) ; }	Returns the value associated with the given config option as a boolean.If no value is mapped under any key of the option, it returns the specifieddefault instead of the option's default value.
@ public evolving public float get float ( config option < float > config option ) { object o = get value or default from option ( config option ) ; return convert to float ( o , config option . default value ( ) ) ; }	Returns the value associated with the given config option as a float.
@ public evolving public float get float ( config option < float > config option , float override default ) { object o = get raw value from option ( config option ) ; if ( o == null ) { return override default ; } return convert to float ( o , config option . default value ( ) ) ; }	Returns the value associated with the given config option as a float.If no value is mapped under any key of the option, it returns the specifieddefault instead of the option's default value.
@ public evolving public string get value ( config option < ? > config option ) { object o = get value or default from option ( config option ) ; return o == null ? null : o . to string ( ) ; }	Returns the value associated with the given config option as a string.
@ public evolving public < t extends enum < t > > t get enum ( final class < t > enum class , final config option < string > config option ) { check not null ( enum class , str ) ; check not null ( config option , str ) ; final string config value = get string ( config option ) ; try { return enum . value of ( enum class , config value . to upper case ( locale . root ) ) ; } catch ( final illegal argument exception | null pointer exception e ) { final string error message = string . format ( str , config option . key ( ) , arrays . to string ( enum class . get enum constants ( ) ) , config value ) ; throw new illegal argument exception ( error message , e ) ; } }	Returns the value associated with the given config option as an enum.
public void add all ( configuration other , string prefix ) { final string builder bld = new string builder ( ) ; bld . append ( prefix ) ; final int pl = bld . length ( ) ; synchronized ( this . conf data ) { synchronized ( other . conf data ) { for ( map . entry < string , object > entry : other . conf data . entry set ( ) ) { bld . set length ( pl ) ; bld . append ( entry . get key ( ) ) ; this . conf data . put ( bld . to string ( ) , entry . get value ( ) ) ; } } } }	Adds all entries from the given configuration into this configuration.
@ public evolving public boolean contains ( config option < ? > config option ) { synchronized ( this . conf data ) {	Checks whether there is an entry for the given config option.
public < t > boolean remove config ( config option < t > config option ) { synchronized ( this . conf data ) {	Removes given config option from the configuration.
@ override public void upload part ( fs file ) throws io {	Adds a part to the uploads without any size limitations.
@ override public s3 recoverable snapshot and get recoverable ( @ nullable final fs incomplete part file ) throws io { final string incomplete part object name = safely upload small part ( incomplete part file ) ;	Creates a snapshot of this MultiPartUpload, from which the upload can be resumed. Data buffered locally which is less than{.
public connected streams < i , i > key by ( int key position1 , int key position2 ) { return new connected streams < > ( this . environment , input stream1 . key by ( key position1 ) , input stream2 . key by ( key position2 ) ) ; }	KeyBy operation for connected data stream.
public connected streams < i , i > key by ( int [ ] key positions1 , int [ ] key positions2 ) { return new connected streams < > ( environment , input stream1 . key by ( key positions1 ) , input stream2 . key by ( key positions2 ) ) ; }	KeyBy operation for connected data stream.
public connected streams < i , i > key by ( key selector < i , ? > key selector1 , key selector < i , ? > key selector2 ) { return new connected streams < > ( environment , input stream1 . key by ( key selector1 ) , input stream2 . key by ( key selector2 ) ) ; }	KeyBy operation for connected data stream.
public < key > connected streams < i , i > key by ( key selector < i , key > key selector1 , key selector < i , key > key selector2 , type information < key > key type ) { return new connected streams < > ( environment , input stream1 . key by ( key selector1 , key type ) , input stream2 . key by ( key selector2 , key type ) ) ; }	KeyBy operation for connected data stream.
public list < memory segment > close ( ) throws io {	Closes this OutputView, closing the underlying writer and returning all memory segments.
public type serializer < uk > get key serializer ( ) { final type serializer < map < uk , uv > > raw serializer = get serializer ( ) ; if ( ! ( raw serializer instanceof map serializer ) ) { throw new illegal state exception ( str ) ; } return ( ( map serializer < uk , uv > ) raw serializer ) . get key serializer ( ) ; }	Gets the serializer for the keys in the state.
protected boolean is element late ( stream record < in > element ) { return ( window assigner . is event time ( ) ) && ( element . get timestamp ( ) + allowed lateness <= internal timer service . current watermark ( ) ) ; }	Decide if a record is currently late, based on current watermark and allowed lateness.
protected void delete cleanup timer ( w window ) { long cleanup time = cleanup time ( window ) ; if ( cleanup time == long . max value ) {	Deletes the cleanup timer set for the contents of the provided window.
public rowtime watermarks periodic bounded ( long delay ) { internal properties . put string ( rowtime watermarks type , rowtime watermarks type value periodic bounded ) ; internal properties . put long ( rowtime watermarks delay , delay ) ; return this ; }	Sets a built-in watermark strategy for rowtime attributes which are out-of-order by a boundedtime interval.
public map < string , accumulator < ? , ? > > deserialize user accumulators ( class loader class loader ) throws io , class not found exception { return user accumulators . deserialize value ( class loader ) ; }	Gets the user-defined accumulators values.
public void reset ( ) { next window = null ; watermark = long . min value ; trigger window start index = num ; empty window triggered = bool ; reset buffer ( ) ; }	Reset for next group.
public boolean has trigger window ( ) { skip empty window ( ) ; preconditions . check state ( watermark == long . min value || next window != null , str ) ; return next window != null && next window . get end ( ) <= watermark ; }	Check if there are windows could be triggered according to the current watermark.
@ override public void user event triggered ( channel handler context ctx , object msg ) throws exception { if ( msg instanceof remote input channel ) { boolean trigger write = input channels with credit . is empty ( ) ; input channels with credit . add ( ( remote input channel ) msg ) ; if ( trigger write ) { write and flush next message if possible ( ctx . channel ( ) ) ; } } else { ctx . fire user event triggered ( msg ) ; } }	Triggered by notifying credit available in the client handler pipeline.
private void write and flush next message if possible ( channel channel ) { if ( channel error . get ( ) != null || ! channel . is writable ( ) ) { return ; } while ( bool ) { remote input channel input channel = input channels with credit . poll ( ) ;	Tries to write&flush unannounced credits for the next input channel in queue.
public list < kafka topic partition leader > get partition leaders for topics ( list < string > topics ) { list < kafka topic partition leader > partitions = new linked list < > ( ) ; retry loop : for ( int retry = num ; retry < num retries ; retry ++ ) { brokers loop : for ( int arr idx = num ; arr idx < seed broker addresses . length ; arr idx ++ ) { log . info ( str , seed broker addresses [ current contact seed broker index ] , retry , num retries ) ; try {	Send request to Kafka to get partitions for topics.
private void use next address as new contact seed broker ( ) { if ( ++ current contact seed broker index == seed broker addresses . length ) { current contact seed broker index = num ; } url new contact url = net utils . get correct hostname port ( seed broker addresses [ current contact seed broker index ] ) ; this . consumer = new simple consumer ( new contact url . get host ( ) , new contact url . get port ( ) , so timeout , buffer size , dummy client id ) ; }	Re-establish broker connection using the next available seed broker address.
private static node broker to node ( broker broker ) { return new node ( broker . id ( ) , broker . host ( ) , broker . port ( ) ) ; }	Turn a broker instance into a node instance.
private static void validate seed brokers ( string [ ] seed brokers , exception exception ) { if ( ! ( exception instanceof closed channel exception ) ) { return ; } int unknown hosts = num ; for ( string broker : seed brokers ) { url broker url = net utils . get correct hostname port ( broker . trim ( ) ) ; try { inet address . get by name ( broker url . get host ( ) ) ; } catch ( unknown host exception e ) { unknown hosts ++ ; } }	Validate that at least one seed broker is valid in case of aClosedChannelException.
public static void max normalized key ( memory segment target , int offset , int num bytes ) {	Max unsigned byte is -1.
public static void put decimal normalized key ( decimal record , memory segment target , int offset , int len ) { assert record . get precision ( ) <= decimal . max compact precision ; put long normalized key ( record . to unscaled long ( ) , target , offset , len ) ; }	Just support the compact precision decimal.
public static void merge hadoop conf ( configuration hadoop config ) {	Merge HadoopConfiguration into Configuration.
public table stats copy ( ) { table stats copy = new table stats ( this . row count ) ; for ( map . entry < string , column stats > entry : this . col stats . entry set ( ) ) { copy . col stats . put ( entry . get key ( ) , entry . get value ( ) . copy ( ) ) ; } return copy ; }	Create a deep copy of "this" instance.
private static linked hash set < class < ? > > get registered subclasses from execution config ( class < ? > base pojo class , execution config execution config ) { linked hash set < class < ? > > subclasses in registration order = new linked hash set < > ( execution config . get registered pojo types ( ) . size ( ) ) ; for ( class < ? > registered class : execution config . get registered pojo types ( ) ) { if ( registered class . equals ( base pojo class ) ) { continue ; } if ( ! base pojo class . is assignable from ( registered class ) ) { continue ; } subclasses in registration order . add ( registered class ) ; } return subclasses in registration order ; }	Extracts the subclasses of the base POJO class registered in the execution config.
private static linked hash map < class < ? > , integer > create registered subclass tags ( linked hash set < class < ? > > registered subclasses ) { final linked hash map < class < ? > , integer > class to tag = new linked hash map < > ( ) ; int id = num ; for ( class < ? > registered class : registered subclasses ) { class to tag . put ( registered class , id ) ; id ++ ; } return class to tag ; }	Builds map of registered subclasses to their class tags.Class tags will be integers starting from 0, assigned incrementally with the order of provided subclasses.
private static type serializer < ? > [ ] create registered subclass serializers ( linked hash set < class < ? > > registered subclasses , execution config execution config ) { final type serializer < ? > [ ] subclass serializers = new type serializer [ registered subclasses . size ( ) ] ; int i = num ; for ( class < ? > registered class : registered subclasses ) { subclass serializers [ i ] = type extractor . create type info ( registered class ) . create serializer ( execution config ) ; i ++ ; } return subclass serializers ; }	Creates an array of serializers for provided list of registered subclasses.Order of returned serializers will correspond to order of provided subclasses.
type serializer < ? > get subclass serializer ( class < ? > subclass ) { type serializer < ? > result = subclass serializer cache . get ( subclass ) ; if ( result == null ) { result = create subclass serializer ( subclass ) ; subclass serializer cache . put ( subclass , result ) ; } return result ; }	Fetches cached serializer for a non-registered subclass;also creates the serializer if it doesn't exist yet.This method is also exposed to package-private accessfor testing purposes.
private static < t > pojo serializer snapshot < t > build snapshot ( class < t > pojo type , linked hash map < class < ? > , integer > registered subclasses to tags , type serializer < ? > [ ] registered subclass serializers , field [ ] fields , type serializer < ? > [ ] field serializers , map < class < ? > , type serializer < ? > > non registered subclass serializer cache ) { final linked hash map < class < ? > , type serializer < ? > > subclass registry = new linked hash map < > ( registered subclasses to tags . size ( ) ) ; for ( map . entry < class < ? > , integer > entry : registered subclasses to tags . entry set ( ) ) { subclass registry . put ( entry . get key ( ) , registered subclass serializers [ entry . get value ( ) ] ) ; } return new pojo serializer snapshot < > ( pojo type , fields , field serializers , subclass registry , non registered subclass serializer cache ) ; }	Build and return a snapshot of the serializer's parameters and currently cached serializers.
public static < t > class < t > compile ( class loader cl , string name , string code ) { tuple2 < class loader , string > cache key = tuple2 . of ( cl , name ) ; class < ? > clazz = compiled cache . get if present ( cache key ) ; if ( clazz == null ) { clazz = do compile ( cl , name , code ) ; compiled cache . put ( cache key , clazz ) ; }	Compiles a generated code to a Class.
private static string add line number ( string code ) { string [ ] lines = code . split ( str ) ; string builder builder = new string builder ( ) ; for ( int i = num ; i < lines . length ; i ++ ) { builder . append ( str ) . append ( i + num ) . append ( str ) . append ( lines [ i ] ) . append ( str ) ; } return builder . to string ( ) ; }	To output more information when an error occurs.Generally, when cook fails, it shows which line is wrong.
protected final void add closeable internal ( closeable closeable , t meta data ) { synchronized ( get synchronization lock ( ) ) { closeable to ref . put ( closeable , meta data ) ; } }	Adds a mapping to the registry map, respecting locking.
private void buffer rows1 ( ) throws io { binary row copy = key1 . copy ( ) ; buffer1 . reset ( ) ; do { buffer1 . add ( row1 ) ; } while ( next row1 ( ) && key comparator . compare ( key1 , copy ) == num ) ; buffer1 . complete ( ) ; }	Buffer rows from iterator1 with same key.
private void buffer rows2 ( ) throws io { binary row copy = key2 . copy ( ) ; buffer2 . reset ( ) ; do { buffer2 . add ( row2 ) ; } while ( next row2 ( ) && key comparator . compare ( key2 , copy ) == num ) ; buffer2 . complete ( ) ; }	Buffer rows from iterator2 with same key.
public static < t > type serializer schema compatibility < t > compatible with reconfigured serializer ( type serializer < t > reconfigured serializer ) { return new type serializer schema compatibility < > ( type . compatible with reconfigured serializer , preconditions . check not null ( reconfigured serializer ) ) ; }	Returns a result that indicates a reconfigured version of the new serializer is compatible, and should beused instead of the original new serializer.
public static checkpoint properties for checkpoint ( checkpoint retention policy policy ) { switch ( policy ) { case never retain after termination : return checkpoint never retained ; case retain on failure : return checkpoint retained on failure ; case retain on cancellation : return checkpoint retained on cancellation ; default : throw new illegal argument exception ( str + policy ) ; } }	Creates the checkpoint properties for a checkpoint.
public long parameter set minimum value ( long minimum value ) { if ( has maximum value ) { util . check parameter ( minimum value <= maximum value , str + minimum value + str + maximum value + str ) ; } this . has minimum value = bool ; this . minimum value = minimum value ; return this ; }	Set the minimum value.
public long parameter set maximum value ( long maximum value ) { if ( has minimum value ) { util . check parameter ( maximum value >= minimum value , str + maximum value + str + minimum value + str ) ; } this . has maximum value = bool ; this . maximum value = maximum value ; return this ; }	Set the maximum value.
protected void clean file ( string path ) { try { print writer writer ; writer = new print writer ( path ) ; writer . print ( str ) ; writer . close ( ) ; } catch ( file not found exception e ) { throw new runtime exception ( str + e . get message ( ) , e ) ; } }	Creates target file if it does not exist, cleans it if it exists.
@ override public iterator < map . entry < k , v > > iterator ( ) { return collections . unmodifiable set ( state . entry set ( ) ) . iterator ( ) ; }	Iterates over all the mappings in the state.
public static void send not modified ( channel handler context ctx ) { full http response response = new default full http response ( http 1 1 , not modified ) ; set date header ( response ) ;	Send the "304 Not Modified" response.
public static void set content type header ( http response response , file file ) { string mime type = mime types . get mime type for file name ( file . get name ( ) ) ; string mime final = mime type != null ? mime type : mime types . get default mime type ( ) ; response . headers ( ) . set ( content type , mime final ) ; }	Sets the content type header for the HTTP Response.
public void add data sink ( generic data sink base < ? > sink ) { check not null ( sink , str ) ; if ( ! this . sinks . contains ( sink ) ) { this . sinks . add ( sink ) ; } }	Adds a data sink to the set of sinks in this program.
@ override public void accept ( visitor < operator < ? > > visitor ) { for ( generic data sink base < ? > sink : this . sinks ) { sink . accept ( visitor ) ; } }	Traverses the job depth first from all data sinks on towards the sources.
public static int cal operator parallelism ( double row count , configuration table conf ) { int max parallelism = get operator max parallelism ( table conf ) ; int min parallelism = get operator min parallelism ( table conf ) ; int result parallelism = ( int ) ( row count / get row count per partition ( table conf ) ) ; return math . max ( math . min ( result parallelism , max parallelism ) , min parallelism ) ; }	Calculates operator parallelism based on rowcount of the operator.
public checkpoint stats snapshot create snapshot ( ) { checkpoint stats snapshot snapshot = latest snapshot ;	Creates a new snapshot of the available stats.
pending checkpoint stats report pending checkpoint ( long checkpoint id , long trigger timestamp , checkpoint properties props ) { concurrent hash map < id , task state stats > task state stats = create empty task state stats map ( ) ; pending checkpoint stats pending = new pending checkpoint stats ( checkpoint id , trigger timestamp , props , total subtask count , task state stats , new pending checkpoint stats callback ( ) ) ; stats read write lock . lock ( ) ; try { counts . increment in progress checkpoints ( ) ; history . add in progress checkpoint ( pending ) ; dirty = bool ; } finally { stats read write lock . unlock ( ) ; } return pending ; }	Creates a new pending checkpoint tracker.
void report restored checkpoint ( restored checkpoint stats restored ) { check not null ( restored , str ) ; stats read write lock . lock ( ) ; try { counts . increment restored checkpoints ( ) ; latest restored checkpoint = restored ; dirty = bool ; } finally { stats read write lock . unlock ( ) ; } }	Callback when a checkpoint is restored.
private void report completed checkpoint ( completed checkpoint stats completed ) { stats read write lock . lock ( ) ; try { latest completed checkpoint = completed ; counts . increment completed checkpoints ( ) ; history . replace pending checkpoint by id ( completed ) ; summary . update summary ( completed ) ; dirty = bool ; } finally { stats read write lock . unlock ( ) ; } }	Callback when a checkpoint completes.
private void report failed checkpoint ( failed checkpoint stats failed ) { stats read write lock . lock ( ) ; try { counts . increment failed checkpoints ( ) ; history . replace pending checkpoint by id ( failed ) ; dirty = bool ; } finally { stats read write lock . unlock ( ) ; } }	Callback when a checkpoint fails.
private void register metrics ( metric group metric group ) { metric group . gauge ( number of checkpoints metric , new checkpoints counter ( ) ) ; metric group . gauge ( number of in progress checkpoints metric , new in progress checkpoints counter ( ) ) ; metric group . gauge ( number of completed checkpoints metric , new completed checkpoints counter ( ) ) ; metric group . gauge ( number of failed checkpoints metric , new failed checkpoints counter ( ) ) ; metric group . gauge ( latest restored checkpoint timestamp metric , new latest restored checkpoint timestamp gauge ( ) ) ; metric group . gauge ( latest completed checkpoint size metric , new latest completed checkpoint size gauge ( ) ) ; metric group . gauge ( latest completed checkpoint duration metric , new latest completed checkpoint duration gauge ( ) ) ; metric group . gauge ( latest completed checkpoint alignment buffered metric , new latest completed checkpoint alignment buffered gauge ( ) ) ; metric group . gauge ( latest completed checkpoint external path metric , new latest completed checkpoint external path gauge ( ) ) ; }	Register the exposed metrics.
@ override public void configure ( configuration parameters ) { super . configure ( parameters ) ;	Configures this input format by reading the path to the file from the configuration and the string thatdefines the record delimiter.
private boolean fill buffer ( int offset ) throws io { int max read length = this . read buffer . length - offset ;	Fills the read buffer with bytes read from the file starting from an offset.
@ override public void set ( final iterator < tuple2 < key , value > > iterator ) { this . iterator = iterator ; if ( this . has next ( ) ) { final tuple2 < key , value > tuple = iterator . next ( ) ; this . cur key = key serializer . copy ( tuple . f0 ) ; this . first value = tuple . f1 ; this . at first = bool ; } else { this . at first = bool ; } }	Set the Flink iterator to wrap.
private static void fix ( indexed sortable s , int p n , int p o , int r n , int r o ) { if ( s . compare ( p n , p o , r n , r o ) > num ) { s . swap ( p n , p o , r n , r o ) ; } }	Fix the records into sorted order, swapping when the first record isgreater than the second record.
public static < w extends window > after first element periodic < w > every ( duration time ) { return new after first element periodic < > ( time . to millis ( ) ) ; }	Creates a trigger that fires by a certain interval after reception of the first element.
@ override public void invoke ( in value ) throws exception { byte [ ] msg = schema . serialize ( value ) ; try { output stream . write ( msg ) ; if ( auto flush ) { output stream . flush ( ) ; } } catch ( io e ) {	Called when new data arrives to the sink, and forwards it to Socket.
boolean report subtask stats ( id job vertex id , subtask state stats subtask ) { task state stats task state stats = task stats . get ( job vertex id ) ; if ( task state stats != null && task state stats . report subtask stats ( subtask ) ) { current num acknowledged subtasks ++ ; latest acknowledged subtask = subtask ; current state size += subtask . get state size ( ) ; long alignment buffered = subtask . get alignment buffered ( ) ; if ( alignment buffered > num ) { current alignment buffered += alignment buffered ; } return bool ; } else { return bool ; } }	Reports statistics for a single subtask.
completed checkpoint stats . discard callback report completed checkpoint ( string external pointer ) { completed checkpoint stats completed = new completed checkpoint stats ( checkpoint id , trigger timestamp , props , number of subtasks , new hash map < > ( task stats ) , current num acknowledged subtasks , current state size , current alignment buffered , latest acknowledged subtask , external pointer ) ; tracker callback . report completed checkpoint ( completed ) ; return completed . get discard callback ( ) ; }	Reports a successfully completed pending checkpoint.
public void set broadcast inputs ( map < operator < ? > , optimizer node > operator to node , execution mode default exchange mode ) {	This function connects the operators that produce the broadcast inputs to this operator.
public void set parallelism ( int parallelism ) { if ( parallelism < num && parallelism != execution config . parallelism default ) { throw new illegal argument exception ( str + parallelism + str ) ; } this . parallelism = parallelism ; }	Sets the parallelism for this optimizer node.The parallelism denotes how many parallel instances of the operator will bespawned during the execution.
public void compute union of interesting properties from successors ( ) { list < dag connection > conns = get outgoing connections ( ) ; if ( conns . size ( ) == num ) {	Computes all the interesting properties that are relevant to this node.
protected boolean are branch compatible ( plan node plan1 , plan node plan2 ) { if ( plan1 == null || plan2 == null ) { throw new null pointer exception ( ) ; }	Checks whether to candidate plans for the sub-plan of this node are comparable.
public < i , o > heartbeat manager < i , o > create heartbeat manager ( id resource id , heartbeat listener < i , o > heartbeat listener , scheduled executor scheduled executor , logger log ) { return new heartbeat manager impl < > ( heartbeat timeout , resource id , heartbeat listener , scheduled executor , scheduled executor , log ) ; }	Creates a heartbeat manager which does not actively send heartbeats.
public < i , o > heartbeat manager < i , o > create heartbeat manager sender ( id resource id , heartbeat listener < i , o > heartbeat listener , scheduled executor scheduled executor , logger log ) { return new heartbeat manager sender impl < > ( heartbeat interval , heartbeat timeout , resource id , heartbeat listener , scheduled executor , scheduled executor , log ) ; }	Creates a heartbeat manager which actively sends heartbeats to monitoring targets.
protected long adjust run loop frequency ( long processing start time nanos , long processing end time nanos ) throws interrupted exception { long end time nanos = processing end time nanos ; if ( fetch interval millis != num ) { long processing time nanos = processing end time nanos - processing start time nanos ; long sleep time millis = fetch interval millis - ( processing time nanos / num ) ; if ( sleep time millis > num ) { thread . sleep ( sleep time millis ) ; end time nanos = system . nano time ( ) ; shard metrics reporter . set sleep time millis ( sleep time millis ) ; } } return end time nanos ; }	Adjusts loop timing to match target frequency if specified.
private int adapt records to read ( long run loop time nanos , int num records , long record batch size bytes , int max number of records per fetch ) { if ( use adaptive reads && num records != num && run loop time nanos != num ) { long average record size bytes = record batch size bytes / num records ;	Calculates how many records to read each time through the loop based on a target throughputand the measured frequenecy of the loop.
public class < ? extends abstract invokable > get invokable class ( class loader cl ) { if ( cl == null ) { throw new null pointer exception ( str ) ; } if ( invokable class name == null ) { return null ; } try { return class . for name ( invokable class name , bool , cl ) . as subclass ( abstract invokable . class ) ; } catch ( class not found exception e ) { throw new runtime exception ( str , e ) ; } catch ( class cast exception e ) { throw new runtime exception ( str + abstract invokable . class . get name ( ) , e ) ; } }	Returns the invokable class which represents the task of this vertex.
public void set resources ( resource spec min resources , resource spec preferred resources ) { this . min resources = check not null ( min resources ) ; this . preferred resources = check not null ( preferred resources ) ; }	Sets the minimum and preferred resources for the task.
public void set slot sharing group ( slot sharing group grp ) { if ( this . slot sharing group != null ) { this . slot sharing group . remove vertex from group ( id ) ; } this . slot sharing group = grp ; if ( grp != null ) { grp . add vertex to group ( id ) ; } }	Associates this vertex with a slot sharing group for scheduling.
private void ensure capacity ( int min capacity ) { long current capacity = data . length ; if ( min capacity <= current capacity ) { return ; }	If the size of the array is insufficient to hold the given capacity thencopy the array into a new, larger array.
private void unregister job from high availability ( ) { try { running jobs registry . set job finished ( job graph . get job id ( ) ) ; } catch ( throwable t ) { log . error ( str + str , job graph . get name ( ) , job graph . get job id ( ) , t ) ; } }	Marks this runner's job as not running.
private void failover ( long global mod version of failover ) { if ( ! execution graph . get restart strategy ( ) . can restart ( ) ) { execution graph . fail global ( new flink exception ( str ) ) ; } else { job status cur status = this . state ; if ( cur status . equals ( job status . running ) ) { cancel ( global mod version of failover ) ; } else if ( cur status . equals ( job status . canceled ) ) { reset ( global mod version of failover ) ; } else { log . info ( str , id , state ) ; } } }	Notice the region to failover,.
private void cancel ( final long global mod version of failover ) { execution graph . get job master main thread executor ( ) . assert running in main thread ( ) ; while ( bool ) { job status cur status = this . state ; if ( cur status . equals ( job status . running ) ) { if ( transition state ( cur status , job status . cancelling ) ) { create termination future over all connected vertexes ( ) . then accept ( ( nullptr ) -> all vertices in terminal state ( global mod version of failover ) ) ; break ; } } else { log . info ( str , id , state ) ; break ; } } }	cancel all executions in this sub graph.
private void reset ( long global mod version of failover ) { try {	reset all executions in this sub graph.
private void restart ( long global mod version of failover ) { try { if ( transition state ( job status . created , job status . running ) ) {	restart all executions in this sub graph.
public static long get managed memory size ( configuration configuration ) { long managed memory size ; string managed memory size default val = task manager options . managed memory size . default value ( ) ; if ( ! configuration . get string ( task manager options . managed memory size ) . equals ( managed memory size default val ) ) { try { managed memory size = memory size . parse ( configuration . get string ( task manager options . managed memory size ) , mega bytes ) . get mebi bytes ( ) ; } catch ( illegal argument exception e ) { throw new illegal configuration exception ( str + task manager options . managed memory size . key ( ) , e ) ; } } else { managed memory size = long . value of ( managed memory size default val ) ; } check config parameter ( configuration . get string ( task manager options . managed memory size ) . equals ( task manager options . managed memory size . default value ( ) ) || managed memory size > num , managed memory size , task manager options . managed memory size . key ( ) , str + str ) ; return managed memory size ; }	Parses the configuration to get the managed memory size and validates the value.
public static float get managed memory fraction ( configuration configuration ) { float managed memory fraction = configuration . get float ( task manager options . managed memory fraction ) ; check config parameter ( managed memory fraction > num && managed memory fraction < num , managed memory fraction , task manager options . managed memory fraction . key ( ) , str ) ; return managed memory fraction ; }	Parses the configuration to get the fraction of managed memory and validates the value.
public static memory type get memory type ( configuration configuration ) {	Parses the configuration to get the type of memory.
public static int get slot ( configuration configuration ) { int slots = configuration . get integer ( task manager options . num task slots , num ) ;	Parses the configuration to get the number of slots and validates the value.
public static void check config parameter ( boolean condition , object parameter , string name , string error message ) throws illegal configuration exception { if ( ! condition ) { throw new illegal configuration exception ( str + name + str + parameter + str + error message ) ; } }	Validates a condition for a config parameter and displays a standard exception, if thethe condition does not hold.
public static yarn high availability services for single job app master ( configuration flink config , org . apache . hadoop . conf . configuration hadoop config ) throws io { check not null ( flink config , str ) ; check not null ( hadoop config , str ) ; final high availability mode mode = high availability mode . from config ( flink config ) ; switch ( mode ) { case none : return new yarn intra non ha master services ( flink config , hadoop config ) ; case zookeeper : throw new unsupported operation exception ( str ) ; default : throw new illegal configuration exception ( str + mode ) ; } }	Creates the high-availability services for a single-job Flink YARN application, to beused in the Application Master that runs both ResourceManager and JobManager.
public static yarn high availability services for yarn task manager ( configuration flink config , org . apache . hadoop . conf . configuration hadoop config ) throws io { check not null ( flink config , str ) ; check not null ( hadoop config , str ) ; final high availability mode mode = high availability mode . from config ( flink config ) ; switch ( mode ) { case none : return new yarn pre configured master non ha services ( flink config , hadoop config , high availability services utils . address resolution . try address resolution ) ; case zookeeper : throw new unsupported operation exception ( str ) ; default : throw new illegal configuration exception ( str + mode ) ; } }	Creates the high-availability services for the TaskManagers participating ina Flink YARN application.
@ override public list < memory segment > close ( ) throws io { if ( this . closed ) { throw new illegal state exception ( str ) ; } this . closed = bool ;	Closes this InputView, closing the underlying reader and returning all memory segments.
protected void send read request ( memory segment seg ) throws io { if ( this . num requests remaining != num ) { this . reader . read block ( seg ) ; if ( this . num requests remaining != - num ) { this . num requests remaining -- ; } } else {	Sends a new read requests, if further requests remain.
public static < t > type information < t > of ( class < t > type class ) { try { return type extractor . create type info ( type class ) ; } catch ( invalid types exception e ) { throw new flink runtime exception ( str + str + str + str ) ; } }	Creates a TypeInformation for the type described by the given class.
public static int calculate fix length part size ( internal type type ) { if ( type . equals ( internal types . boolean ) ) { return num ; } else if ( type . equals ( internal types . byte ) ) { return num ; } else if ( type . equals ( internal types . short ) ) { return num ; } else if ( type . equals ( internal types . int ) ) { return num ; } else if ( type . equals ( internal types . float ) ) { return num ; } else if ( type . equals ( internal types . char ) ) { return num ; } else if ( type . equals ( internal types . date ) ) { return num ; } else if ( type . equals ( internal types . time ) ) { return num ; } else {	It store real value when type is primitive.It store the length and offset of variable-length part when type is string, map, etc.
public collection < completable future < task manager location > > get preferred locations based on state ( ) { task manager location prior location ; if ( current execution . get task restore ( ) != null && ( prior location = get latest prior location ( ) ) != null ) { return collections . singleton ( completable future . completed future ( prior location ) ) ; } else { return null ; } }	Gets the preferred location to execute the current task execution attempt, based on the statethat the execution attempt will resume.
void schedule or update consumers ( id partition id ) { final execution execution = current execution ;	Schedules or updates the consumer tasks of the result partition with the given ID.
boolean check input dependency constraints ( ) { if ( get input dependency constraint ( ) == input dependency constraint . any ) {	Check whether the InputDependencyConstraint is satisfied for this vertex.
boolean is input consumable ( int input number ) { return arrays . stream ( input edges [ input number ] ) . map ( execution edge :: get source ) . any match ( intermediate result partition :: is consumable ) ; }	Get whether an input of the vertex is consumable.An input is consumable when when any partition in it is consumable.Note that a BLOCKING result partition is only consumable when all partitions in the result are FINISHED.
void notify state transition ( execution execution , execution state new state , throwable error ) {	Simply forward this notification.
private static void get contained generic types ( composite type < ? > type info , list < generic type info < ? > > target ) { for ( int i = num ; i < type info . get arity ( ) ; i ++ ) { type information < ? > type = type info . get type at ( i ) ; if ( type instanceof composite type ) { get contained generic types ( ( composite type < ? > ) type , target ) ; } else if ( type instanceof generic type info ) { if ( ! target . contains ( type ) ) { target . add ( ( generic type info < ? > ) type ) ; } } } }	Returns all GenericTypeInfos contained in a composite type.
@ override public ref counted file apply ( file file ) throws io { final file directory = temp directories [ next index ( ) ] ; while ( bool ) { try { if ( file == null ) { final file new file = new file ( directory , str + uuid . random uuid ( ) ) ; final output stream out = files . new output stream ( new file . to path ( ) , standard open option . create new ) ; return ref counted file . new file ( new file , out ) ; } else { final output stream out = files . new output stream ( file . to path ( ) , standard open option . append ) ; return ref counted file . restored file ( file , out , file . length ( ) ) ; } } catch ( file already exists exception ignored ) {	Gets the next temp file and stream to temp file.This creates the temp file atomically, making sure no previous file is overwritten.
protected boolean register all requests processed listener ( notification listener listener ) throws io { check not null ( listener ) ; synchronized ( listener lock ) { if ( all requests processed listener == null ) {	Registers a listener to be notified when all outstanding requests have been processed.
@ override public data set < vertex < k , vv > > create result ( ) { if ( this . initial vertices == null ) { throw new illegal state exception ( str ) ; }	Creates the operator that represents this scatter-gather graph computation.
private co group operator < ? , ? , tuple2 < k , message > > build scatter function vertices with degrees ( delta iteration < vertex < k , tuple3 < vv , long value , long value > > , vertex < k , tuple3 < vv , long value , long value > > > iteration , type information < tuple2 < k , message > > message type info , int where arg , int equal to arg , data set < long value > number of vertices ) {	Method that builds the scatter function using a coGroup operator for a vertexcontaining degree information.It afterwards configures the function with a custom name and broadcast variables.
private data set < vertex < k , vv > > create result simple vertex ( edge direction messaging direction , type information < tuple2 < k , message > > message type info , data set < long value > number of vertices ) { data set < tuple2 < k , message > > messages ; type information < vertex < k , vv > > vertex types = initial vertices . get type ( ) ; final delta iteration < vertex < k , vv > , vertex < k , vv > > iteration = initial vertices . iterate delta ( initial vertices , this . maximum number of iterations , num ) ; set up iteration ( iteration ) ; switch ( messaging direction ) { case in : messages = build scatter function ( iteration , message type info , num , num , number of vertices ) ; break ; case out : messages = build scatter function ( iteration , message type info , num , num , number of vertices ) ; break ; case all : messages = build scatter function ( iteration , message type info , num , num , number of vertices ) . union ( build scatter function ( iteration , message type info , num , num , number of vertices ) ) ; break ; default : throw new illegal argument exception ( str ) ; } gather udf < k , vv , message > update udf = new vv < > ( gather function , vertex types ) ;	Creates the operator that represents this scatter-gather graph computation for a simple vertex.
public o set parallelism ( int parallelism ) { preconditions . check argument ( parallelism > num || parallelism == execution config . parallelism default , str ) ; this . parallelism = parallelism ; @ suppress warnings ( str ) o return type = ( o ) this ; return return type ; }	Sets the parallelism for this operator.The parallelism must be 1 or more.
private o set resources ( resource spec min resources , resource spec preferred resources ) { preconditions . check not null ( min resources , str ) ; preconditions . check not null ( preferred resources , str ) ; preconditions . check argument ( min resources . is valid ( ) && preferred resources . is valid ( ) && min resources . less than or equal ( preferred resources ) , str ) ; this . min resources = min resources ; this . preferred resources = preferred resources ; @ suppress warnings ( str ) o return type = ( o ) this ; return return type ; }	Sets the minimum and preferred resources for this operator.
private o set resources ( resource spec resources ) { preconditions . check not null ( resources , str ) ; preconditions . check argument ( resources . is valid ( ) , str ) ; this . min resources = resources ; this . preferred resources = resources ; @ suppress warnings ( str ) o return type = ( o ) this ; return return type ; }	Sets the resources for this operator.
public graph algorithm wrapping base < k , vv , ev , r > set parallelism ( int parallelism ) { preconditions . check argument ( parallelism > num || parallelism == parallelism default , str ) ; this . parallelism = parallelism ; return this ; }	Set the parallelism for this algorithm's operators.
public int get number of available slots for group ( id group id ) { synchronized ( lock ) { map < id , list < shared slot > > available = available slots per jid . get ( group id ) ; if ( available != null ) { set < shared slot > set = new hash set < shared slot > ( ) ; for ( list < shared slot > list : available . values ( ) ) { for ( shared slot slot : list ) { set . add ( slot ) ; } } return set . size ( ) ; } else {	Gets the number of shared slots into which the given group can place subtasks ornested task groups.
void release simple slot ( simple slot simple slot ) { synchronized ( lock ) {	Releases the simple slot from the assignment group.
public int [ ] to array ( ) { int [ ] a = new int [ this . collection . size ( ) ] ; int i = num ; for ( int col : this . collection ) { a [ i ++ ] = col ; } return a ; }	Transforms the field set into an array of field IDs.
public adamic adar < k , vv , ev > set minimum score ( float score ) { preconditions . check argument ( score >= num , str ) ; this . minimum score = score ; return this ; }	Filter out Adamic-Adar scores less than the given minimum.
public adamic adar < k , vv , ev > set minimum ratio ( float ratio ) { preconditions . check argument ( ratio >= num , str ) ; this . minimum ratio = ratio ; return this ; }	Filter out Adamic-Adar scores less than the given ratio times the average score.
public schema schema ( table schema schema ) { table schema . clear ( ) ; last field = null ; for ( int i = num ; i < schema . get field count ( ) ; i ++ ) { field ( schema . get field name ( i ) . get ( ) , schema . get field type ( i ) . get ( ) ) ; } return this ; }	Sets the schema with field names and the types.
public schema field ( string field name , type information < ? > field type ) { field ( field name , type string utils . write type info ( field type ) ) ; return this ; }	Adds a field with the field name and the type information.
public schema field ( string field name , string field type ) { if ( table schema . contains key ( field name ) ) { throw new validation exception ( str ) ; } linked hash map < string , string > field properties = new linked hash map < > ( ) ; field properties . put ( schema type , field type ) ; table schema . put ( field name , field properties ) ; last field = field name ; return this ; }	Adds a field with the field name and the type string.
public schema from ( string origin field name ) { if ( last field == null ) { throw new validation exception ( str ) ; } table schema . get ( last field ) . put ( schema from , origin field name ) ; last field = null ; return this ; }	Specifies the origin of the previously defined field.
public schema proctime ( ) { if ( last field == null ) { throw new validation exception ( str ) ; } table schema . get ( last field ) . put ( schema proctime , str ) ; last field = null ; return this ; }	Specifies the previously defined field as a processing-time attribute.
public schema rowtime ( rowtime rowtime ) { if ( last field == null ) { throw new validation exception ( str ) ; } table schema . get ( last field ) . put all ( rowtime . to properties ( ) ) ; last field = null ; return this ; }	Specifies the previously defined field as an event-time attribute.
public void on complete ( final consumer < stream element queue entry < t > > complete function , executor executor ) { final stream element queue entry < t > this reference = this ; get future ( ) . when complete async (	Register the given complete function to be called once this queue entry has been completed.
public static string format system properties ( configuration jvm args ) { string builder sb = new string builder ( ) ; for ( map . entry < string , string > entry : jvm args . to map ( ) . entry set ( ) ) { if ( sb . length ( ) > num ) { sb . append ( str ) ; } boolean quoted = entry . get value ( ) . contains ( str ) ; if ( quoted ) { sb . append ( str ) ; } sb . append ( str ) . append ( entry . get key ( ) ) . append ( str ) . append ( entry . get value ( ) ) ; if ( quoted ) { sb . append ( str ) ; } } return sb . to string ( ) ; }	Format the system properties as a shell-compatible command-line argument.
public void register timeout ( final k key , final long delay , final time unit unit ) { preconditions . check state ( timeout listener != null , str + get class ( ) . get simple name ( ) + str ) ; if ( timeouts . contains key ( key ) ) { unregister timeout ( key ) ; } timeouts . put ( key , new timeout < > ( timeout listener , key , delay , unit , scheduled executor service ) ) ; }	Register a timeout for the given key which shall occur in the given delay.
public void unregister timeout ( k key ) { timeout < k > timeout = timeouts . remove ( key ) ; if ( timeout != null ) { timeout . cancel ( ) ; } }	Unregister the timeout for the given key.
protected void unregister all timeouts ( ) { for ( timeout < k > timeout : timeouts . values ( ) ) { timeout . cancel ( ) ; } timeouts . clear ( ) ; }	Unregister all timeouts.
public static memory segment allocate unpooled off heap memory ( int size , object owner ) { byte buffer memory = byte buffer . allocate direct ( size ) ; return wrap pooled off heap memory ( memory , owner ) ; }	Allocates some unpooled off-heap memory and creates a new memory segment thatrepresents that memory.
@ override public void write block ( buffer buffer ) throws io { try {	Writes the given block asynchronously.
public void set num fields ( final int num fields ) { final int old num fields = this . num fields ;	Sets the number of fields in the record.
public void make space ( int num fields ) { final int old num fields = this . num fields ;	Reserves space for at least the given number of fields in the internal arrays.
public boolean get field into ( int field num , value target ) {	Gets the field at the given position.
public boolean get fields into ( int [ ] positions , value [ ] targets ) { for ( int i = num ; i < positions . length ; i ++ ) { if ( ! get field into ( positions [ i ] , targets [ i ] ) ) { return bool ; } } return bool ; }	Gets the fields at the given positions into an array.If at any position a field is null, then this method returns false.All fields that have been successfully read until the failing read are correctly contained in the record.All other fields are not set.
public void update binary represenation ( ) {	Updates the binary representation of the data, such that it reflects the state of the currentlystored fields.
public static < k , vv , ev , message > vertex centric iteration < k , vv , ev , message > with edges ( data set < edge < k , ev > > edges with value , compute function < k , vv , ev , message > cf , int maximum number of iterations ) { return new vertex centric iteration < > ( cf , edges with value , null , maximum number of iterations ) ; }	Creates a new vertex-centric iteration operator.
private void set up iteration ( delta iteration < ? , ? > iteration ) {	Helper method which sets up an iteration with the given vertex value.
private static list < string > get topics ( list < kafka topic partition state < topic and partition > > partitions ) { hash set < string > unique topics = new hash set < > ( ) ; for ( kafka topic partition state < topic and partition > fp : partitions ) { unique topics . add ( fp . get topic ( ) ) ; } return new array list < > ( unique topics ) ; }	Returns a list of unique topics from for the given partitions.
private static map < node , list < kafka topic partition state < topic and partition > > > find leader for partitions ( list < kafka topic partition state < topic and partition > > partitions to assign , properties kafka properties ) throws exception { if ( partitions to assign . is empty ( ) ) { throw new illegal argument exception ( str ) ; } log . info ( str , partitions to assign ) ;	Find leaders for the partitions.
public static void validate aws configuration ( properties config ) { if ( config . contains key ( aws . aws credentials provider ) ) { string credentials provider type = config . get property ( aws . aws credentials provider ) ;	Validate configuration properties related to Amazon AWS service.
public static string get memory usage stats as string ( mx memory mx ) { memory usage heap = memory mx . get heap memory usage ( ) ; memory usage non heap = memory mx . get non heap memory usage ( ) ; long heap used = heap . get used ( ) > > num ; long heap committed = heap . get committed ( ) > > num ; long heap max = heap . get max ( ) > > num ; long non heap used = non heap . get used ( ) > > num ; long non heap committed = non heap . get committed ( ) > > num ; long non heap max = non heap . get max ( ) > > num ; return string . format ( str + str , heap used , heap committed , heap max , non heap used , non heap committed , non heap max ) ; }	Gets the memory footprint of the JVM in a string representation.
public static string get memory pool stats as string ( list < mx > pool beans ) { string builder bld = new string builder ( str ) ; int count = num ; for ( mx bean : pool beans ) { if ( bean . get type ( ) == memory type . non heap ) { if ( count > num ) { bld . append ( str ) ; } count ++ ; memory usage usage = bean . get usage ( ) ; long used = usage . get used ( ) > > num ; long committed = usage . get committed ( ) > > num ; long max = usage . get max ( ) > > num ; bld . append ( str ) . append ( bean . get name ( ) ) . append ( str ) ; bld . append ( used ) . append ( str ) . append ( committed ) . append ( str ) . append ( max ) ; bld . append ( str ) ; } } return bld . to string ( ) ; }	Gets the memory pool statistics from the JVM.
public static string get garbage collector stats as string ( list < mx > gc mx ) { string builder bld = new string builder ( str ) ; for ( mx bean : gc mx ) { bld . append ( str ) . append ( bean . get name ( ) ) . append ( str ) . append ( bean . get collection time ( ) ) ; bld . append ( str ) . append ( bean . get collection count ( ) ) . append ( str ) ; bld . append ( str ) ; } if ( ! gc mx . is empty ( ) ) { bld . set length ( bld . length ( ) - num ) ; } return bld . to string ( ) ; }	Gets the garbage collection statistics from the JVM.
public < t extends comparable < t > > graph < t , null value , null value > simplify ( graph < t , null value , null value > graph , int parallelism ) throws exception { switch ( value ) { case directed : graph = graph . run ( new org . apache . flink . graph . asm . simple . directed . simplify < t , null value , null value > ( ) . set parallelism ( parallelism ) ) ; break ; case undirected : graph = graph . run ( new org . apache . flink . graph . asm . simple . undirected . simplify < t , null value , null value > ( bool ) . set parallelism ( parallelism ) ) ; break ; case undirected clip and flip : graph = graph . run ( new org . apache . flink . graph . asm . simple . undirected . simplify < t , null value , null value > ( bool ) . set parallelism ( parallelism ) ) ; break ; } return graph ; }	Simplify the given graph based on the configured value.
public iterator < t > sample in coordinator ( iterator < intermediate sample data < t > > input ) { if ( num samples == num ) { return empty iterable ; }	Sample algorithm for the second phase.
@ override public iterator < t > sample ( iterator < t > input ) { return sample in coordinator ( sample in partition ( input ) ) ; }	Combine the first phase and second phase in sequence, implemented for test purpose only.
public static fixed delay restart strategy factory create factory ( configuration configuration ) throws exception { int max attempts = configuration . get integer ( config constants . restart strategy fixed delay attempts , num ) ; string delay string = configuration . get string ( config constants . restart strategy fixed delay delay ) ; long delay ; try { delay = duration . apply ( delay string ) . to millis ( ) ; } catch ( number format exception nfe ) { throw new exception ( str + config constants . restart strategy fixed delay delay + str + delay string + str ) ; } return new fixed delay restart strategy factory ( max attempts , delay ) ; }	Creates a FixedDelayRestartStrategy from the given Configuration.
public static boolean is in sssp ( final edge < long , double > edge to be removed , data set < edge < long , double > > edges in sssp ) throws exception { return edges in sssp . filter ( new filter function < edge < long , double > > ( ) { @ override public boolean filter ( edge < long , double > edge ) throws exception { return edge . equals ( edge to be removed ) ; } } ) . count ( ) > num ; }	Function that verifies whether the edge to be removed is part of the SSSP or not.If it is, the src vertex will be invalidated.
public boolean mark active ( ) { if ( task slot state . allocated == state || task slot state . active == state ) { state = task slot state . active ; return bool ; } else { return bool ; } }	Mark this slot as active.
public boolean mark free ( ) { if ( is empty ( ) ) { state = task slot state . free ; this . job id = null ; this . allocation id = null ; return bool ; } else { return bool ; } }	Mark the slot as free.
public slot offer generate slot offer ( ) { preconditions . check state ( task slot state . active == state || task slot state . allocated == state , str ) ; preconditions . check state ( allocation id != null , str ) ; return new slot offer ( allocation id , index , resource profile ) ; }	Generate the slot offer from this TaskSlot.
@ override public db configure ( configuration config , class loader class loader ) { return new db ( this , config , class loader ) ; }	Creates a copy of this state backend that uses the values defined in the configurationfor fields where that were not yet specified in this state backend.
public string [ ] get db storage paths ( ) { if ( local rocks db directories == null ) { return null ; } else { string [ ] paths = new string [ local rocks db directories . length ] ; for ( int i = num ; i < paths . length ; i ++ ) { paths [ i ] = local rocks db directories [ i ] . to string ( ) ; } return paths ; } }	Gets the configured local DB storage paths, or null, if none were configured.
public void assign exclusive segments ( network buffer pool network buffer pool , int network buffers per channel ) throws io { check state ( this . is credit based , str ) ; check state ( this . network buffer pool == null , str + str ) ; this . network buffer pool = check not null ( network buffer pool ) ; this . network buffers per channel = network buffers per channel ; synchronized ( request lock ) { for ( input channel input channel : input channels . values ( ) ) { if ( input channel instanceof remote input channel ) { ( ( remote input channel ) input channel ) . assign exclusive segments ( network buffer pool . request memory segments ( network buffers per channel ) ) ; } } } }	Assign the exclusive buffers to all remote input channels directly for credit-based mode.
public void retrigger partition request ( id partition id ) throws io , interrupted exception { synchronized ( request lock ) { if ( ! is released ) { final input channel ch = input channels . get ( partition id ) ; check not null ( ch , str + partition id ) ; log . debug ( str , owning task name , ch . partition id , consumed subpartition index ) ; if ( ch . get class ( ) == remote input channel . class ) { final remote input channel rch = ( remote input channel ) ch ; rch . retrigger subpartition request ( consumed subpartition index ) ; } else if ( ch . get class ( ) == local input channel . class ) { final local input channel ich = ( local input channel ) ch ; if ( retrigger local request timer == null ) { retrigger local request timer = new timer ( bool ) ; } ich . retrigger subpartition request ( retrigger local request timer , consumed subpartition index ) ; } else { throw new illegal state exception ( str + ch . get class ( ) ) ; } } } }	Retriggers a partition request.
public static single input gate create ( string owning task name , id job id , input gate deployment descriptor igdd , network environment network environment , task event publisher task event publisher , task actions task actions , input channel metrics metrics , counter num bytes in counter ) { final id consumed result id = check not null ( igdd . get consumed result id ( ) ) ; final result partition type consumed partition type = check not null ( igdd . get consumed partition type ( ) ) ; final int consumed subpartition index = igdd . get consumed subpartition index ( ) ; check argument ( consumed subpartition index >= num ) ; final input channel deployment descriptor [ ] icdd = check not null ( igdd . get input channel deployment descriptors ( ) ) ; final network environment configuration network config = network environment . get configuration ( ) ; final single input gate input gate = new single input gate ( owning task name , job id , consumed result id , consumed partition type , consumed subpartition index , icdd . length , task actions , num bytes in counter , network config . is credit based ( ) ) ;	Creates an input gate and all of its input channels.
public static int get available port ( ) { for ( int i = num ; i < num ; i ++ ) { try ( server socket server socket = new server socket ( num ) ) { int port = server socket . get local port ( ) ; if ( port != num ) { return port ; } } catch ( io ignored ) { } } throw new runtime exception ( str ) ; }	Find a non-occupied port.
public static string unresolved host to normalized string ( string host ) {	Returns an address in a normalized format for Akka.When an IPv6 address is specified, it normalizes the IPv6 address to avoidcomplications with the exact URL match policy of Akka.
public static string ip address to url string ( inet address address ) { if ( address == null ) { throw new null pointer exception ( str ) ; } else if ( address instanceof inet4 address ) { return address . get host address ( ) ; } else if ( address instanceof inet6 address ) { return get i ( ( inet6 address ) address ) ; } else { throw new illegal argument exception ( str + address ) ; } }	Encodes an IP address properly as a URL string.
public static string socket address to url string ( inet socket address address ) { if ( address . is unresolved ( ) ) { throw new illegal argument exception ( str + address . get host string ( ) ) ; } return ip address and port to url string ( address . get address ( ) , address . get port ( ) ) ; }	Encodes an IP address and port to be included in URL.
public static string host and port to url string ( string host , int port ) throws unknown host exception { return ip address and port to url string ( inet address . get by name ( host ) , port ) ; }	Normalizes and encodes a hostname and port to be included in URL.In particular, this method makes sure that IPv6 address literals have the properformatting to be included in URLs.
private static string get i ( byte [ ] address bytes ) {	Creates a compressed URL style representation of an Inet6Address.
public static iterator < integer > get port range from string ( string range definition ) throws number format exception { final string [ ] ranges = range definition . trim ( ) . split ( str ) ; union iterator < integer > iterators = new union iterator < > ( ) ; for ( string raw range : ranges ) { iterator < integer > range iterator ; string range = raw range . trim ( ) ; int dash idx = range . index of ( str ) ; if ( dash idx == - num ) {	Returns an iterator over available ports defined by the range definition.
public static server socket create socket from ports ( iterator < integer > ports iterator , socket factory factory ) { while ( ports iterator . has next ( ) ) { int port = ports iterator . next ( ) ; log . debug ( str , port ) ; try { return factory . create socket ( port ) ; } catch ( io | illegal argument exception e ) { if ( log . is debug enabled ( ) ) { log . debug ( str , e ) ; } else { log . info ( str , port , e . get message ( ) ) ; } } } return null ; }	Tries to allocate a socket from the given sets of ports.
public void set range partitioned ( ordering ordering , data distribution distribution ) { if ( ordering == null ) { throw new null pointer exception ( ) ; } this . partitioning = partitioning property . range partitioned ; this . ordering = ordering ; this . partitioning fields = ordering . get involved indexes ( ) ; this . distribution = distribution ; }	Set the parameters for range partition.
public static < k , vv , ev > graph < k , vv , ev > from collection ( collection < vertex < k , vv > > vertices , collection < edge < k , ev > > edges , execution environment context ) { return from data set ( context . from collection ( vertices ) , context . from collection ( edges ) , context ) ; }	Creates a graph from a Collection of vertices and a Collection of edges.
public static < k , ev > graph < k , null value , ev > from collection ( collection < edge < k , ev > > edges , execution environment context ) { return from data set ( context . from collection ( edges ) , context ) ; }	Creates a graph from a Collection of edges.Vertices are created automatically and their values are set toNullValue.
public static < k , vv , ev > graph < k , vv , ev > from collection ( collection < edge < k , ev > > edges , final map function < k , vv > vertex value initializer , execution environment context ) { return from data set ( context . from collection ( edges ) , vertex value initializer , context ) ; }	Creates a graph from a Collection of edges.Vertices are created automatically and their values are setby applying the provided map function to the vertex IDs.
public static < k , vv , ev > graph < k , vv , ev > from data set ( data set < vertex < k , vv > > vertices , data set < edge < k , ev > > edges , execution environment context ) { return new graph < > ( vertices , edges , context ) ; }	Creates a graph from a DataSet of vertices and a DataSet of edges.
public static < k , ev > graph < k , null value , ev > from data set ( data set < edge < k , ev > > edges , execution environment context ) { data set < vertex < k , null value > > vertices = edges . flat map ( new emit src and target < > ( ) ) . name ( str ) . distinct ( ) . name ( str ) ; return new graph < > ( vertices , edges , context ) ; }	Creates a graph from a DataSet of edges.Vertices are created automatically and their values are set toNullValue.
public static < k , vv , ev > graph < k , vv , ev > from data set ( data set < edge < k , ev > > edges , final map function < k , vv > vertex value initializer , execution environment context ) { type information < k > key type = ( ( tuple type info < ? > ) edges . get type ( ) ) . get type at ( num ) ; type information < vv > value type = type extractor . create type info ( map function . class , vertex value initializer . get class ( ) , num , key type , null ) ; @ suppress warnings ( { str , str } ) type information < vertex < k , vv > > return type = ( type information < vertex < k , vv > > ) new tuple type info ( vertex . class , key type , value type ) ; data set < vertex < k , vv > > vertices = edges . flat map ( new emit src and target as tuple1 < > ( ) ) . name ( str ) . distinct ( ) . name ( str ) . map ( new map function < tuple1 < k > , vertex < k , vv > > ( ) { private vertex < k , vv > output = new vertex < > ( ) ; public vertex < k , vv > map ( tuple1 < k > value ) throws exception { output . f0 = value . f0 ; output . f1 = vertex value initializer . map ( value . f0 ) ; return output ; } } ) . returns ( return type ) . with forwarded fields ( str ) . name ( str ) ; return new graph < > ( vertices , edges , context ) ; }	Creates a graph from a DataSet of edges.Vertices are created automatically and their values are setby applying the provided map function to the vertex IDs.
public static < k , vv , ev > graph < k , vv , ev > from tuple data set ( data set < tuple2 < k , vv > > vertices , data set < tuple3 < k , k , ev > > edges , execution environment context ) { data set < vertex < k , vv > > vertex data set = vertices . map ( new tuple2 to vertex map < > ( ) ) . name ( str ) ; data set < edge < k , ev > > edge data set = edges . map ( new tuple3 to edge map < > ( ) ) . name ( str ) ; return from data set ( vertex data set , edge data set , context ) ; }	Creates a graph from a DataSet of Tuple2 objects for vertices andTuple3 objects for edges.
public static < k > graph < k , null value , null value > from tuple2 data set ( data set < tuple2 < k , k > > edges , execution environment context ) { data set < edge < k , null value > > edge data set = edges . map ( new tuple2 to edge map < > ( ) ) . name ( str ) ; return from data set ( edge data set , context ) ; }	Creates a graph from a DataSet of Tuple2 objects for edges.Each Tuple2 will become one Edge, where the source ID will be the first field of the Tuple2and the target ID will be the second field of the Tuple2.
public static graph csv reader from csv reader ( string vertices path , string edges path , execution environment context ) { return new graph csv reader ( vertices path , edges path , context ) ; }	Creates a Graph from a CSV file of vertices and a CSV file of edges.
public data set < triplet < k , vv , ev > > get triplets ( ) { return this . get vertices ( ) . join ( this . get edges ( ) ) . where ( num ) . equal to ( num ) . with ( new project edge with src value < > ( ) ) . name ( str ) . join ( this . get vertices ( ) ) . where ( num ) . equal to ( num ) . with ( new project edge with vertex values < > ( ) ) . name ( str ) ; }	This method allows access to the graph's edge values along with its source and target vertex values.
public graph < k , vv , ev > filter on vertices ( filter function < vertex < k , vv > > vertex filter ) { data set < vertex < k , vv > > filtered vertices = this . vertices . filter ( vertex filter ) ; data set < edge < k , ev > > remaining edges = this . edges . join ( filtered vertices ) . where ( num ) . equal to ( num ) . with ( new project edge < > ( ) ) . join ( filtered vertices ) . where ( num ) . equal to ( num ) . with ( new project edge < > ( ) ) . name ( str ) ; return new graph < > ( filtered vertices , remaining edges , this . context ) ; }	Apply a filtering function to the graph and return a sub-graph thatsatisfies the predicates only for the vertices.
public graph < k , vv , ev > filter on edges ( filter function < edge < k , ev > > edge filter ) { data set < edge < k , ev > > filtered edges = this . edges . filter ( edge filter ) . name ( str ) ; return new graph < > ( this . vertices , filtered edges , this . context ) ; }	Apply a filtering function to the graph and return a sub-graph thatsatisfies the predicates only for the edges.
public data set < tuple2 < k , long value > > out degrees ( ) { return vertices . co group ( edges ) . where ( num ) . equal to ( num ) . with ( new count neighbors co group < > ( ) ) . name ( str ) ; }	Return the out-degree of all vertices in the graph.
public data set < tuple2 < k , long value > > get degrees ( ) { return out degrees ( ) . union ( in degrees ( ) ) . name ( str ) . group by ( num ) . sum ( num ) . name ( str ) ; }	Return the degree of all vertices in the graph.
public graph < k , vv , ev > get undirected ( ) { data set < edge < k , ev > > undirected edges = edges . flat map ( new regular and reversed edges map < > ( ) ) . name ( str ) ; return new graph < > ( vertices , undirected edges , this . context ) ; }	This operation adds all inverse-direction edges to the graph.
public < t > data set < t > group reduce on edges ( edges function with vertex value < k , vv , ev , t > edges function , edge direction direction ) throws illegal argument exception { switch ( direction ) { case in : return vertices . co group ( edges ) . where ( num ) . equal to ( num ) . with ( new apply co group function < > ( edges function ) ) . name ( str ) ; case out : return vertices . co group ( edges ) . where ( num ) . equal to ( num ) . with ( new apply co group function < > ( edges function ) ) . name ( str ) ; case all : return vertices . co group ( edges . flat map ( new emit one edge per node < > ( ) ) . name ( str ) ) . where ( num ) . equal to ( num ) . with ( new apply co group function on all edges < > ( edges function ) ) . name ( str ) ; default : throw new illegal argument exception ( str ) ; } }	Groups by vertex and computes a GroupReduce transformation over the edge values of each vertex.The edgesFunction applied on the edges has access to both the id and the valueof the grouping vertex.
public graph < k , vv , ev > reverse ( ) throws unsupported operation exception { data set < edge < k , ev > > reversed edges = edges . map ( new reverse edges map < > ( ) ) . name ( str ) ; return new graph < > ( vertices , reversed edges , this . context ) ; }	Reverse the direction of the edges in the graph.
public graph < k , vv , ev > add vertex ( final vertex < k , vv > vertex ) { list < vertex < k , vv > > new vertex = new array list < > ( ) ; new vertex . add ( vertex ) ; return add vertices ( new vertex ) ; }	Adds the input vertex to the graph.
public graph < k , vv , ev > add vertices ( list < vertex < k , vv > > vertices to add ) {	Adds the list of vertices, passed as input, to the graph.If the vertices already exist in the graph, they will not be added once more.
public graph < k , vv , ev > add edge ( vertex < k , vv > source , vertex < k , vv > target , ev edge value ) { graph < k , vv , ev > partial graph = from collection ( arrays . as list ( source , target ) , collections . singleton list ( new edge < > ( source . f0 , target . f0 , edge value ) ) , this . context ) ; return this . union ( partial graph ) ; }	Adds the given edge to the graph.
public graph < k , vv , ev > add edges ( list < edge < k , ev > > new edges ) { data set < edge < k , ev > > new edges data set = this . context . from collection ( new edges ) ; data set < edge < k , ev > > valid new edges = this . get vertices ( ) . join ( new edges data set ) . where ( num ) . equal to ( num ) . with ( new join vertices with edges on src < > ( ) ) . name ( str ) . join ( this . get vertices ( ) ) . where ( num ) . equal to ( num ) . with ( new join with vertices on trg < > ( ) ) . name ( str ) ; return graph . from data set ( this . vertices , this . edges . union ( valid new edges ) , this . context ) ; }	Adds the given list edges to the graph.
public graph < k , vv , ev > remove vertex ( vertex < k , vv > vertex ) { list < vertex < k , vv > > vertex to be removed = new array list < > ( ) ; vertex to be removed . add ( vertex ) ; return remove vertices ( vertex to be removed ) ; }	Removes the given vertex and its edges from the graph.
public graph < k , vv , ev > remove edge ( edge < k , ev > edge ) { data set < edge < k , ev > > new edges = get edges ( ) . filter ( new edge removal edge filter < > ( edge ) ) . name ( str ) ; return new graph < > ( this . vertices , new edges , this . context ) ; }	Removes all edges that match the given edge from the graph.
public graph < k , vv , ev > remove edges ( list < edge < k , ev > > edges to be removed ) { data set < edge < k , ev > > new edges = get edges ( ) . co group ( this . context . from collection ( edges to be removed ) ) . where ( num , num ) . equal to ( num , num ) . with ( new edge removal co group < > ( ) ) . name ( str ) ; return new graph < > ( this . vertices , new edges , context ) ; }	Removes all the edges that match the edges in the given data set from the graph.
public graph < k , vv , ev > union ( graph < k , vv , ev > graph ) { data set < vertex < k , vv > > unioned vertices = graph . get vertices ( ) . union ( this . get vertices ( ) ) . name ( str ) . distinct ( ) . name ( str ) ; data set < edge < k , ev > > unioned edges = graph . get edges ( ) . union ( this . get edges ( ) ) . name ( str ) ; return new graph < > ( unioned vertices , unioned edges , this . context ) ; }	Performs union on the vertices and edges sets of the input graphsremoving duplicate vertices but maintaining duplicate edges.
private data set < edge < k , ev > > get distinct edge intersection ( data set < edge < k , ev > > edges ) { return this . get edges ( ) . join ( edges ) . where ( num , num , num ) . equal to ( num , num , num ) . with ( new join function < edge < k , ev > , edge < k , ev > , edge < k , ev > > ( ) { @ override public edge < k , ev > join ( edge < k , ev > first , edge < k , ev > second ) throws exception { return first ; } } ) . with forwarded fields first ( str ) . name ( str ) . distinct ( ) . name ( str ) ; }	Computes the intersection between the edge set and the given edge set.
private data set < edge < k , ev > > get pairwise edge intersection ( data set < edge < k , ev > > edges ) { return this . get edges ( ) . co group ( edges ) . where ( num , num , num ) . equal to ( num , num , num ) . with ( new matching edge reducer < > ( ) ) . name ( str ) ; }	Computes the intersection between the edge set and the given edge set.
public < m > graph < k , vv , ev > run scatter gather iteration ( scatter function < k , vv , m , ev > scatter function , org . apache . flink . graph . spargel . gather function < k , vv , m > gather function , int maximum number of iterations ) { return this . run scatter gather iteration ( scatter function , gather function , maximum number of iterations , null ) ; }	Runs a ScatterGather iteration on the graph.No configuration options are provided.
public < m > graph < k , vv , ev > run scatter gather iteration ( scatter function < k , vv , m , ev > scatter function , org . apache . flink . graph . spargel . gather function < k , vv , m > gather function , int maximum number of iterations , scatter gather configuration parameters ) { scatter gather iteration < k , vv , m , ev > iteration = scatter gather iteration . with edges ( edges , scatter function , gather function , maximum number of iterations ) ; iteration . configure ( parameters ) ; data set < vertex < k , vv > > new vertices = this . get vertices ( ) . run operation ( iteration ) ; return new graph < > ( new vertices , this . edges , this . context ) ; }	Runs a ScatterGather iteration on the graph with configuration options.
public < m > graph < k , vv , ev > run gather sum apply iteration ( org . apache . flink . graph . gsa . gather function < vv , ev , m > gather function , sum function < vv , ev , m > sum function , apply function < k , vv , m > apply function , int maximum number of iterations ) { return this . run gather sum apply iteration ( gather function , sum function , apply function , maximum number of iterations , null ) ; }	Runs a Gather-Sum-Apply iteration on the graph.No configuration options are provided.
public < m > graph < k , vv , ev > run gather sum apply iteration ( org . apache . flink . graph . gsa . gather function < vv , ev , m > gather function , sum function < vv , ev , m > sum function , apply function < k , vv , m > apply function , int maximum number of iterations , gsa parameters ) { gather sum apply iteration < k , vv , ev , m > iteration = gather sum apply iteration . with edges ( edges , gather function , sum function , apply function , maximum number of iterations ) ; iteration . configure ( parameters ) ; data set < vertex < k , vv > > new vertices = vertices . run operation ( iteration ) ; return new graph < > ( new vertices , this . edges , this . context ) ; }	Runs a Gather-Sum-Apply iteration on the graph with configuration options.
public static long to timestamp ( string date str , string format , time zone tz ) { simple date format formatter = formatter cache . get ( format ) ; formatter . set time zone ( tz ) ; try { return formatter . parse ( date str ) . get time ( ) ; } catch ( parse exception e ) { return null ; } }	Parse date time string to timestamp based on the given time zone and format.Returns null if parsing failed.
public static long to timestamp tz ( string date str , string format , string tz str ) { time zone tz = timezone cache . get ( tz str ) ; return to timestamp ( date str , format , tz ) ; }	Parse date time string to timestamp based on the given time zone string and format.Returns null if parsing failed.
public static int str to date ( string date str , string from format ) {	Returns the epoch days since 1970-01-01.
public static string date format ( long ts , string format , time zone tz ) { simple date format formatter = formatter cache . get ( format ) ; formatter . set time zone ( tz ) ; date date time = new date ( ts ) ; return formatter . format ( date time ) ; }	Format a timestamp as specific.
public static string date format ( string date str , string from format , string to format , time zone tz ) { simple date format from formatter = formatter cache . get ( from format ) ; from formatter . set time zone ( tz ) ; simple date format to formatter = formatter cache . get ( to format ) ; to formatter . set time zone ( tz ) ; try { return to formatter . format ( from formatter . parse ( date str ) ) ; } catch ( parse exception e ) { log . error ( str + date str + str + from format + str + to format + str , e ) ; return null ; } }	Format a string datetime as specific.
public static string convert tz ( string date str , string format , string tz from , string tz to ) { return date format tz ( to timestamp tz ( date str , format , tz from ) , tz to ) ; }	Convert datetime string from a time zone to another time zone.
public static string timestamp to string ( long ts , int precision , time zone tz ) { int p = ( precision <= num && precision >= num ) ? precision : num ; string format = default datetime formats [ p ] ; return date format ( ts , format , tz ) ; }	Convert a timestamp to string.
private static int get millis ( string date str ) { int length = date str . length ( ) ; if ( length == num ) {	Returns the milli second part of the datetime.
public static long timestamp ceil ( time unit range range , long ts , time zone tz ) {	Keep the algorithm consistent with Calcite DateTimeUtils.julianDateFloor, but herewe take time zone into account.
@ override public boolean validate ( graph < k , vv , ev > graph ) throws exception { data set < tuple1 < k > > edge ids = graph . get edges ( ) . flat map ( new map edge ids < > ( ) ) . distinct ( ) ; data set < k > invalid ids = graph . get vertices ( ) . co group ( edge ids ) . where ( num ) . equal to ( num ) . with ( new group invalid ids < > ( ) ) . first ( num ) ; return invalid ids . map ( new k < > ( ) ) . count ( ) == num ; }	Checks that the edge set input contains valid vertex Ids, i.e.
public h < t > get fields ( string ... fields ) throws io {	Specifies the fields which are returned by the InputFormat and their order.
static byte [ ] read binary field from segments ( memory segment [ ] segments , int base offset , int field offset , long variable part offset and len ) { long mark = variable part offset and len & highest first bit ; if ( mark == num ) { final int sub offset = ( int ) ( variable part offset and len > > num ) ; final int len = ( int ) variable part offset and len ; return segments util . copy to bytes ( segments , base offset + sub offset , len ) ; } else { int len = ( int ) ( ( variable part offset and len & highest second to eighth bit ) > > > num ) ; if ( segments util . little endian ) { return segments util . copy to bytes ( segments , field offset , len ) ; } else {	Get binary, if len less than 8, will be include in variablePartOffsetAndLen.
static binary string read binary string field from segments ( memory segment [ ] segments , int base offset , int field offset , long variable part offset and len ) { long mark = variable part offset and len & highest first bit ; if ( mark == num ) { final int sub offset = ( int ) ( variable part offset and len > > num ) ; final int len = ( int ) variable part offset and len ; return new binary string ( segments , base offset + sub offset , len ) ; } else { int len = ( int ) ( ( variable part offset and len & highest second to eighth bit ) > > > num ) ; if ( segments util . little endian ) { return new binary string ( segments , field offset , len ) ; } else {	Get binary string, if len less than 8, will be include in variablePartOffsetAndLen.
public static string get version ( ) { string version = environment information . class . get package ( ) . get implementation version ( ) ; return version != null ? version : unknown ; }	Returns the version of the code as String.
public static string get hadoop user ( ) { try { class < ? > ugi class = class . for name ( str , bool , environment information . class . get class loader ( ) ) ; method current user method = ugi class . get method ( str ) ; method short user name method = ugi class . get method ( str ) ; object ugi = current user method . invoke ( null ) ; return ( string ) short user name method . invoke ( ugi ) ; } catch ( class not found exception e ) { return str ; } catch ( linkage error e ) {	Gets the name of the user that is running the JVM.
public static long get max jvm heap memory ( ) { final long max memory = runtime . get runtime ( ) . max memory ( ) ; if ( max memory != long . max value ) {	The maximum JVM heap size, in bytes.
public json schema ( type information < row > schema type ) { preconditions . check not null ( schema type ) ; this . schema = type string utils . write type info ( schema type ) ; this . json schema = null ; this . derive schema = null ; return this ; }	Sets the schema using type information.
@ override public void open ( ) { synchronized ( state lock ) { if ( ! closed ) { throw new illegal state exception ( str ) ; } closed = bool ; }	Initialize the hash table.
@ override public void close ( ) {	Closes the hash table.
private long get size ( ) { long num segments = num ; num segments += this . available memory . size ( ) ; num segments += this . buckets . length ; for ( in memory partition < t > p : this . partitions ) { num segments += p . get block count ( ) ; num segments += p . num overflow segments ; } num segments += this . compaction memory . get block count ( ) ; return num segments * this . segment size ; }	Size of all memory segments owned by this hash table.
private long get partition size ( ) { long num segments = num ; for ( in memory partition < t > p : this . partitions ) { num segments += p . get block count ( ) ; } return num segments * this . segment size ; }	Size of all memory segments owned by the partitions of this hash table excluding the compaction partition.
private void try delete empty parent z ( ) throws exception {	Tries to delete empty parent znodes. IMPORTANT: This method can be removed once all supported ZooKeeper versionssupport the container {.
@ visible for testing public int num key value state entries ( object namespace ) { int sum = num ; for ( state table < ? , ? , ? > state : registered kv . values ( ) ) { sum += state . size of namespace ( namespace ) ; } return sum ; }	Returns the total number of state entries across all keys for the given namespace.
static void skip serialized states ( data input view in ) throws io { type serializer < string > name serializer = string serializer . instance ; type serializer < state . state type > state type serializer = new enum serializer < > ( state . state type . class ) ; type serializer < state transition action > action serializer = new enum serializer < > ( state transition action . class ) ; final int no of states = in . read int ( ) ; for ( int i = num ; i < no of states ; i ++ ) { name serializer . deserialize ( in ) ; state type serializer . deserialize ( in ) ; } for ( int i = num ; i < no of states ; i ++ ) { string src name = name serializer . deserialize ( in ) ; int no of transitions = in . read int ( ) ; for ( int j = num ; j < no of transitions ; j ++ ) { string src = name serializer . deserialize ( in ) ; preconditions . check state ( src . equals ( src name ) , str + src name + str + src + str ) ; name serializer . deserialize ( in ) ; action serializer . deserialize ( in ) ; try { skip condition ( in ) ; } catch ( class not found exception e ) { e . print stack trace ( ) ; } } } }	Skips bytes corresponding to serialized states.
public static boolean is in fixed length part ( internal type type ) { if ( type instanceof decimal type ) { return ( ( decimal type ) type ) . precision ( ) <= decimal type . max compact precision ; } else { return mutable field types . contains ( type ) ; } }	If it is a fixed-length field, we can call this BinaryRow's setXX method for in-place updates.If it is variable-length field, can't use this method, because the underlying data is stored continuously.
public boolean any null ( ) {	The bit is 1 when the field is null.
private static < r extends jar request body , m extends message parameters > list < string > get program args ( handler request < r , m > request , logger log ) throws rest handler exception { jar request body request body = request . get request body ( ) ; @ suppress warnings ( str ) list < string > program args = tokenize arguments ( from request body or query parameter ( empty to null ( request body . get program arguments ( ) ) , ( ) -> get query parameter ( request , program args query parameter . class ) , null , log ) ) ; list < string > program args list = from request body or query parameter ( request body . get program arguments list ( ) , ( ) -> request . get query parameter ( program arg query parameter . class ) , null , log ) ; if ( ! program args list . is empty ( ) ) { if ( ! program args . is empty ( ) ) { throw new rest handler exception ( str , http response status . bad request ) ; } return program args list ; } else { return program args ; } }	Parse program arguments in jar run or plan request.
@ visible for testing static list < string > tokenize arguments ( @ nullable final string args ) { if ( args == null ) { return collections . empty list ( ) ; } final matcher matcher = arguments tokenize pattern . matcher ( args ) ; final list < string > tokens = new array list < > ( ) ; while ( matcher . find ( ) ) { tokens . add ( matcher . group ( ) . trim ( ) . replace ( str , str ) . replace ( str , str ) ) ; } return tokens ; }	Takes program arguments as a single string, and splits them into a list of string.
public map < string , optional failure < accumulator < ? , ? > > > aggregate user accumulators ( ) { map < string , optional failure < accumulator < ? , ? > > > user accumulators = new hash map < > ( ) ; for ( execution vertex vertex : get all execution vertices ( ) ) { map < string , accumulator < ? , ? > > next = vertex . get current execution attempt ( ) . get user accumulators ( ) ; if ( next != null ) { accumulator helper . merge into ( user accumulators , next ) ; } } return user accumulators ; }	Merges all accumulator results from the tasks previously executed in the Executions.
@ override public map < string , serialized value < optional failure < object > > > get accumulators serialized ( ) { return aggregate user accumulators ( ) . entry set ( ) . stream ( ) . collect ( collectors . to map ( map . entry :: get key , entry -> serialize accumulator ( entry . get key ( ) , entry . get value ( ) ) ) ) ; }	Gets a serialized accumulator map.
@ override public stringified accumulator result [ ] get accumulator results stringified ( ) { map < string , optional failure < accumulator < ? , ? > > > accumulator map = aggregate user accumulators ( ) ; return stringified accumulator result . stringify accumulator results ( accumulator map ) ; }	Returns the a stringified version of the user-defined accumulators.
public void suspend ( throwable suspension cause ) { assert running in job master main thread ( ) ; if ( state . is terminal state ( ) ) {	Suspends the current ExecutionGraph. The JobStatus will be directly set to {.
public void fail global ( throwable t ) { assert running in job master main thread ( ) ; while ( bool ) { job status current = state ;	Fails the execution graph globally.
public boolean update state ( task execution state state ) { assert running in job master main thread ( ) ; final execution attempt = current executions . get ( state . get id ( ) ) ; if ( attempt != null ) { try { map < string , accumulator < ? , ? > > accumulators ; switch ( state . get execution state ( ) ) { case running : return attempt . switch to running ( ) ; case finished :	Updates the state of one of the ExecutionVertex's Execution attempts.If the new status if "FINISHED", this also updates the accumulators.
private map < string , accumulator < ? , ? > > deserialize accumulators ( task execution state state ) { accumulator snapshot serialized accumulators = state . get accumulators ( ) ; if ( serialized accumulators != null ) { try { return serialized accumulators . deserialize user accumulators ( user class loader ) ; } catch ( throwable t ) {	Deserializes accumulators from a task state update. This method never throws an exception!.
public void schedule or update consumers ( id partition id ) throws execution graph exception { assert running in job master main thread ( ) ; final execution execution = current executions . get ( partition id . get producer id ( ) ) ; if ( execution == null ) { throw new execution graph exception ( str + partition id . get partition id ( ) + str ) ; } else if ( execution . get vertex ( ) == null ) { throw new execution graph exception ( str + partition id . get partition id ( ) + str ) ; } else { execution . get vertex ( ) . schedule or update consumers ( partition id ) ; } }	Schedule or updates consumers of the given result partition.
public void update accumulators ( accumulator snapshot accumulator snapshot ) { map < string , accumulator < ? , ? > > user accumulators ; try { user accumulators = accumulator snapshot . deserialize user accumulators ( user class loader ) ; id exec id = accumulator snapshot . get execution attempt id ( ) ; execution execution = current executions . get ( exec id ) ; if ( execution != null ) { execution . set accumulators ( user accumulators ) ; } else { log . debug ( str , exec id ) ; } } catch ( exception e ) { log . error ( str , get job id ( ) , e ) ; } }	Updates the accumulators during the runtime of a job.
private set < id > compute all prior allocation ids ( ) { hash set < id > all previous allocation ids = new hash set < > ( get number of execution job vertices ( ) ) ; for ( execution vertex execution vertex : get all execution vertices ( ) ) { id latest prior allocation = execution vertex . get latest prior allocation ( ) ; if ( latest prior allocation != null ) { all previous allocation ids . add ( latest prior allocation ) ; } } return all previous allocation ids ; }	Computes and returns a set with the prior allocation ids from all execution vertices in the graph.
public static void clean ( object func , boolean check serializable ) { if ( func == null ) { return ; } final class < ? > cls = func . get class ( ) ;	Tries to clean the closure of the given object, if the object is a non-static innerclass.
public static < t > void apply to all while suppressing exceptions ( iterable < t > inputs , throwing consumer < t , ? extends exception > throwing consumer ) throws exception { if ( inputs != null && throwing consumer != null ) { exception exception = null ; for ( t input : inputs ) { if ( input != null ) { try { throwing consumer . accept ( input ) ; } catch ( exception ex ) { exception = exception utils . first or suppressed ( ex , exception ) ; } } } if ( exception != null ) { throw exception ; } } }	This method supplies all elements from the input to the consumer.
private executor service create query executor ( ) { thread factory thread factory = new thread factory builder ( ) . set daemon ( bool ) . set name format ( str + get server name ( ) + str ) . build ( ) ; return executors . new fixed thread pool ( num query threads , thread factory ) ; }	Creates a thread pool for the query execution.
private boolean attempt to bind ( final int port ) throws throwable { log . debug ( str , server name , port ) ; this . query executor = create query executor ( ) ; this . handler = initialize handler ( ) ; final netty buffer pool buffer pool = new netty buffer pool ( num event loop threads ) ; final thread factory thread factory = new thread factory builder ( ) . set daemon ( bool ) . set name format ( str + server name + str ) . build ( ) ; final nio event loop group nio group = new nio event loop group ( num event loop threads , thread factory ) ; this . bootstrap = new server bootstrap ( ) . local address ( bind address , port ) . group ( nio group ) . channel ( nio server socket channel . class ) . option ( channel option . allocator , buffer pool ) . child option ( channel option . allocator , buffer pool ) . child handler ( new server channel initializer < > ( handler ) ) ; final int default high water mark = num * num ;	Tries to start the server at the provided port. This, in conjunction with {.
public static metric query service create metric query service ( rpc service rpc service , id resource id , long maximum frame size ) { string endpoint id = resource id == null ? metric query service name : metric query service name + str + resource id . get resource id string ( ) ; return new metric query service ( rpc service , endpoint id , maximum frame size ) ; }	Starts the MetricQueryService actor in the given actor system.
public int size ( ) { int ret = any method router . size ( ) ; for ( methodless router < t > router : routers . values ( ) ) { ret += router . size ( ) ; } return ret ; }	Returns the number of routes in this router.
public router < t > add route ( http method method , string path pattern , t target ) { get methodless router ( method ) . add route ( path pattern , target ) ; return this ; }	Add route. A path pattern can only point to one target. This method does nothing if the patternhas already been added.
public set < http method > allowed methods ( string uri ) { query string decoder decoder = new query string decoder ( uri ) ; string [ ] tokens = path pattern . remove slashes at both ends ( decoder . path ( ) ) . split ( str ) ; if ( any method router . any matched ( tokens ) ) { return all allowed methods ( ) ; } set < http method > ret = new hash set < http method > ( routers . size ( ) ) ; for ( map . entry < http method , methodless router < t > > entry : routers . entry set ( ) ) { methodless router < t > router = entry . get value ( ) ; if ( router . any matched ( tokens ) ) { http method method = entry . get key ( ) ; ret . add ( method ) ; } } return ret ; }	Returns allowed methods for a specific URI. For {.
@ public evolving public queryable state stream < key , t > as queryable state ( string queryable state name ) { value state descriptor < t > value state descriptor = new value state descriptor < t > ( uuid . random uuid ( ) . to string ( ) , get type ( ) ) ; return as queryable state ( queryable state name , value state descriptor ) ; }	Publishes the keyed stream as queryable ValueState instance.
@ public evolving public queryable state stream < key , t > as queryable state ( string queryable state name , value state descriptor < t > state descriptor ) { transform ( str + queryable state name , get type ( ) , new queryable value state operator < > ( queryable state name , state descriptor ) ) ; state descriptor . initialize serializer unless set ( get execution config ( ) ) ; return new queryable state stream < > ( queryable state name , state descriptor , get key type ( ) . create serializer ( get execution config ( ) ) ) ; }	Publishes the keyed stream as a queryable ValueState instance.
@ public evolving @ deprecated public < acc > queryable state stream < key , acc > as queryable state ( string queryable state name , folding state descriptor < t , acc > state descriptor ) { transform ( str + queryable state name , get type ( ) , new queryable appending state operator < > ( queryable state name , state descriptor ) ) ; state descriptor . initialize serializer unless set ( get execution config ( ) ) ; return new queryable state stream < > ( queryable state name , state descriptor , get key type ( ) . create serializer ( get execution config ( ) ) ) ; }	Publishes the keyed stream as a queryable FoldingState instance.
@ public evolving public queryable state stream < key , t > as queryable state ( string queryable state name , reducing state descriptor < t > state descriptor ) { transform ( str + queryable state name , get type ( ) , new queryable appending state operator < > ( queryable state name , state descriptor ) ) ; state descriptor . initialize serializer unless set ( get execution config ( ) ) ; return new queryable state stream < > ( queryable state name , state descriptor , get key type ( ) . create serializer ( get execution config ( ) ) ) ; }	Publishes the keyed stream as a queryable ReducingState instance.
public static environment parse ( url url ) throws io { try { return new config util . lower case yaml mapper ( ) . read value ( url , environment . class ) ; } catch ( json mapping exception e ) { throw new sql client exception ( str + e . get message ( ) ) ; } }	Parses an environment file from an URL.
public static environment parse ( string content ) throws io { try { return new config util . lower case yaml mapper ( ) . read value ( content , environment . class ) ; } catch ( json mapping exception e ) { throw new sql client exception ( str + e . get message ( ) ) ; } }	Parses an environment file from an String.
public static environment merge ( environment env1 , environment env2 ) { final environment merged env = new environment ( ) ;	Merges two environments. The properties of the first environment might be overwritten by the second one.
void add ( long value ) { if ( value >= num ) { if ( count > num ) { min = math . min ( min , value ) ; max = math . max ( max , value ) ; } else { min = value ; max = value ; } count ++ ; sum += value ; } }	Adds the value to the stats if it is >= 0.
public void clear ( ) { final int array offset = get head element index ( ) ; arrays . fill ( queue , array offset , array offset + size , null ) ; size = num ; }	Clears the queue.
private void set map for key group ( int key group id , map < n , map < k , s > > map ) { try { state [ index to offset ( key group id ) ] = map ; } catch ( array index out of bounds exception e ) { throw new illegal argument exception ( str + key group id + str + str + key group offset + str + ( key group offset + state . length ) + str ) ; } }	Sets the given map for the given key-group.
public void random emit ( t record ) throws io , interrupted exception { emit ( record , rng . next int ( number of channels ) ) ; }	This is used to send LatencyMarks to a random target channel.
private void notify flusher exception ( throwable t ) { if ( flusher exception == null ) { log . error ( str , t ) ; flusher exception = t ; } }	Notifies the writer that the output flusher thread encountered an exception.
private void handle rpc invocation ( rpc invocation rpc invocation ) { method rpc method = null ; try { string method name = rpc invocation . get method name ( ) ; class < ? > [ ] parameter types = rpc invocation . get parameter types ( ) ; rpc method = lookup rpc method ( method name , parameter types ) ; } catch ( class not found exception e ) { log . error ( str , e ) ; rpc connection exception rpc exception = new rpc connection exception ( str , e ) ; get sender ( ) . tell ( new status . failure ( rpc exception ) , get self ( ) ) ; } catch ( io e ) { log . error ( str , e ) ; rpc connection exception rpc exception = new rpc connection exception ( str , e ) ; get sender ( ) . tell ( new status . failure ( rpc exception ) , get self ( ) ) ; } catch ( final no such method exception e ) { log . error ( str , e ) ; rpc connection exception rpc exception = new rpc connection exception ( str , e ) ; get sender ( ) . tell ( new status . failure ( rpc exception ) , get self ( ) ) ; } if ( rpc method != null ) { try {	Handle rpc invocations by looking up the rpc method on the rpc endpoint and calling thismethod with the provided method arguments.
protected void send error if sender ( throwable throwable ) { if ( ! get sender ( ) . equals ( actor ref . no sender ( ) ) ) { get sender ( ) . tell ( new status . failure ( throwable ) , get self ( ) ) ; } }	Send throwable to sender if the sender is specified.
private void stop ( rpc endpoint termination result rpc endpoint termination result ) { if ( rpc endpoint stopped . compare and set ( bool , bool ) ) { this . rpc endpoint termination result = rpc endpoint termination result ; get context ( ) . stop ( get self ( ) ) ; } }	Stop the actor immediately.
private static void print custom cli options ( collection < custom command line < ? > > custom command lines , help formatter formatter , boolean run options ) {	Prints custom cli options.
public static void delete file or directory ( file file ) throws io { check not null ( file , str ) ; guard if windows ( file utils :: delete file or directory internal , file ) ; }	Removes the given file or directory recursively.
public static void delete directory ( file directory ) throws io { check not null ( directory , str ) ; guard if windows ( file utils :: delete directory internal , directory ) ; }	Deletes the given directory recursively.
public static void clean directory ( file directory ) throws io { check not null ( directory , str ) ; guard if windows ( file utils :: clean directory internal , directory ) ; }	Removes all files contained within a directory, without removing the directory itself.
public static void copy ( path source path , path target path , boolean executable ) throws io {	Copies all files from source to target and sets executable flag.
public void release payload ( throwable cause ) { final payload payload = payload reference . get ( ) ; if ( payload != null ) { payload . release ( cause ) ; payload reference . set ( null ) ; } }	Triggers the release of the assigned payload.
public static protos . environment . variable variable ( string name , string value ) { check not null ( name ) ; return protos . environment . variable . new builder ( ) . set name ( name ) . set value ( value ) . build ( ) ; }	Construct a Mesos environment variable.
public static list < protos . resource > resources ( protos . resource ... resources ) { check not null ( resources ) ; return arrays . as list ( resources ) ; }	Construct a list of resources.
public static protos . resource scalar ( string name , string role , double value ) { check not null ( name ) ; check not null ( role ) ; check not null ( value ) ; return protos . resource . new builder ( ) . set name ( name ) . set type ( protos . value . type . scalar ) . set scalar ( protos . value . scalar . new builder ( ) . set value ( value ) ) . set role ( role ) . build ( ) ; }	Construct a scalar resource.
public static protos . value . range range ( long begin , long end ) { return protos . value . range . new builder ( ) . set begin ( begin ) . set end ( end ) . build ( ) ; }	Construct a range value.
public static protos . resource ranges ( string name , string role , protos . value . range ... ranges ) { check not null ( name ) ; check not null ( role ) ; check not null ( ranges ) ; return protos . resource . new builder ( ) . set name ( name ) . set type ( protos . value . type . ranges ) . set ranges ( protos . value . ranges . new builder ( ) . add all range ( arrays . as list ( ranges ) ) . build ( ) ) . set role ( role ) . build ( ) ; }	Construct a range resource.
public static long stream range values ( collection < protos . resource > resources ) { check not null ( resources ) ; return resources . stream ( ) . filter ( protos . resource :: has ranges ) . flat map ( r -> r . get ranges ( ) . get range list ( ) . stream ( ) ) . flat map to long ( utils :: range values ) ; }	Gets a stream of values from a collection of range resources.
public static long stream range values ( protos . value . range range ) { check not null ( range ) ; return long stream . range closed ( range . get begin ( ) , range . get end ( ) ) ; }	Gets a stream of values from a range.
public optional < type information < ? > > get field type ( int field index ) { if ( field index < num || field index >= field types . length ) { return optional . empty ( ) ; } return optional . of ( field types [ field index ] ) ; }	Returns the specified type information for the given field index.
public optional < type information < ? > > get field type ( string field name ) { if ( field name to index . contains key ( field name ) ) { return optional . of ( field types [ field name to index . get ( field name ) ] ) ; } return optional . empty ( ) ; }	Returns the specified type information for the given field name.
public optional < string > get field name ( int field index ) { if ( field index < num || field index >= field names . length ) { return optional . empty ( ) ; } return optional . of ( field names [ field index ] ) ; }	Returns the specified name for the given field index.
public void deploy ( ) throws job exception { assert running in job master main thread ( ) ; final logical slot slot = assigned resource ; check not null ( slot , str ) ;	Deploys the execution to the previously assigned resource.
public completable future < stack trace sample response > request stack trace sample ( int sample id , int num samples , time delay between samples , int max stack trace depth , time timeout ) { final logical slot slot = assigned resource ; if ( slot != null ) { final task manager gateway task manager gateway = slot . get task manager gateway ( ) ; return task manager gateway . request stack trace sample ( attempt id , sample id , num samples , delay between samples , max stack trace depth , timeout ) ; } else { return future utils . completed exceptionally ( new exception ( str ) ) ; } }	Request a stack trace sample from the task of this execution.
public void notify checkpoint complete ( long checkpoint id , long timestamp ) { final logical slot slot = assigned resource ; if ( slot != null ) { final task manager gateway task manager gateway = slot . get task manager gateway ( ) ; task manager gateway . notify checkpoint complete ( attempt id , get vertex ( ) . get job id ( ) , checkpoint id , timestamp ) ; } else { log . debug ( str + str ) ; } }	Notify the task of this execution about a completed checkpoint.
private void send cancel rpc call ( int number retries ) { final logical slot slot = assigned resource ; if ( slot != null ) { final task manager gateway task manager gateway = slot . get task manager gateway ( ) ; final component main thread executor job master main thread executor = get vertex ( ) . get execution graph ( ) . get job master main thread executor ( ) ; completable future < acknowledge > cancel result future = future utils . retry ( ( ) -> task manager gateway . cancel task ( attempt id , rpc timeout ) , number retries , job master main thread executor ) ; cancel result future . when complete ( ( ack , failure ) -> { if ( failure != null ) { fail ( new exception ( str , failure ) ) ; } } ) ; } }	This method sends a CancelTask message to the instance of the assigned slot.
private void send update partition info rpc call ( final iterable < partition info > partition infos ) { final logical slot slot = assigned resource ; if ( slot != null ) { final task manager gateway task manager gateway = slot . get task manager gateway ( ) ; final task manager location task manager location = slot . get task manager location ( ) ; completable future < acknowledge > update partitions result future = task manager gateway . update partitions ( attempt id , partition infos , rpc timeout ) ; update partitions result future . when complete async ( ( ack , failure ) -> {	Update the partition infos on the assigned resource.
@ visible for testing public completable future < collection < task manager location > > calculate preferred locations ( location preference constraint location preference constraint ) { final collection < completable future < task manager location > > preferred location futures = get vertex ( ) . get preferred locations ( ) ; final completable future < collection < task manager location > > preferred locations future ; switch ( location preference constraint ) { case all : preferred locations future = future utils . combine all ( preferred location futures ) ; break ; case any : final array list < task manager location > completed task manager locations = new array list < > ( preferred location futures . size ( ) ) ; for ( completable future < task manager location > preferred location future : preferred location futures ) { if ( preferred location future . is done ( ) && ! preferred location future . is completed exceptionally ( ) ) { final task manager location task manager location = preferred location future . get now ( null ) ; if ( task manager location == null ) { throw new flink runtime exception ( str ) ; } completed task manager locations . add ( task manager location ) ; } } preferred locations future = completable future . completed future ( completed task manager locations ) ; break ; default : throw new runtime exception ( str + location preference constraint + str ) ; } return preferred locations future ; }	Calculates the preferred locations based on the location preference constraint.
public static < t > void write version and serialize ( simple versioned serializer < t > serializer , t datum , data output view out ) throws io { check not null ( serializer , str ) ; check not null ( datum , str ) ; check not null ( out , str ) ; final byte [ ] data = serializer . serialize ( datum ) ; out . write int ( serializer . get version ( ) ) ; out . write int ( data . length ) ; out . write ( data ) ; }	Serializes the version and datum into a stream. Data serialized via this method can be deserialized via{.
public static < t > t read version and de serialize ( simple versioned serializer < t > serializer , data input view in ) throws io { check not null ( serializer , str ) ; check not null ( in , str ) ; final int version = in . read int ( ) ; final int length = in . read int ( ) ; final byte [ ] data = new byte [ length ] ; in . read fully ( data ) ; return serializer . deserialize ( version , data ) ; }	Deserializes the version and datum from a stream. This method deserializes data serialized via{.
public boolean cleanup ( ) throws io { return ! state . compare and set ( state . ongoing , state . deleted ) || file system . delete ( directory , bool ) ; }	Calling this method will attempt delete the underlying snapshot directory recursively, if the state is"ongoing".
private boolean should roll ( ) throws io { boolean should roll = bool ; int subtask index = get runtime context ( ) . get index of this subtask ( ) ; if ( ! is writer open ) { should roll = bool ; log . debug ( str , subtask index ) ; } if ( bucketer . should start new bucket ( new path ( base path ) , current bucket directory ) ) { should roll = bool ; log . debug ( str , subtask index , bucketer ) ;	Determines whether we should change the bucket file we are writing to. This will roll if no file was created yet, if the file size is larger than the specified sizeor if the {.
private void open new part file ( ) throws exception { close current part file ( ) ; path new bucket directory = bucketer . get next bucket path ( new path ( base path ) ) ; if ( ! new bucket directory . equals ( current bucket directory ) ) { current bucket directory = new bucket directory ; try { if ( fs . mkdirs ( current bucket directory ) ) { log . debug ( str , current bucket directory ) ; } } catch ( io e ) { throw new runtime exception ( str , e ) ; } } int subtask index = get runtime context ( ) . get index of this subtask ( ) ; current part path = new path ( current bucket directory , part prefix + str + subtask index + str + part counter ) ;	Opens a new part file. This closes the old bucket file and retrieves a new bucket path from the {.
private void close current part file ( ) throws exception { if ( is writer open ) { writer . close ( ) ; is writer open = bool ; } if ( current part path != null ) { path in progress path = get in progress path for ( current part path ) ; path pending path = get pending path for ( current part path ) ; fs . rename ( in progress path , pending path ) ; log . debug ( str , in progress path , pending path ) ; this . bucket state . pending files . add ( current part path . to string ( ) ) ; } }	Closes the current part file.
public grid graph add dimension ( long size , boolean wrap endpoints ) { preconditions . check argument ( size >= num , str ) ; vertex count = math . multiply exact ( vertex count , size ) ;	Required configuration for each dimension of the graph.
public < t > type serializer < t > get restored nested serializer ( int pos ) { check argument ( pos < nested snapshots . length ) ; @ suppress warnings ( str ) type serializer snapshot < t > snapshot = ( type serializer snapshot < t > ) nested snapshots [ pos ] ; return snapshot . restore serializer ( ) ; }	Creates the restore serializer from the pos-th config snapshot.
@ deprecated public < t > type serializer schema compatibility < t > resolve compatibility with nested ( type serializer schema compatibility < ? > outer compatibility , type serializer < ? > ... new nested serializers ) { check argument ( new nested serializers . length == nested snapshots . length , str ) ;	Resolves the compatibility of the nested serializer snapshots with the nestedserializers of the new outer serializer.
public final void write nested serializer snapshots ( data output view out ) throws io { out . write int ( magic number ) ; out . write int ( version ) ; out . write int ( nested snapshots . length ) ; for ( type serializer snapshot < ? > snap : nested snapshots ) { type serializer snapshot . write versioned snapshot ( out , snap ) ; } }	Writes the composite snapshot of all the contained serializers.
public static nested serializers snapshot delegate read nested serializer snapshots ( data input view in , class loader cl ) throws io { final int magic number = in . read int ( ) ; if ( magic number != magic number ) { throw new io ( string . format ( str , magic number , magic number ) ) ; } final int version = in . read int ( ) ; if ( version != version ) { throw new io ( str + version ) ; } final int num snapshots = in . read int ( ) ; final type serializer snapshot < ? > [ ] nested snapshots = new type serializer snapshot < ? > [ num snapshots ] ; for ( int i = num ; i < num snapshots ; i ++ ) { nested snapshots [ i ] = type serializer snapshot . read versioned snapshot ( in , cl ) ; } return new nested serializers snapshot delegate ( nested snapshots ) ; }	Reads the composite snapshot of all the contained serializers.
@ suppress warnings ( str ) private static < e > type serializer schema compatibility < e > resolve compatibility ( type serializer < ? > serializer , type serializer snapshot < ? > snapshot ) { type serializer < e > typed serializer = ( type serializer < e > ) serializer ; type serializer snapshot < e > typed snapshot = ( type serializer snapshot < e > ) snapshot ; return typed snapshot . resolve schema compatibility ( typed serializer ) ; }	Utility method to conjure up a new scope for the generic parameters.
public void set custom endpoint initializer ( endpoint initializer initializer ) { objects . require non null ( initializer , str ) ; closure cleaner . ensure serializable ( initializer ) ; this . initializer = initializer ; }	Set a custom endpoint initializer.
void unregister output stream ( out stream stream ) { lock . lock ( ) ; try {	Atomically removes the given output stream from the set of currently open output streams,and signals that new stream can now be opened.
void unregister input stream ( in stream stream ) { lock . lock ( ) ; try {	Atomically removes the given input stream from the set of currently open input streams,and signals that new stream can now be opened.
public void set max parallelism ( int max parallelism ) { preconditions . check argument ( max parallelism > num && max parallelism <= stream graph generator . upper bound max parallelism , str + stream graph generator . upper bound max parallelism + str + max parallelism ) ; this . max parallelism = max parallelism ; }	Sets the maximum parallelism for this stream transformation.
public static boolean validate class loadable ( class not found exception cnfe , class loader cl ) { try { string class name = cnfe . get message ( ) ; class . for name ( class name , bool , cl ) ; return bool ; } catch ( class not found exception e ) { return bool ; } catch ( exception e ) { return bool ; } }	Checks, whether the class that was not found in the given exception, can be resolved throughthe given class loader.
private mapping extract projects and mapping ( aggregate aggregate , rel node input , rel builder rel builder ) {	Extract projects from the Aggregate and return the index mapping between the new projectsand it's input.
private immutable bit set . builder get input field used ( aggregate aggregate , rel node input ) {	Compute which input fields are used by the aggregate.
public static containered task manager parameters create ( configuration config , long container memory mb , int num slots ) {	Computes the parameters to be used to start a TaskManager Java process.
@ internal public static set < annotation > read single forward annotations ( class < ? > udf class ) { forwarded fields forwarded fields = udf class . get annotation ( forwarded fields . class ) ; non forwarded fields non forwarded fields = udf class . get annotation ( non forwarded fields . class ) ; read fields read set = udf class . get annotation ( read fields . class ) ; set < annotation > annotations = new hash set < annotation > ( ) ; if ( forwarded fields != null ) { annotations . add ( forwarded fields ) ; } if ( non forwarded fields != null ) { if ( ! annotations . is empty ( ) ) { throw new invalid program exception ( str + forwarded fields . class . get simple name ( ) + str + non forwarded fields . class . get simple name ( ) + str ) ; } annotations . add ( non forwarded fields ) ; } if ( read set != null ) { annotations . add ( read set ) ; } return ! annotations . is empty ( ) ? annotations : null ; }	Reads the annotations of a user defined function with one input and returns semantic properties according to the forwarded fields annotated.
@ internal public static set < annotation > read dual forward annotations ( class < ? > udf class ) {	Reads the annotations of a user defined function with two inputs and returns semantic properties according to the forwarded fields annotated.
public static binary string blank string ( int length ) { byte [ ] spaces = new byte [ length ] ; arrays . fill ( spaces , ( byte ) str ) ; return from bytes ( spaces ) ; }	Creates an BinaryString that contains `length` spaces.
private int compare multi segments ( binary string other ) { if ( size in bytes == num || other . size in bytes == num ) { return size in bytes - other . size in bytes ; } int len = math . min ( size in bytes , other . size in bytes ) ; memory segment seg1 = segments [ num ] ; memory segment seg2 = other . segments [ num ] ; int segment size = segments [ num ] . size ( ) ; int other segment size = other . segments [ num ] . size ( ) ; int size of first1 = segment size - offset ; int size of first2 = other segment size - other . offset ; int var seg index1 = num ; int var seg index2 = num ;	Find the boundaries of segments, and then compare MemorySegment.
public static binary string concat ( iterable < binary string > inputs ) {	Concatenates input strings together into a single string.
public boolean contains ( final binary string substring ) { ensure materialized ( ) ; substring . ensure materialized ( ) ; if ( substring . size in bytes == num ) { return bool ; } int find = segments util . find ( segments , offset , size in bytes , substring . segments , substring . offset , substring . size in bytes ) ; return find != - num ; }	Returns whether this contains `substring` or not.Same to like '%substring%'.
public boolean ends with ( final binary string suffix ) { ensure materialized ( ) ; suffix . ensure materialized ( ) ; return match at ( suffix , size in bytes - suffix . size in bytes ) ; }	Same to like '%suffix'.
public binary string trim ( binary string trim str ) { if ( trim str == null ) { return null ; } return trim left ( trim str ) . trim right ( trim str ) ; }	Walk each character of current string from both ends, remove the character if itis in trim string.Return the new substring which both ends trim characters have been removed.
public binary string trim left ( binary string trim str ) { ensure materialized ( ) ; if ( trim str == null ) { return null ; } trim str . ensure materialized ( ) ; if ( trim str . is space string ( ) ) { return trim left ( ) ; } if ( in first segment ( ) ) { int search idx = num ; while ( search idx < this . size in bytes ) { int char bytes = num bytes for first byte ( get byte one segment ( search idx ) ) ; binary string current char = copy binary string in one seg ( search idx , search idx + char bytes - num ) ;	Walk each character of current string from left end, remove the character if itis in trim string.
public binary string trim right ( binary string trim str ) { ensure materialized ( ) ; if ( trim str == null ) { return null ; } trim str . ensure materialized ( ) ; if ( trim str . is space string ( ) ) { return trim right ( ) ; } if ( in first segment ( ) ) { int char idx = num ; int byte idx = num ;	Walk each character of current string from right end, remove the character if itis in trim string.
public int index of ( binary string sub str , int start ) { ensure materialized ( ) ; sub str . ensure materialized ( ) ; if ( sub str . size in bytes == num ) { return num ; } if ( in first segment ( ) ) {	Returns the position of the first occurence of substr in current string starting from givenposition.
public binary string reverse ( ) { ensure materialized ( ) ; if ( in first segment ( ) ) { byte [ ] result = new byte [ this . size in bytes ] ;	Reverse each character in current string.
public long to long ( ) { ensure materialized ( ) ; if ( size in bytes == num ) { return null ; } int size = segments [ num ] . size ( ) ; segment and offset segment and offset = start segment and offset ( size ) ; int total offset = num ; byte b = segment and offset . value ( ) ; final boolean negative = b == str ; if ( negative || b == str ) { segment and offset . next byte ( size ) ; total offset ++ ; if ( size in bytes == num ) { return null ; } } long result = num ; final byte separator = str ; final int radix = num ; final long stop value = long . min value / radix ; while ( total offset < this . size in bytes ) { b = segment and offset . value ( ) ; total offset ++ ; segment and offset . next byte ( size ) ; if ( b == separator ) {	Parses this BinaryString to Long.
public binary string to upper case ( ) { if ( java object != null ) { return to upper case slow ( ) ; } if ( size in bytes == num ) { return empty ut ; } int size = segments [ num ] . size ( ) ; segment and offset segment and offset = start segment and offset ( size ) ; byte [ ] bytes = new byte [ size in bytes ] ; bytes [ num ] = ( byte ) character . to title case ( segment and offset . value ( ) ) ; for ( int i = num ; i < size in bytes ; i ++ ) { byte b = segment and offset . value ( ) ; if ( num bytes for first byte ( b ) != num ) {	Returns the upper case of this string.
public binary string to lower case ( ) { if ( java object != null ) { return to lower case slow ( ) ; } if ( size in bytes == num ) { return empty ut ; } int size = segments [ num ] . size ( ) ; segment and offset segment and offset = start segment and offset ( size ) ; byte [ ] bytes = new byte [ size in bytes ] ; bytes [ num ] = ( byte ) character . to title case ( segment and offset . value ( ) ) ; for ( int i = num ; i < size in bytes ; i ++ ) { byte b = segment and offset . value ( ) ; if ( num bytes for first byte ( b ) != num ) {	Returns the lower case of this string.
public boolean to boolean sql ( ) { if ( true strings . contains ( to lower case ( ) ) ) { return bool ; } else if ( false strings . contains ( to lower case ( ) ) ) { return bool ; } else { return null ; } }	Decide boolean representation of a string.
public single output stream operator < t > set parallelism ( int parallelism ) { preconditions . check argument ( can be parallel ( ) || parallelism == num , str ) ; transformation . set parallelism ( parallelism ) ; return this ; }	Sets the parallelism for this operator.
@ public evolving public single output stream operator < t > set max parallelism ( int max parallelism ) { preconditions . check argument ( max parallelism > num , str ) ; preconditions . check argument ( can be parallel ( ) || max parallelism == num , str ) ; transformation . set max parallelism ( max parallelism ) ; return this ; }	Sets the maximum parallelism of this operator.
private single output stream operator < t > set resources ( resource spec resources ) { preconditions . check not null ( resources , str ) ; preconditions . check argument ( resources . is valid ( ) , str ) ; transformation . set resources ( resources , resources ) ; return this ; }	Sets the resources for this operator, the minimum and preferred resources are the same by default.
@ public evolving public single output stream operator < t > force non parallel ( ) { transformation . set parallelism ( num ) ; transformation . set max parallelism ( num ) ; non parallel = bool ; return this ; }	Sets the parallelism and maximum parallelism of this operator to one.And mark this operator cannot set a non-1 degree of parallelism.
int finalize build phase ( io io access , io . enumerator probe channel enumerator ) throws io { this . final buffer limit = this . build side write buffer . get current position in segment ( ) ; this . partition buffers = this . build side write buffer . close ( ) ; if ( ! is in memory ( ) ) {	After build phase.
public void report error ( throwable t ) {	Sets the exception and interrupts the target thread,if no other exception has occurred so far.
private void emit window result ( w window ) throws exception { base row agg result = window function . get window aggregation result ( window ) ; if ( send retraction ) { previous state . set current namespace ( window ) ; base row previous agg result = previous state . value ( ) ;	Emits the window result of the given window.
private void register cleanup timer ( w window ) { long cleanup time = cleanup time ( window ) ; if ( cleanup time == long . max value ) {	Registers a timer to cleanup the content of the window.
public data sink < t > set parallelism ( int parallelism ) { preconditions . check argument ( parallelism > num || parallelism == execution config . parallelism default , str ) ; this . parallelism = parallelism ; return this ; }	Sets the parallelism for this data sink.The degree must be 1 or more.
private data sink < t > set resources ( resource spec min resources , resource spec preferred resources ) { preconditions . check not null ( min resources , str ) ; preconditions . check not null ( preferred resources , str ) ; preconditions . check argument ( min resources . is valid ( ) && preferred resources . is valid ( ) && min resources . less than or equal ( preferred resources ) , str ) ; this . min resources = min resources ; this . preferred resources = preferred resources ; return this ; }	Sets the minimum and preferred resources for this data sink.
private data sink < t > set resources ( resource spec resources ) { preconditions . check not null ( resources , str ) ; preconditions . check argument ( resources . is valid ( ) , str ) ; this . min resources = resources ; this . preferred resources = resources ; return this ; }	Sets the resources for this data sink, and the minimum and preferred resources are the same by default.
void restore partition buffers ( io io manager , list < memory segment > available memory ) throws io { final bulk block channel reader reader = io manager . create bulk block channel reader ( this . initial build side channel , available memory , this . initial partition buffers count ) ; reader . close ( ) ; final list < memory segment > partition buffers from disk = reader . get full segments ( ) ; this . partition buffers = ( memory segment [ ] ) partition buffers from disk . to array ( new memory segment [ partition buffers from disk . size ( ) ] ) ; this . overflow segments = new memory segment [ num ] ; this . num overflow segments = num ; this . next overflow bucket = num ; this . is restored = bool ; }	This method is called every time a multi-match hash map is opened again for a new probe input.
public option add ( string name ) throws required parameters exception { if ( ! this . data . contains key ( name ) ) { option option = new option ( name ) ; this . data . put ( name , option ) ; return option ; } else { throw new required parameters exception ( str + name + str ) ; } }	Add a parameter based on its name.
private void check and apply default value ( option o , map < string , string > data ) throws required parameters exception { if ( has no default value and no value passed on alternative name ( o , data ) ) { throw new required parameters exception ( str + o . get name ( ) ) ; } }	else throw an exception.
private boolean has no default value and no value passed on alternative name ( option o , map < string , string > data ) throws required parameters exception { if ( o . has alt ( ) && data . contains key ( o . get alt ( ) ) ) { data . put ( o . get name ( ) , data . get ( o . get alt ( ) ) ) ; } else { if ( o . has default value ( ) ) { data . put ( o . get name ( ) , o . get default value ( ) ) ; if ( o . has alt ( ) ) { data . put ( o . get alt ( ) , o . get default value ( ) ) ; } } else { return bool ; } } return bool ; }	else return true to indicate parameter is 'really' missing.
public string get help ( ) { string builder sb = new string builder ( data . size ( ) * help text length per param ) ; sb . append ( str ) ; sb . append ( help text line delimiter ) ; for ( option o : data . values ( ) ) { sb . append ( this . help text ( o ) ) ; } sb . append ( help text line delimiter ) ; return sb . to string ( ) ; }	Build a help text for the defined parameters. The format of the help text will be:Required Parameters:\t -:shortName:, --:name: \t :helpText: \t default: :defaultValue: \t choices: :choices: \n.
public jaccard index < k , vv , ev > set group size ( int group size ) { preconditions . check argument ( group size > num , str ) ; this . group size = group size ; return this ; }	Override the default group size for the quadratic expansion of neighborpairs.
public jaccard index < k , vv , ev > set minimum score ( int numerator , int denominator ) { preconditions . check argument ( numerator >= num , str ) ; preconditions . check argument ( denominator > num , str ) ; preconditions . check argument ( numerator <= denominator , str ) ; this . unbounded scores = bool ; this . minimum score numerator = numerator ; this . minimum score denominator = denominator ; return this ; }	Filter out Jaccard Index scores less than the given minimum fraction.
public jaccard index < k , vv , ev > set maximum score ( int numerator , int denominator ) { preconditions . check argument ( numerator >= num , str ) ; preconditions . check argument ( denominator > num , str ) ; preconditions . check argument ( numerator <= denominator , str ) ; this . unbounded scores = bool ; this . maximum score numerator = numerator ; this . maximum score denominator = denominator ; return this ; }	Filter out Jaccard Index scores greater than the given maximum fraction.
public data set < st > close with ( data set < st > solution set delta , data set < wt > new workset ) { return new delta iteration result set < st , wt > ( initial solution set . get execution environment ( ) , initial solution set . get type ( ) , initial workset . get type ( ) , this , solution set delta , new workset , keys , max iterations ) ; }	Closes the delta iteration.
public delta iteration < st , wt > parallelism ( int parallelism ) { preconditions . check argument ( parallelism > num || parallelism == execution config . parallelism default , str ) ; this . parallelism = parallelism ; return this ; }	Sets the parallelism for the iteration.
private delta iteration < st , wt > set resources ( resource spec min resources , resource spec preferred resources ) { preconditions . check not null ( min resources , str ) ; preconditions . check not null ( preferred resources , str ) ; preconditions . check argument ( min resources . is valid ( ) && preferred resources . is valid ( ) && min resources . less than or equal ( preferred resources ) , str ) ; this . min resources = min resources ; this . preferred resources = preferred resources ; return this ; }	Sets the minimum and preferred resources for the iteration.
private delta iteration < st , wt > set resources ( resource spec resources ) { preconditions . check not null ( resources , str ) ; preconditions . check argument ( resources . is valid ( ) , str ) ; this . min resources = resources ; this . preferred resources = resources ; return this ; }	Sets the resources for the iteration, and the minimum and preferred resources are the same by default.The lower and upper resource limits will be considered in dynamic resource resize feature for future plan.
private static void request and set specific time offsets from kafka ( simple consumer consumer , list < kafka topic partition state < topic and partition > > partitions , long which time ) throws io { map < topic and partition , partition offset request info > request info = new hash map < > ( ) ; for ( kafka topic partition state < topic and partition > part : partitions ) { request info . put ( part . get kafka partition handle ( ) , new partition offset request info ( which time , num ) ) ; } request and set offsets from kafka ( consumer , partitions , request info ) ; }	Request offsets before a specific time for a set of partitions, via a Kafka consumer.
private static void request and set offsets from kafka ( simple consumer consumer , list < kafka topic partition state < topic and partition > > partition states , map < topic and partition , partition offset request info > partition to request info ) throws io { int retries = num ; offset response response ; while ( bool ) { kafka . javaapi . offset request request = new kafka . javaapi . offset request ( partition to request info , kafka . api . offset request . current version ( ) , consumer . client id ( ) ) ; response = consumer . get offsets before ( request ) ; if ( response . has error ( ) ) { string builder exception = new string builder ( ) ; for ( kafka topic partition state < topic and partition > part : partition states ) { short code ; if ( ( code = response . error code ( part . get topic ( ) , part . get partition ( ) ) ) != error mapping . no error ( ) ) { exception . append ( str ) . append ( part . get topic ( ) ) . append ( str ) . append ( part . get partition ( ) ) . append ( str ) . append ( exception utils . stringify exception ( error mapping . exception for ( code ) ) ) ; } } if ( ++ retries >= num ) { throw new io ( str + partition states + str + exception . to string ( ) ) ; } else { log . warn ( str , exception ) ; } } else { break ;	Request offsets from Kafka with a specified set of partition's offset request information.The returned offsets are used to set the internal partition states.
public static < t > completable future < t > retry ( final supplier < completable future < t > > operation , final int retries , final executor executor ) { final completable future < t > result future = new completable future < > ( ) ; retry operation ( result future , operation , retries , executor ) ; return result future ; }	Retry the given operation the given number of times in case of a failure.
private static < t > void retry operation ( final completable future < t > result future , final supplier < completable future < t > > operation , final int retries , final executor executor ) { if ( ! result future . is done ( ) ) { final completable future < t > operation future = operation . get ( ) ; operation future . when complete async ( ( t , throwable ) -> { if ( throwable != null ) { if ( throwable instanceof cancellation exception ) { result future . complete exceptionally ( new retry exception ( str , throwable ) ) ; } else { if ( retries > num ) { retry operation ( result future , operation , retries - num , executor ) ; } else { result future . complete exceptionally ( new retry exception ( str + str , throwable ) ) ; } } } else { result future . complete ( t ) ; } } , executor ) ; result future . when complete ( ( t , throwable ) -> operation future . cancel ( bool ) ) ; } }	Helper method which retries the provided operation in case of a failure.
public static < t > completable future < t > retry successful with delay ( final supplier < completable future < t > > operation , final time retry delay , final deadline deadline , final predicate < t > acceptance predicate , final scheduled executor scheduled executor ) { final completable future < t > result future = new completable future < > ( ) ; retry successful operation with delay ( result future , operation , retry delay , deadline , acceptance predicate , scheduled executor ) ; return result future ; }	Retry the given operation with the given delay in between successful completions where theresult does not match a given predicate.
public static completable future < void > compose afterwards ( completable future < ? > future , supplier < completable future < ? > > composed action ) { final completable future < void > result future = new completable future < > ( ) ; future . when complete ( ( object outer ignored , throwable outer throwable ) -> { final completable future < ? > composed action future = composed action . get ( ) ; composed action future . when complete ( ( object inner ignored , throwable inner throwable ) -> { if ( inner throwable != null ) { result future . complete exceptionally ( exception utils . first or suppressed ( inner throwable , outer throwable ) ) ; } else if ( outer throwable != null ) { result future . complete exceptionally ( outer throwable ) ; } else { result future . complete ( null ) ; } } ) ; } ) ; return result future ; }	Run the given asynchronous action after the completion of the given future.
public static void main ( string [ ] args ) throws exception { final kafka collector [ ] collectors = new kafka collector [ num partitions ] ;	Entry point to the kafka data producer.
@ override public void suspend ( ) { component main thread executor . assert running in main thread ( ) ; log . info ( str ) ;	Suspends this pool, meaning it has lost its authority to accept and distribute slots.
@ nullable private pending request remove pending request ( slot request id request id ) { pending request result = waiting for resource manager . remove ( request id ) ; if ( result != null ) {	Checks whether there exists a pending request with the given slot request id and removes itfrom the internal data structures.
private void try fulfill slot request or make available ( allocated slot allocated slot ) { preconditions . check state ( ! allocated slot . is used ( ) , str ) ; final pending request pending request = poll matching pending request ( allocated slot ) ; if ( pending request != null ) { log . debug ( str , pending request . get slot request id ( ) , allocated slot . get allocation id ( ) ) ; allocated slots . add ( pending request . get slot request id ( ) , allocated slot ) ; pending request . get allocated slot future ( ) . complete ( allocated slot ) ; } else { log . debug ( str , allocated slot . get allocation id ( ) ) ; available slots . add ( allocated slot , clock . relative time millis ( ) ) ; } }	Tries to fulfill with the given allocated slot a pending slot request or add theallocated slot to the set of available slots if no matching request is available.
@ override public optional < id > fail allocation ( final id allocation id , final exception cause ) { component main thread executor . assert running in main thread ( ) ; final pending request pending request = pending requests . remove key b ( allocation id ) ; if ( pending request != null ) {	Fail the specified allocation and release the corresponding slot if we have one.This may triggered by JobManager when some slot allocation failed with rpcTimeout.Or this could be triggered by TaskManager, when it finds out something went wrong with the slot,and decided to take it back.
@ override public boolean register task manager ( final id resource id ) { component main thread executor . assert running in main thread ( ) ; log . debug ( str , resource id ) ; return registered task managers . add ( resource id ) ; }	Register TaskManager to this pool, only those slots come from registered TaskManager will be considered valid.Also it provides a way for us to keep "dead" or "abnormal" TaskManagers out of this pool.
@ override public boolean release task manager ( final id resource id , final exception cause ) { component main thread executor . assert running in main thread ( ) ; if ( registered task managers . remove ( resource id ) ) { release task manager internal ( resource id , cause ) ; return bool ; } else { return bool ; } }	Unregister TaskManager from this pool, all the related slots will be released and tasks be canceled.
private void check idle slot ( ) {	Check the available slots, release the slot that is idle for a long time.
private void clear ( ) { available slots . clear ( ) ; allocated slots . clear ( ) ; pending requests . clear ( ) ; waiting for resource manager . clear ( ) ; registered task managers . clear ( ) ; }	Clear the internal state of the SlotPool.
public void start query service ( rpc service rpc service , id resource id ) { synchronized ( lock ) { preconditions . check state ( ! is shutdown ( ) , str ) ; try { metric query service rpc service = rpc service ; query service = metric query service . create metric query service ( rpc service , resource id , maximum framesize ) ; query service . start ( ) ; } catch ( exception e ) { log . warn ( str , e ) ; } } }	Initializes the MetricQueryService.
public static task manager services from configuration ( task manager services configuration task manager services configuration , task manager metric group task manager metric group , id resource id , executor task io , long free heap memory with defrag , long max jvm heap memory ) throws exception {	Creates and returns the task manager services.
public static external catalog find and create external catalog ( descriptor descriptor ) { map < string , string > properties = descriptor . to properties ( ) ; return table factory service . find ( external catalog factory . class , properties ) . create external catalog ( properties ) ; }	Returns an external catalog.
public static < t > table source < t > find and create table source ( descriptor descriptor ) { map < string , string > properties = descriptor . to properties ( ) ; table source table source ; try { table source = table factory service . find ( table source factory . class , properties ) . create table source ( properties ) ; } catch ( throwable t ) { throw new table exception ( str , t ) ; } return table source ; }	Returns a table source matching the descriptor.
public static < t > table sink < t > find and create table sink ( descriptor descriptor ) { map < string , string > properties = descriptor . to properties ( ) ; table sink table sink ; try { table sink = table factory service . find ( table sink factory . class , properties ) . create table sink ( properties ) ; } catch ( throwable t ) { throw new table exception ( str , t ) ; } return table sink ; }	Returns a table sink matching the descriptor.
public < t > void set broadcast variables ( map < string , operator < t > > inputs ) { this . broadcast inputs . clear ( ) ; this . broadcast inputs . put all ( inputs ) ; }	Clears all previous broadcast inputs and binds the given inputs asbroadcast variables of this operator.
protected static < u > class < u > [ ] as array ( class < u > clazz ) { @ suppress warnings ( str ) class < u > [ ] array = new class [ ] { clazz } ; return array ; }	Generic utility function that wraps a single class object into an array of that class type.
protected static < u > class < u > [ ] empty class array ( ) { @ suppress warnings ( str ) class < u > [ ] array = new class [ num ] ; return array ; }	Generic utility function that returns an empty class array.
@ override public void update ( ) { synchronized ( this ) { long current time = system . current time millis ( ) ; if ( current time - last update time > update interval ) { last update time = current time ; fetch metrics ( ) ; } } }	This method can be used to signal this MetricFetcher that the metrics are still in use and should be updated.
private void retrieve and query metrics ( string query service address ) { log . debug ( str , query service address ) ; final completable future < metric query service gateway > query service gateway future = query service retriever . retrieve service ( query service address ) ; query service gateway future . when complete async ( ( metric query service gateway query service gateway , throwable t ) -> { if ( t != null ) { log . debug ( str , t ) ; } else { query metrics ( query service gateway ) ; } } , executor ) ; }	Retrieves and queries the specified QueryServiceGateway.
private void query metrics ( final metric query service gateway query service gateway ) { log . debug ( str , query service gateway . get address ( ) ) ; query service gateway . query metrics ( timeout ) . when complete async ( ( metric dump serialization . metric serialization result result , throwable t ) -> { if ( t != null ) { log . debug ( str , t ) ; } else { metrics . add all ( deserializer . deserialize ( result ) ) ; } } , executor ) ; }	Query the metrics from the given QueryServiceGateway.
public static rel opt cluster create ( rel opt planner planner , rex builder rex builder ) { return new rel opt cluster ( planner , rex builder . get type factory ( ) , rex builder , new atomic integer ( num ) , new hash map < string , rel node > ( ) ) ; }	Creates a cluster.
@ override public expression get value expression ( ) { return if then else ( equal to ( count , literal ( num ) ) , null of ( get result type ( ) ) , div ( sum , count ) ) ; }	If all input are nulls, count will be 0 and we will get null after the division.
private void close current part file ( bucket state < t > bucket state ) throws exception { if ( bucket state . is writer open ) { bucket state . writer . close ( ) ; bucket state . is writer open = bool ; } if ( bucket state . current file != null ) { path current part path = new path ( bucket state . current file ) ; path in progress path = get in progress path for ( current part path ) ; path pending path = get pending path for ( current part path ) ; fs . rename ( in progress path , pending path ) ; log . debug ( str , in progress path , pending path ) ; bucket state . pending files . add ( current part path . to string ( ) ) ; bucket state . current file = null ; } }	Closes the current part file and moves it from the in-progress state to the pending state.
public void try add ( abstract checkpoint stats checkpoint ) {	Try to add the checkpoint to the cache.
public < r > r wrap class loader ( supplier < r > supplier ) { try ( temporary class loader context tmp cl = new temporary class loader context ( class loader ) ) { return supplier . get ( ) ; } }	Executes the given supplier using the execution context's classloader as thread classloader.
@ override public void open ( configuration config ) throws io { streamer . open ( ) ; streamer . send broad cast variables ( config ) ; }	Opens this function.
public accumulator snapshot get snapshot ( ) { try { return new accumulator snapshot ( job id , task id , user accumulators ) ; } catch ( throwable e ) { log . warn ( str , e ) ; return null ; } }	Creates a snapshot of this accumulator registry.
public void register kv state ( key group range key group range , string registration name , internal kv state < ? , ? , ? > kv state ) { id kv state id = registry . register kv state ( job id , job vertex id , key group range , registration name , kv state ) ; registered kv states . add ( new kv state info ( key group range , registration name , kv state id ) ) ; }	Registers the KvState instance at the KvStateRegistry.
public void unregister all ( ) { for ( kv state info kv state : registered kv states ) { registry . unregister kv state ( job id , job vertex id , kv state . key group range , kv state . registration name , kv state . kv state id ) ; } }	Unregisters all registered KvState instances from the KvStateRegistry.
public config option < t > with fallback keys ( string ... fallback keys ) { final stream < fallback key > new fallback keys = arrays . stream ( fallback keys ) . map ( fallback key :: create fallback key ) ; final stream < fallback key > current alternative keys = arrays . stream ( this . fallback keys ) ;	Creates a new config option, using this option's key and default value, andadding the given fallback keys. When obtaining a value from the configuration via {.
public config option < t > with deprecated keys ( string ... deprecated keys ) { final stream < fallback key > new deprecated keys = arrays . stream ( deprecated keys ) . map ( fallback key :: create deprecated key ) ; final stream < fallback key > current alternative keys = arrays . stream ( this . fallback keys ) ;	Creates a new config option, using this option's key and default value, andadding the given deprecated keys. When obtaining a value from the configuration via {.
public iterable < fallback key > fallback keys ( ) { return ( fallback keys == empty ) ? collections . empty list ( ) : arrays . as list ( fallback keys ) ; }	Gets the fallback keys, in the order to be checked.
public static boolean has hdfs ( ) throws exception { user group information login user = user group information . get current user ( ) ; collection < token < ? extends token identifier > > usr tok = login user . get tokens ( ) ; for ( token < ? extends token identifier > token : usr tok ) { if ( token . get kind ( ) . equals ( hdfs delegation token kind ) ) { return bool ; } } return bool ; }	Indicates whether the current user has an HDFS delegation token.
public static boolean is min hadoop version ( int major , int minor ) throws flink runtime exception { string version string = version info . get version ( ) ; string [ ] version parts = version string . split ( str ) ; if ( version parts . length < num ) { throw new flink runtime exception ( str + version string ) ; } int maj = integer . parse int ( version parts [ num ] ) ; int min = integer . parse int ( version parts [ num ] ) ; return maj > major || ( maj == major && min >= minor ) ; }	Checks if the Hadoop dependency is at least of the given version.
public static < t extends specific record base > parquet writer factory < t > for specific record ( class < t > type ) { final string schema string = specific data . get ( ) . get schema ( type ) . to string ( ) ; final parquet builder < t > builder = ( out ) -> create avro parquet writer ( schema string , specific data . get ( ) , out ) ; return new parquet writer factory < > ( builder ) ; }	Creates a ParquetWriterFactory for an Avro specific type.
public static parquet writer factory < generic record > for generic record ( schema schema ) { final string schema string = schema . to string ( ) ; final parquet builder < generic record > builder = ( out ) -> create avro parquet writer ( schema string , generic data . get ( ) , out ) ; return new parquet writer factory < > ( builder ) ; }	Creates a ParquetWriterFactory that accepts and writes Avro generic types.The Parquet writers will use the given schema to build and write the columnar data.
public static < t > parquet writer factory < t > for reflect record ( class < t > type ) { final string schema string = reflect data . get ( ) . get schema ( type ) . to string ( ) ; final parquet builder < t > builder = ( out ) -> create avro parquet writer ( schema string , reflect data . get ( ) , out ) ; return new parquet writer factory < > ( builder ) ; }	Creates a ParquetWriterFactory for the given type.
@ override public option < protos . id > get framework id ( ) throws exception { synchronized ( start stop lock ) { verify is running ( ) ; option < protos . id > framework id ; byte [ ] value = framework id in zoo keeper . get value ( ) ; if ( value . length == num ) { framework id = option . empty ( ) ; } else { framework id = option . apply ( protos . id . new builder ( ) . set value ( new string ( value , config constants . default charset ) ) . build ( ) ) ; } return framework id ; } }	Get the persisted framework ID.
@ override public void set framework id ( option < protos . id > framework id ) throws exception { synchronized ( start stop lock ) { verify is running ( ) ; byte [ ] value = framework id . is defined ( ) ? framework id . get ( ) . get value ( ) . get bytes ( config constants . default charset ) : new byte [ num ] ; framework id in zoo keeper . set value ( value ) ; } }	Update the persisted framework ID.
@ override public protos . id new task id ( ) throws exception { synchronized ( start stop lock ) { verify is running ( ) ; int next count ; boolean success ; do { zoo keeper versioned value < integer > count = total task count in zoo keeper . get versioned value ( ) ; next count = count . get value ( ) + num ; success = total task count in zoo keeper . try set count ( count , next count ) ; } while ( ! success ) ; protos . id task id = protos . id . new builder ( ) . set value ( taskid format . format ( next count ) ) . build ( ) ; return task id ; } }	Generates a new task ID.
public void shutdown ( ) {	Shuts the memory manager down, trying to release all the memory it managed.
public void release ( memory segment segment ) {	Tries to release the memory for the specified segment.
public void release ( collection < memory segment > segments ) { if ( segments == null ) { return ; }	Tries to release many memory segments together.
public void release all ( object owner ) { if ( owner == null ) { return ; }	Releases all memory segments for the given owner.
public list < field reference expression > get all input fields ( ) { return field references . stream ( ) . flat map ( input -> input . values ( ) . stream ( ) ) . collect ( to list ( ) ) ; }	Gives all fields of underlying inputs in order of those inputs and order of fields within input.
public csv schema ( type information < row > schema type ) { preconditions . check not null ( schema type ) ; internal properties . put string ( format schema , type string utils . write type info ( schema type ) ) ; return this ; }	Sets the format schema with field names and the types.
public static void main ( string [ ] args ) throws exception { configuration global config = global configuration . load configuration ( ) ; python plan binder binder = new python plan binder ( global config ) ; try { binder . run plan ( args ) ; } catch ( exception e ) { system . out . println ( str + e . get message ( ) ) ; log . error ( str , e ) ; } }	Entry point for the execution of a python plan.
public < v , a extends serializable > void add accumulator ( string name , accumulator < v , a > accumulator ) { get runtime context ( ) . add accumulator ( id + separator + name , accumulator ) ; }	Adds an accumulator by prepending the given name with a random string.
static void process last row ( base row current row , boolean generate retraction , value state < base row > state , collector < base row > out ) throws exception {	Processes element to deduplicate on keys, sends current element as last row, retracts previous element ifneeded.
static void process first row ( base row current row , value state < boolean > state , collector < base row > out ) throws exception {	Processes element to deduplicate on keys, sends current element if it is first row.
@ override public iterator < t > sample ( final iterator < t > input ) { if ( fraction == num ) { return empty iterable ; } return new sampled iterator < t > ( ) { t current = null ; @ override public boolean has next ( ) { if ( current == null ) { current = get next sampled element ( ) ; } return current != null ; } @ override public t next ( ) { if ( current == null ) { return get next sampled element ( ) ; } else { t result = current ; current = null ; return result ; } } private t get next sampled element ( ) { if ( fraction <= threshold ) { double rand = random . next double ( ) ; double u = math . max ( rand , epsilon ) ; int gap = ( int ) ( math . log ( u ) / math . log ( num - fraction ) ) ; int element count = num ; if ( input . has next ( ) ) { t element = input . next ( ) ; while ( input . has next ( ) && element count < gap ) { element = input . next ( ) ; element count ++ ; } if ( element count < gap ) { return null ; } else { return element ; } } else { return null ; } } else { while ( input . has next ( ) ) { t element = input . next ( ) ; if ( random . next double ( ) <= fraction ) { return element ; } } return null ; } } } ; }	Sample the input elements, for each input element, take a Bernoulli trail for sampling.
public static < t > byte [ ] serialize value ( t value , type serializer < t > serializer ) throws io { if ( value != null ) {	Serializes the value with the given serializer.
public static < t > t deserialize value ( byte [ ] serialized value , type serializer < t > serializer ) throws io { if ( serialized value == null ) { return null ; } else { final data input deserializer deser = new data input deserializer ( serialized value , num , serialized value . length ) ; final t value = serializer . deserialize ( deser ) ; if ( deser . available ( ) > num ) { throw new io ( str + str + str ) ; } return value ; } }	Deserializes the value with the given serializer.
public static < t > list < t > deserialize list ( byte [ ] serialized value , type serializer < t > serializer ) throws io { if ( serialized value != null ) { final data input deserializer in = new data input deserializer ( serialized value , num , serialized value . length ) ; try { final list < t > result = new array list < > ( ) ; while ( in . available ( ) > num ) { result . add ( serializer . deserialize ( in ) ) ;	Deserializes all values with the given serializer.
public static < uk , uv > byte [ ] serialize map ( iterable < map . entry < uk , uv > > entries , type serializer < uk > key serializer , type serializer < uv > value serializer ) throws io { if ( entries != null ) {	Serializes all values of the Iterable with the given serializer.
public static < uk , uv > map < uk , uv > deserialize map ( byte [ ] serialized value , type serializer < uk > key serializer , type serializer < uv > value serializer ) throws io { if ( serialized value != null ) { data input deserializer in = new data input deserializer ( serialized value , num , serialized value . length ) ; map < uk , uv > result = new hash map < > ( ) ; while ( in . available ( ) > num ) { uk key = key serializer . deserialize ( in ) ; boolean is null = in . read boolean ( ) ; uv value = is null ? null : value serializer . deserialize ( in ) ; result . put ( key , value ) ; } return result ; } else { return null ; } }	Deserializes all kv pairs with the given serializer.
protected void seek input ( memory segment segment , int position in segment , int limit in segment ) { this . current segment = segment ; this . position in segment = position in segment ; this . limit in segment = limit in segment ; }	Sets the internal state of the view such that the next bytes will be read from the given memory segment,starting at the given position.
public void set shard assigner ( kinesis shard assigner shard assigner ) { this . shard assigner = check not null ( shard assigner , str ) ; closure cleaner . clean ( shard assigner , bool ) ; }	Provide a custom assigner to influence how shards are distributed over subtasks.
protected kinesis data fetcher < t > create fetcher ( list < string > streams , source function . source context < t > source context , runtime context runtime context , properties config props , kinesis deserialization schema < t > deserialization schema ) { return new kinesis data fetcher < > ( streams , source context , runtime context , config props , deserialization schema , shard assigner , periodic watermark assigner ) ; }	This method is exposed for tests that need to mock the KinesisDataFetcher in the consumer.
public static void open chained tasks ( list < chained driver < ? , ? > > tasks , abstract invokable parent ) throws exception {	Opens all chained tasks, in the order as they are stored in the array.
public static void close chained tasks ( list < chained driver < ? , ? > > tasks , abstract invokable parent ) throws exception { for ( int i = num ; i < tasks . size ( ) ; i ++ ) { final chained driver < ? , ? > task = tasks . get ( i ) ; task . close task ( ) ; if ( log . is debug enabled ( ) ) { log . debug ( construct log string ( str , task . get task name ( ) , parent ) ) ; } } }	Closes all chained tasks, in the order as they are stored in the array.
public static < t > t instantiate user code ( task config config , class loader cl , class < ? super t > super class ) { try { t stub = config . < t > get stub wrapper ( cl ) . get user code object ( super class , cl ) ;	Instantiates a user code class from is definition in the task configuration.The class is instantiated without arguments using the null-ary constructor.
@ public evolving public void set resources ( resource spec min resources , resource spec preferred resources ) { this . min resources = min resources ; this . preferred resources = preferred resources ; }	Sets the minimum and preferred resources for this contract instance.
protected void run async without fencing ( runnable runnable ) { if ( rpc server instanceof fenced main thread executable ) { ( ( fenced main thread executable ) rpc server ) . run async without fencing ( runnable ) ; } else { throw new runtime exception ( str ) ; } }	Run the given runnable in the main thread of the RpcEndpoint without checking the fencingtoken.
protected < v > completable future < v > call async without fencing ( callable < v > callable , time timeout ) { if ( rpc server instanceof fenced main thread executable ) { return ( ( fenced main thread executable ) rpc server ) . call async without fencing ( callable , timeout ) ; } else { throw new runtime exception ( str ) ; } }	Run the given callable in the main thread of the RpcEndpoint without checking the fencingtoken.
static file system kind get kind for scheme ( string scheme ) { scheme = scheme . to lower case ( locale . us ) ; if ( scheme . starts with ( str ) || scheme . starts with ( str ) || scheme . starts with ( str ) ) {	Gets the kind of the file system from its scheme.
void update summary ( completed checkpoint stats completed ) { state size . add ( completed . get state size ( ) ) ; duration . add ( completed . get end to end duration ( ) ) ; alignment buffered . add ( completed . get alignment buffered ( ) ) ; }	Updates the summary with the given completed checkpoint.
private int get partitioning fan out no estimates ( ) { return math . max ( num , find smaller prime ( ( int ) math . min ( build row count * avg record len / ( num * segment size ) , max num partitions ) ) ) ; }	Gets the number of partitions to be used for an initial hash-table.
public void free current ( ) { int before release num = available memory . size ( ) ; mem manager . release ( available memory ) ; allocated floating num -= ( before release num - available memory . size ( ) ) ; }	Free the memory not used.
public static void add deprecations ( deprecation delta [ ] deltas ) { deprecation context prev , next ; do { prev = deprecation context . get ( ) ; next = new deprecation context ( prev , deltas ) ; } while ( ! deprecation context . compare and set ( prev , next ) ) ; }	Adds a set of deprecated keys to the global deprecations.This method is lockless.
public void set deprecated properties ( ) { deprecation context deprecations = deprecation context . get ( ) ; properties props = get props ( ) ; properties overlay = get overlay ( ) ; for ( map . entry < string , deprecated key info > entry : deprecations . get deprecated key map ( ) . entry set ( ) ) { string dep key = entry . get key ( ) ; if ( ! overlay . contains ( dep key ) ) { for ( string new key : entry . get value ( ) . new keys ) { string val = overlay . get property ( new key ) ; if ( val != null ) { props . set property ( dep key , val ) ; overlay . set property ( dep key , val ) ; break ; } } } } }	Sets all deprecated properties that are not currently set but have acorresponding new property that is set.
public static synchronized void reload existing configurations ( ) { if ( log . is debug enabled ( ) ) { log . debug ( str + registry . key set ( ) . size ( ) + str ) ; } for ( configuration conf : registry . key set ( ) ) { conf . reload configuration ( ) ; } }	Reload existing configuration instances.
public double get storage size ( string name , string default value , storage unit target unit ) { preconditions . check state ( is not blank ( name ) , str ) ; string v string = get ( name ) ; if ( is blank ( v string ) ) { v string = default value ; }	Gets the Storage Size from the config, or returns the defaultValue.
public void set storage size ( string name , double value , storage unit unit ) { set ( name , value + unit . get short name ( ) ) ; }	Sets Storage Size for the specified key.
private double convert storage unit ( double value , storage unit source unit , storage unit target unit ) { double byte value = source unit . to bytes ( value ) ; return target unit . from bytes ( byte value ) ; }	convert the value from one storage unit to another.
public integer ranges get range ( string name , string default value ) { return new integer ranges ( get ( name , default value ) ) ; }	Parse the given attribute as a set of integer ranges.
public char [ ] get password ( string name ) throws io { char [ ] pass = null ; pass = get password from credential providers ( name ) ; if ( pass == null ) { pass = get password from config ( name ) ; } return pass ; }	Get the value for a known password configuration element.In order to enable the elimination of clear text passwords in config,this method attempts to resolve the property name as an alias throughthe CredentialProvider API and conditionally fallsback to config.
private credential entry get credential entry ( credential provider provider , string name ) throws io { credential entry entry = provider . get credential entry ( name ) ; if ( entry != null ) { return entry ; }	Get the credential entry by name from a credential provider.Handle key deprecation.
public class < ? > get class by name or null ( string name ) { map < string , weak reference < class < ? > > > map ; synchronized ( cache classes ) { map = cache classes . get ( class loader ) ; if ( map == null ) { map = collections . synchronized map ( new weak hash map < string , weak reference < class < ? > > > ( ) ) ; cache classes . put ( class loader , map ) ; } } class < ? > clazz = null ; weak reference < class < ? > > ref = map . get ( name ) ; if ( ref != null ) { clazz = ref . get ( ) ; } if ( clazz == null ) { try { clazz = class . for name ( name , bool , class loader ) ; } catch ( class not found exception e ) {	Load a class by name, returning null rather than throwing an exceptionif it couldn't be loaded.
public set < string > get final parameters ( ) { set < string > set final params = collections . new set from map ( new concurrent hash map < string , boolean > ( ) ) ; set final params . add all ( final parameters ) ; return set final params ; }	Get the set of parameters marked final.
private void check for override ( properties properties , string name , string attr , string value ) { string property value = properties . get property ( attr ) ; if ( property value != null && ! property value . equals ( value ) ) { log . warn ( name + str + attr + str ) ; } }	Print a warning if a property with a given name already exists with adifferent value.
public static boolean has warned deprecation ( string name ) { deprecation context deprecations = deprecation context . get ( ) ; if ( deprecations . get deprecated key map ( ) . contains key ( name ) ) { if ( deprecations . get deprecated key map ( ) . get ( name ) . accessed . get ( ) ) { return bool ; } } return bool ; }	Returns whether or not a deprecated name has been warned. If the name is notdeprecated then always return false.
public void put build row ( base row row ) throws io { final int hash code = hash ( this . build side projection . apply ( row ) . hash code ( ) , num ) ;	Put a build side row to hash table.
public void end build ( ) throws io {	End build phase.
public boolean try probe ( base row record ) throws io { if ( ! this . probe iterator . has source ( ) ) {	Find matched build side rows for a probe row.
public boolean is properly shut down ( ) { for ( file path : paths ) { if ( path != null && path . exists ( ) ) { return bool ; } } return bool ; }	Utility method to check whether the IO manager has been properly shut down.For this base implementation, this means that all files have been removed.
public void delete channel ( io . id channel ) throws io { if ( channel != null ) { if ( channel . get path file ( ) . exists ( ) && ! channel . get path file ( ) . delete ( ) ) { log . warn ( str , channel . get path ( ) ) ; } } }	Deletes the file underlying the given channel.
public static string byte to hex string ( final byte [ ] bytes , final int start , final int end ) { if ( bytes == null ) { throw new illegal argument exception ( str ) ; } int length = end - start ; char [ ] out = new char [ length * num ] ; for ( int i = start , j = num ; i < end ; i ++ ) { out [ j ++ ] = hex chars [ ( num & bytes [ i ] ) > > > num ] ; out [ j ++ ] = hex chars [ num & bytes [ i ] ] ; } return new string ( out ) ; }	Given an array of bytes it will convert the bytes to a hex stringrepresentation of the bytes.
public static string generate random alphanumeric string ( random rnd , int length ) { check not null ( rnd ) ; check argument ( length >= num ) ; string builder buffer = new string builder ( length ) ; for ( int i = num ; i < length ; i ++ ) { buffer . append ( next alphanumeric char ( rnd ) ) ; } return buffer . to string ( ) ; }	Creates a random alphanumeric string of given length.
public void start threads ( ) { if ( this . sort thread != null ) { this . sort thread . start ( ) ; } if ( this . spill thread != null ) { this . spill thread . start ( ) ; } if ( this . merge thread != null ) { this . merge thread . start ( ) ; } }	Starts all the threads that are used by this sorter.
@ override public void dispose ( ) { io . close quietly ( cancel stream registry ) ; if ( kv state registry != null ) { kv state registry . unregister all ( ) ; } last name = null ; last state = null ; key value states by name . clear ( ) ; }	Closes the state backend, releasing all internal resources, but does not delete any persistentcheckpoint data.
@ suppress warnings ( str ) public static < t > t strip proxy ( @ nullable final wrapping proxy < t > wrapping proxy ) { if ( wrapping proxy == null ) { return null ; } t delegate = wrapping proxy . get wrapped delegate ( ) ; int num proxies stripped = num ; while ( delegate instanceof wrapping proxy ) { throw if safety net exceeded ( ++ num proxies stripped ) ; delegate = ( ( wrapping proxy < t > ) delegate ) . get wrapped delegate ( ) ; } return delegate ; }	Expects a proxy, and returns the unproxied delegate.
@ override public void close ( ) { io . close quietly ( default column family handle ) ; io . close quietly ( native metric monitor ) ; io . close quietly ( db ) ;	Necessary clean up iff restore operation failed.
public static boolean is rest ssl ( configuration ssl config ) { @ suppress warnings ( str ) final boolean fallback flag = ssl config . get boolean ( security options . ssl enabled ) ; return ssl config . get boolean ( security options . ssl rest enabled , fallback flag ) ; }	Checks whether SSL for the external REST endpoint is enabled.
public static boolean is rest ssl ( configuration ssl config ) { check not null ( ssl config , str ) ; return is rest ssl ( ssl config ) && ssl config . get boolean ( security options . ssl rest authentication enabled ) ; }	Checks whether mutual SSL authentication for the external REST endpoint is enabled.
public static server socket factory create ssl ( configuration config ) throws exception { ssl ssl context = create internal ssl ( config ) ; if ( ssl context == null ) { throw new illegal configuration exception ( str ) ; } string [ ] protocols = get enabled protocols ( config ) ; string [ ] cipher suites = get enabled cipher suites ( config ) ; ssl factory = ssl context . get server socket factory ( ) ; return new ssl ( factory , protocols , cipher suites ) ; }	Creates a factory for SSL Server Sockets from the given configuration.SSL Server Sockets are always part of internal communication.
public static socket factory create ssl ( configuration config ) throws exception { ssl ssl context = create internal ssl ( config ) ; if ( ssl context == null ) { throw new illegal configuration exception ( str ) ; } return ssl context . get socket factory ( ) ; }	Creates a factory for SSL Client Sockets from the given configuration.SSL Client Sockets are always part of internal communication.
public static ssl create internal server ssl ( final configuration config ) throws exception { ssl ssl context = create internal ssl ( config ) ; if ( ssl context == null ) { throw new illegal configuration exception ( str ) ; } return new ssl ( ssl context , get enabled protocols ( config ) , get enabled cipher suites ( config ) , bool , bool , config . get integer ( security options . ssl internal handshake timeout ) , config . get integer ( security options . ssl internal close notify flush timeout ) ) ; }	Creates a SSLEngineFactory to be used by internal communication server endpoints.
@ nullable public static ssl create rest server ssl ( configuration config ) throws exception { final ssl config mode ; if ( is rest ssl ( config ) ) { config mode = ssl . mutual ; } else { config mode = ssl . server ; } return create rest ssl ( config , config mode ) ; }	Creates an SSL context for the external REST endpoint server.
@ nullable public static ssl create rest client ssl ( configuration config ) throws exception { final ssl config mode ; if ( is rest ssl ( config ) ) { config mode = ssl . mutual ; } else { config mode = ssl . client ; } return create rest ssl ( config , config mode ) ; }	Creates an SSL context for clients against the external REST endpoint.
@ nonnull public final type serializer < t > previous schema serializer ( ) { if ( cached restored serializer != null ) { return cached restored serializer ; } if ( previous serializer snapshot == null ) { throw new unsupported operation exception ( str ) ; } this . cached restored serializer = previous serializer snapshot . restore serializer ( ) ; return cached restored serializer ; }	Gets the serializer that recognizes the previous serialization schema of the state.This is the serializer that should be used for restoring the state, i.e.
@ override public completable future < acknowledge > deregister application ( final application status final status , @ nullable final string diagnostics ) { log . info ( str , final status , diagnostics ) ; try { internal deregister application ( final status , diagnostics ) ; } catch ( resource manager exception e ) { log . warn ( str , e ) ; } return completable future . completed future ( acknowledge . get ( ) ) ; }	Cleanup application and shut down cluster.
private registration response register job master internal ( final job master gateway job master gateway , id job id , string job manager address , id job manager resource id ) { if ( job manager registrations . contains key ( job id ) ) { job manager registration old job manager registration = job manager registrations . get ( job id ) ; if ( objects . equals ( old job manager registration . get job master id ( ) , job master gateway . get fencing token ( ) ) ) {	Registers a new JobMaster.
private registration response register task executor internal ( task executor gateway task executor gateway , string task executor address , id task executor resource id , int data port , hardware description hardware description ) { worker registration < worker type > old registration = task executors . remove ( task executor resource id ) ; if ( old registration != null ) {	Registers a new TaskExecutor.
protected void close job manager connection ( id job id , exception cause ) { job manager registration job manager registration = job manager registrations . remove ( job id ) ; if ( job manager registration != null ) { final id job manager resource id = job manager registration . get job manager resource id ( ) ; final job master gateway job master gateway = job manager registration . get job manager gateway ( ) ; final job master id job master id = job manager registration . get job master id ( ) ; log . info ( str , job master id , job master gateway . get address ( ) , job id ) ; job manager heartbeat manager . unmonitor target ( job manager resource id ) ; jm resource id registrations . remove ( job manager resource id ) ;	This method should be called by the framework once it detects that a currently registeredjob manager has failed.
protected void close task manager connection ( final id resource id , final exception cause ) { task manager heartbeat manager . unmonitor target ( resource id ) ; worker registration < worker type > worker registration = task executors . remove ( resource id ) ; if ( worker registration != null ) { log . info ( str , resource id , cause . get message ( ) ) ;	This method should be called by the framework once it detects that a currently registeredtask executor has failed.
protected void on fatal error ( throwable t ) { try { log . error ( str , t ) ; } catch ( throwable ignored ) { }	Notifies the ResourceManager that a fatal error has occurred and it cannot proceed.
public void notify kv state registered ( id job vertex id , key group range key group range , string registration name , id kv state id , inet socket address kv state server address ) { kv state location location = lookup table . get ( registration name ) ; if ( location == null ) {	Notifies the registry about a registered KvState instance.
public void notify kv state unregistered ( id job vertex id , key group range key group range , string registration name ) { kv state location location = lookup table . get ( registration name ) ; if ( location != null ) {	Notifies the registry about an unregistered KvState instance.
private static void extract intersecting state ( collection < keyed state handle > original subtask state handles , key group range range to extract , list < keyed state handle > extracted state collector ) { for ( keyed state handle keyed state handle : original subtask state handles ) { if ( keyed state handle != null ) { keyed state handle intersected keyed state handle = keyed state handle . get intersection ( range to extract ) ; if ( intersected keyed state handle != null ) { extracted state collector . add ( intersected keyed state handle ) ; } } } }	Extracts certain key group ranges from the given state handles and adds them to the collector.
private static void check parallelism preconditions ( operator state operator state , execution job vertex execution job vertex ) {	Verifies conditions in regards to parallelism and maxParallelism that must be met when restoring state.
private static void check state mapping completeness ( boolean allow non restored state , map < id , operator state > operator states , map < id , execution job vertex > tasks ) { set < id > all operator i = new hash set < > ( ) ; for ( execution job vertex execution job vertex : tasks . values ( ) ) { all operator i . add all ( execution job vertex . get operator i ( ) ) ; } for ( map . entry < id , operator state > operator group state entry : operator states . entry set ( ) ) { operator state operator state = operator group state entry . get value ( ) ;	Verifies that all operator states can be mapped to an execution job vertex.
public void shutdown and wait ( ) { try { client . shutdown ( ) . get ( ) ; log . info ( str ) ; } catch ( exception e ) { log . warn ( str , e ) ; } }	Shuts down the client and waits until shutdown is completed.
private completable future < kv state response > get kv state ( final id job id , final string queryable state name , final int key hash code , final byte [ ] serialized key and namespace ) { log . debug ( str , remote address ) ; try { kv state request request = new kv state request ( job id , queryable state name , key hash code , serialized key and namespace ) ; return client . send request ( remote address , request ) ; } catch ( exception e ) { log . error ( str , e ) ; return future utils . get failed future ( e ) ; } }	Returns a future holding the serialized request result.
public type serializer < t > get element serializer ( ) {	Gets the serializer for the elements contained in the list.
public static void main ( string [ ] args ) { environment information . log environment info ( log , str , args ) ; signal handler . register ( log ) ; jvm shutdown safeguard . install as shutdown hook ( log ) ; run ( args ) ; }	The entry point for the YARN task executor runner.
@ override public void return logical slot ( logical slot logical slot ) { check not null ( logical slot ) ; check argument ( logical slot instanceof slot ) ; final slot slot = ( ( slot ) logical slot ) ; check argument ( ! slot . is alive ( ) , str ) ; check argument ( slot . get owner ( ) == this , str ) ; if ( slot . mark released ( ) ) { log . debug ( str , slot ) ; synchronized ( instance lock ) { if ( is dead ) { return ; } if ( this . allocated slots . remove ( slot ) ) { this . available slots . add ( slot . get slot number ( ) ) ; if ( this . slot availability listener != null ) { this . slot availability listener . new slot available ( this ) ; } } else { throw new illegal argument exception ( str ) ; } } } }	Returns a slot that has been allocated from this instance.
public void close ( ) throws io { throwable throwable = null ; try { socket . close ( ) ; sender . close ( ) ; receiver . close ( ) ; } catch ( throwable t ) { throwable = t ; } try { destroy process ( process ) ; } catch ( throwable t ) { throwable = exception utils . first or suppressed ( t , throwable ) ; } shutdown hook util . remove shutdown hook ( shutdown thread , get class ( ) . get simple name ( ) , log ) ; exception utils . try rethrow io ( throwable ) ; }	Closes this streamer.
public final void send broad cast variables ( configuration config ) throws io { try { int broadcast count = config . get integer ( planbinder config bcvar count , num ) ; string [ ] names = new string [ broadcast count ] ; for ( int x = num ; x < names . length ; x ++ ) { names [ x ] = config . get string ( planbinder config bcvar name prefix + x , null ) ; } out . write ( new int serializer ( ) . serialize without type info ( broadcast count ) ) ; string serializer string serializer = new string serializer ( ) ; for ( string name : names ) { iterator < byte [ ] > bcv = function . get runtime context ( ) . < byte [ ] > get broadcast variable ( name ) . iterator ( ) ; out . write ( string serializer . serialize without type info ( name ) ) ; while ( bcv . has next ( ) ) { out . write byte ( num ) ; out . write ( bcv . next ( ) ) ; } out . write byte ( num ) ; } } catch ( socket timeout exception ignored ) { throw new runtime exception ( str + function . get runtime context ( ) . get task name ( ) + str + msg ) ; } }	Sends all broadcast-variables encoded in the configuration to the external process.
protected static path get checkpoint directory for job ( path base checkpoint path , id job id ) { return new path ( base checkpoint path , job id . to string ( ) ) ; }	Builds directory into which a specific job checkpoints, meaning the directory inside whichit creates the checkpoint-specific subdirectories.
public static checkpoint storage location reference encode path as reference ( path path ) { byte [ ] ref bytes = path . to string ( ) . get bytes ( standard charsets . utf 8 ) ; byte [ ] bytes = new byte [ reference magic number . length + ref bytes . length ] ; system . arraycopy ( reference magic number , num , bytes , num , reference magic number . length ) ; system . arraycopy ( ref bytes , num , bytes , reference magic number . length , ref bytes . length ) ; return new checkpoint storage location reference ( bytes ) ; }	Encodes the given path as a reference in bytes.
@ suppress warnings ( str ) public t new instance ( class loader class loader ) { try { return ( t ) compile ( class loader ) . get constructor ( object [ ] . class )	Create a new instance of this generated class.
private void restore without rescaling ( keyed state handle keyed state handle ) throws exception { if ( keyed state handle instanceof incremental remote keyed state handle ) { incremental remote keyed state handle incremental remote keyed state handle = ( incremental remote keyed state handle ) keyed state handle ; restore previous incremental files status ( incremental remote keyed state handle ) ; restore from remote state ( incremental remote keyed state handle ) ; } else if ( keyed state handle instanceof incremental local keyed state handle ) { incremental local keyed state handle incremental local keyed state handle = ( incremental local keyed state handle ) keyed state handle ; restore previous incremental files status ( incremental local keyed state handle ) ; restore from local state ( incremental local keyed state handle ) ; } else { throw new backend building exception ( str + str + incremental remote keyed state handle . class + str + incremental local keyed state handle . class + str + keyed state handle . get class ( ) ) ; } }	Recovery from a single remote incremental state without rescaling.
private void restore with rescaling ( collection < keyed state handle > restore state handles ) throws exception {	Recovery from multi incremental states with rescaling.
private keyed backend serialization proxy < k > read meta data ( stream state handle meta state handle ) throws exception { fs input stream = null ; try { input stream = meta state handle . open input stream ( ) ; cancel stream registry . register closeable ( input stream ) ; data input view in = new data input view stream wrapper ( input stream ) ; return read meta data ( in ) ; } finally { if ( cancel stream registry . unregister closeable ( input stream ) ) { input stream . close ( ) ; } } }	Reads Flink's state meta data file from the state handle.
private static void delete range ( db db , list < column family handle > column family handles , byte [ ] begin key bytes , byte [ ] end key bytes ) throws db { for ( column family handle column family handle : column family handles ) { try ( rocks iterator wrapper iterator wrapper = db . get rocks iterator ( db , column family handle ) ; db write batch wrapper = new db ( db ) ) { iterator wrapper . seek ( begin key bytes ) ; while ( iterator wrapper . is valid ( ) ) { final byte [ ] current key = iterator wrapper . key ( ) ; if ( before the prefix bytes ( current key , end key bytes ) ) { write batch wrapper . remove ( column family handle , current key ) ; } else { break ; } iterator wrapper . next ( ) ; } } } }	Delete the record falls into [beginKeyBytes, endKeyBytes) of the db.
public double parameter set default value ( double default value ) { super . set default value ( default value ) ; if ( has minimum value ) { if ( minimum value inclusive ) { util . check parameter ( default value >= minimum value , str + default value + str + minimum value + str ) ; } else { util . check parameter ( default value > minimum value , str + default value + str + minimum value + str ) ; } } if ( has maximum value ) { if ( maximum value inclusive ) { util . check parameter ( default value <= maximum value , str + default value + str + maximum value + str ) ; } else { util . check parameter ( default value < maximum value , str + default value + str + maximum value + str ) ; } } return this ; }	Set the default value.
public double parameter set minimum value ( double minimum value , boolean inclusive ) { if ( has default value ) { if ( inclusive ) { util . check parameter ( minimum value <= default value , str + minimum value + str + default value + str ) ; } else { util . check parameter ( minimum value < default value , str + minimum value + str + default value + str ) ; } } else if ( has maximum value ) { if ( inclusive && maximum value inclusive ) { util . check parameter ( minimum value <= maximum value , str + minimum value + str + maximum value + str ) ; } else { util . check parameter ( minimum value < maximum value , str + minimum value + str + maximum value + str ) ; } } this . has minimum value = bool ; this . minimum value = minimum value ; this . minimum value inclusive = inclusive ; return this ; }	Set the minimum value.
public double parameter set maximum value ( double maximum value , boolean inclusive ) { if ( has default value ) { if ( inclusive ) { util . check parameter ( maximum value >= default value , str + maximum value + str + default value + str ) ; } else { util . check parameter ( maximum value > default value , str + maximum value + str + default value + str ) ; } } else if ( has minimum value ) { if ( inclusive && minimum value inclusive ) { util . check parameter ( maximum value >= minimum value , str + maximum value + str + minimum value + str ) ; } else { util . check parameter ( maximum value > minimum value , str + maximum value + str + minimum value + str ) ; } } this . has maximum value = bool ; this . maximum value = maximum value ; this . maximum value inclusive = inclusive ; return this ; }	Set the maximum value.
public void add broadcast set for sum function ( string name , data set < ? > data ) { this . bc vars sum . add ( new tuple2 < > ( name , data ) ) ; }	Adds a data set as a broadcast set to the sum function.
public void add broadcast set for apply function ( string name , data set < ? > data ) { this . bc vars apply . add ( new tuple2 < > ( name , data ) ) ; }	Adds a data set as a broadcast set to the apply function.
@ suppress warnings ( str ) public < x > stream record < x > replace ( x element ) { this . value = ( t ) element ; return ( stream record < x > ) this ; }	Replace the currently stored value by the given new value.
@ suppress warnings ( str ) public < x > stream record < x > replace ( x value , long timestamp ) { this . timestamp = timestamp ; this . value = ( t ) value ; this . has timestamp = bool ; return ( stream record < x > ) this ; }	Replace the currently stored value by the given new value and the currently storedtimestamp with the new timestamp.
public stream record < t > copy ( t value copy ) { stream record < t > copy = new stream record < > ( value copy ) ; copy . timestamp = this . timestamp ; copy . has timestamp = this . has timestamp ; return copy ; }	Creates a copy of this stream record.
public void copy to ( t value copy , stream record < t > target ) { target . value = value copy ; target . timestamp = this . timestamp ; target . has timestamp = this . has timestamp ; }	Copies this record into the new stream record.
@ override public void on event ( task event event ) { if ( event instanceof termination event ) { termination signaled = bool ; } else if ( event instanceof all workers done event ) { all workers done event wde = ( all workers done event ) event ; aggregator names = wde . get aggregator names ( ) ; aggregates = wde . get aggregates ( user code class loader ) ; } else { throw new illegal argument exception ( str ) ; } latch . count down ( ) ; }	Barrier will release the waiting thread if an event occurs.
public static mesos task manager parameters create ( configuration flink config ) { list < constraint evaluator > constraints = parse constraints ( flink config . get string ( mesos constraints hard hostattr ) ) ;	Create the Mesos TaskManager parameters.
public static list < string > build uris ( option < string > uris ) { if ( uris . is empty ( ) ) { return collections . empty list ( ) ; } else { list < string > uris list = new array list < > ( ) ; for ( string uri : uris . get ( ) . split ( str ) ) { uris list . add ( uri . trim ( ) ) ; } return uris list ; } }	Build a list of URIs for providing custom artifacts to Mesos tasks.
public static < t extends restful gateway > optional < static file server handler < t > > try load web content ( gateway retriever < ? extends t > leader retriever , time timeout , file tmp dir ) throws io { if ( is flink runtime web in class path ( ) ) { return optional . of ( new static file server handler < > ( leader retriever , timeout , tmp dir ) ) ; } else { return optional . empty ( ) ; } }	Checks whether the flink-runtime-web dependency is available and if so returns aStaticFileServerHandler which can serve the static file contents.
@ override public iterator < t > sample ( final iterator < t > input ) { if ( fraction == num ) { return empty iterable ; } return new sampled iterator < t > ( ) { t current element ; int current count = num ; @ override public boolean has next ( ) { if ( current count > num ) { return bool ; } else { sampling process ( ) ; if ( current count > num ) { return bool ; } else { return bool ; } } } @ override public t next ( ) { if ( current count <= num ) { sampling process ( ) ; } current count -- ; return current element ; } public int poisson ge1 ( double p ) {	Sample the input elements, for each input element, generate its count following a poissondistribution.
public static simple date format new date format ( string format ) { simple date format sdf = new simple date format ( format , locale . root ) ; sdf . set lenient ( bool ) ; return sdf ; }	Creates a new date formatter with Farrago specific options.
private static long first monday of first week ( int year ) { final long jan first = ymd to julian ( year , num , num ) ; final long jan first dow = floor mod ( jan first + num , num ) ;	Returns the first day of the first week of a year.Per ISO-8601 it is the Monday of the week that contains Jan 4,or equivalently, it is a Monday between Dec 29 and Jan 4.Sometimes it is in the year before the given year.
public static long add months ( long timestamp , int m ) { final long millis = date time utils . floor mod ( timestamp , date time utils . millis per day ) ; timestamp -= millis ; final long x = add months ( ( int ) ( timestamp / date time utils . millis per day ) , m ) ; return x * date time utils . millis per day + millis ; }	Adds a given number of months to a timestamp, represented as the numberof milliseconds since the epoch.
public static int add months ( int date , int m ) { int y0 = ( int ) date time utils . unix date extract ( time unit range . year , date ) ; int m0 = ( int ) date time utils . unix date extract ( time unit range . month , date ) ; int d0 = ( int ) date time utils . unix date extract ( time unit range . day , date ) ; int y = m / num ; y0 += y ; m0 += m - y * num ; int last = last day ( y0 , m0 ) ; if ( d0 > last ) { d0 = num ; if ( ++ m0 > num ) { m0 = num ; ++ y0 ; } } return date time utils . ymd to unix date ( y0 , m0 , d0 ) ; }	Adds a given number of months to a date, represented as the number ofdays since the epoch.
public static int subtract months ( int date0 , int date1 ) { if ( date0 < date1 ) { return - subtract months ( date1 , date0 ) ; }	Finds the number of months between two dates, each represented as thenumber of days since the epoch.
public static single input semantic properties add source field offset ( single input semantic properties props , int num input fields , int offset ) { single input semantic properties offset props = new single input semantic properties ( ) ; if ( props . get read fields ( num ) != null ) { field set offset read fields = new field set ( ) ; for ( int r : props . get read fields ( num ) ) { offset read fields = offset read fields . add field ( r + offset ) ; } offset props . add read fields ( offset read fields ) ; } for ( int s = num ; s < num input fields ; s ++ ) { field set target fields = props . get forwarding target fields ( num , s ) ; for ( int t : target fields ) { offset props . add forwarded field ( s + offset , t ) ; } } return offset props ; }	Creates SemanticProperties by adding an offset to each input field index of the given SemanticProperties.
public static dual input semantic properties add source field offsets ( dual input semantic properties props , int num input fields1 , int num input fields2 , int offset1 , int offset2 ) { dual input semantic properties offset props = new dual input semantic properties ( ) ;	Creates SemanticProperties by adding offsets to each input field index of the given SemanticProperties.
public static memory size parse ( string text , memory unit default unit ) throws illegal argument exception { if ( ! has unit ( text ) ) { return parse ( text + default unit . get units ( ) [ num ] ) ; } return parse ( text ) ; }	Parses the given string with a default unit.
protected void build initial table ( final mutable object iterator < bt > input ) throws io {	Creates the initial hash table.
final void build bloom filter for bucket ( int bucket in segment pos , memory segment bucket , hash partition < bt , pt > p ) { final int count = bucket . get short ( bucket in segment pos + header count offset ) ; if ( count <= num ) { return ; } int [ ] hash codes = new int [ count ] ;	Set all the bucket memory except bucket header as the bit set of bloom filter, and use hash code of build recordsto build bloom filter.
public static int get num write behind buffers ( int num buffers ) { int num io = ( int ) ( math . log ( num buffers ) / math . log ( num ) - num ) ; return num io > num ? num : num io ; }	Determines the number of buffers to be used for asynchronous write behind.
public job with jars get plan without jars ( ) throws program invocation exception { if ( is using program entry point ( ) ) { return new job with jars ( get plan ( ) , collections . < url > empty list ( ) , classpaths , user code class loader ) ; } else { throw new program invocation exception ( str + job with jars . class . get simple name ( ) + str , get plan ( ) . get job id ( ) ) ; } }	Returns the plan without the required jars when the files are already provided by the cluster.
public list < url > get all libraries ( ) { list < url > libs = new array list < url > ( this . extracted temp libraries . size ( ) + num ) ; if ( jar file != null ) { libs . add ( jar file ) ; } for ( file tmp lib : this . extracted temp libraries ) { try { libs . add ( tmp lib . get absolute file ( ) . to uri ( ) . to url ( ) ) ; } catch ( url e ) { throw new runtime exception ( str , e ) ; } } return libs ; }	Returns all provided libraries needed to run the program.
private plan get plan ( ) throws program invocation exception { if ( this . plan == null ) { thread . current thread ( ) . set context class loader ( this . user code class loader ) ; this . plan = create plan from program ( this . program , this . args ) ; } return this . plan ; }	Returns the plan as generated from the Pact Assembler.
private static plan create plan from program ( program program , string [ ] options ) throws program invocation exception { try { return program . get plan ( options ) ; } catch ( throwable t ) { throw new program invocation exception ( str + t . get message ( ) , t ) ; } }	Takes the jar described by the given file and invokes its pact assembler class toassemble a plan.
public static task manager services configuration from configuration ( configuration configuration , long max jvm heap memory , inet address remote address , boolean local communication ) { final string [ ] tmp dirs = configuration utils . parse temp directories ( configuration ) ; string [ ] local state root dir = configuration utils . parse local state directories ( configuration ) ; if ( local state root dir . length == num ) {	Utility method to extract TaskManager config parameters from the configuration and tosanity check them.
public table operation create ( set table operation type type , table operation left , table operation right , boolean all ) { fail if streaming ( type , all ) ; validate set operation ( type , left , right ) ; return new set table operation ( left , right , type , all ) ; }	Creates a valid algebraic operation.
@ override protected union < t > translate to data flow ( operator < t > input1 , operator < t > input2 ) { return new union < t > ( input1 , input2 , union location name ) ; }	Returns the BinaryNodeTranslation of the Union.
public < t > t get field not null ( int pos ) { t field = get field ( pos ) ; if ( field != null ) { return field ; } else { throw new null field exception ( pos ) ; } }	Gets the field at the specified position, throws NullFieldException if the field is null.
public static tuple new instance ( int arity ) { switch ( arity ) { case num : return tuple0 . instance ; case num : return new tuple1 ( ) ; case num : return new tuple2 ( ) ; case num : return new tuple3 ( ) ; case num : return new tuple4 ( ) ; case num : return new tuple5 ( ) ; case num : return new tuple6 ( ) ; case num : return new tuple7 ( ) ; case num : return new tuple8 ( ) ; case num : return new tuple9 ( ) ; case num : return new tuple10 ( ) ; case num : return new tuple11 ( ) ; case num : return new tuple12 ( ) ; case num : return new tuple13 ( ) ; case num : return new tuple14 ( ) ; case num : return new tuple15 ( ) ; case num : return new tuple16 ( ) ; case num : return new tuple17 ( ) ; case num : return new tuple18 ( ) ; case num : return new tuple19 ( ) ; case num : return new tuple20 ( ) ; case num : return new tuple21 ( ) ; case num : return new tuple22 ( ) ; case num : return new tuple23 ( ) ; case num : return new tuple24 ( ) ; case num : return new tuple25 ( ) ; default : throw new illegal argument exception ( str + max arity + str ) ; } }	GENERATED FROM org.apache.flink.api.java.tuple.TupleGenerator.
public final void stream buffer without groups ( iterator < in > iterator , collector < out > c ) { single element push back iterator < in > i = new single element push back iterator < > ( iterator ) ; try { int size ; if ( i . has next ( ) ) { while ( bool ) { int sig = in . read int ( ) ; switch ( sig ) { case signal buffer request : if ( i . has next ( ) ) { size = sender . send buffer ( i ) ; send write notification ( size , i . has next ( ) ) ; } else { throw new runtime exception ( str ) ; } break ; case signal finished : return ; case signal error : try { out printer . join ( ) ; } catch ( interrupted exception e ) { out printer . interrupt ( ) ; } try { error printer . join ( ) ; } catch ( interrupted exception e ) { error printer . interrupt ( ) ; } throw new runtime exception ( str + function . get runtime context ( ) . get task name ( ) + str + msg ) ; default : receiver . collect buffer ( c , sig ) ; send read confirmation ( ) ; break ; } } } } catch ( socket timeout exception ignored ) { throw new runtime exception ( str + function . get runtime context ( ) . get task name ( ) + str + msg . get ( ) ) ; } catch ( exception e ) { throw new runtime exception ( str + function . get runtime context ( ) . get task name ( ) + str + msg . get ( ) , e ) ; } }	Sends all values contained in the iterator to the external process and collects all results.
@ visible for testing protected < k , v > kafka producer < k , v > get kafka producer ( properties props ) { return new kafka producer < > ( props ) ; }	Used for testing only.
@ override public void invoke ( in next , context context ) throws exception {	Called when new data arrives to the sink, and forwards it to Kafka.
public < t > broadcast variable materialization < t , ? > materialize broadcast variable ( string name , int superstep , batch task < ? , ? > holder , mutable reader < ? > reader , type serializer factory < t > serializer factory ) throws io { final broadcast variable key key = new broadcast variable key ( holder . get environment ( ) . get job vertex id ( ) , name , superstep ) ; while ( bool ) { final broadcast variable materialization < t , object > new mat = new broadcast variable materialization < t , object > ( key ) ; final broadcast variable materialization < ? , ? > previous = variables . put if absent ( key , new mat ) ; @ suppress warnings ( str ) final broadcast variable materialization < t , ? > materialization = ( previous == null ) ? new mat : ( broadcast variable materialization < t , ? > ) previous ; try { materialization . materialize variable ( reader , serializer factory , holder ) ; return materialization ; } catch ( materialization expired exception e ) {	Materializes the broadcast variable for the given name, scoped to the given task and its iteration superstep.
public void set group order ( int input num , ordering order ) { if ( input num == num ) { this . group order1 = order ; } else if ( input num == num ) { this . group order2 = order ; } else { throw new index out of bounds exception ( ) ; } }	Sets the order of the elements within a group for the given input.
public ordering append ordering ( integer index , class < ? extends comparable < ? > > type , order order ) { if ( index < num ) { throw new illegal argument exception ( str ) ; } if ( order == null ) { throw new null pointer exception ( ) ; } if ( order == order . none ) { throw new illegal argument exception ( str ) ; } if ( ! this . indexes . contains ( index ) ) { this . indexes = this . indexes . add field ( index ) ; this . types . add ( type ) ; this . orders . add ( order ) ; } return this ; }	Extends this ordering by appending an additional order requirement.If the index has been previously appended then the unmodified Orderingis returned.
@ override protected amazon kinesis create kinesis client ( properties config props ) { client configuration aws client config = new client configuration factory ( ) . get config ( ) ; set aws client config properties ( aws client config , config props ) ; aws credentials = get credentials provider ( config props ) ; aws client config . set user agent prefix ( string . format ( user agent format , environment information . get version ( ) , environment information . get revision information ( ) . commit id ) ) ; db adapter client = new db ( credentials , aws client config ) ; if ( config props . contains key ( aws endpoint ) ) { adapter client . set endpoint ( config props . get property ( aws endpoint ) ) ; } else { adapter client . set region ( region . get region ( regions . from name ( config props . get property ( aws region ) ) ) ) ; } return adapter client ; }	Creates an AmazonDynamoDBStreamsAdapterClient.Uses it as the internal client interacting with the DynamoDB streams.
public static string concat ( character filter filter , character delimiter , string ... components ) { string builder sb = new string builder ( ) ; sb . append ( filter . filter characters ( components [ num ] ) ) ; for ( int x = num ; x < components . length ; x ++ ) { sb . append ( delimiter ) ; sb . append ( filter . filter characters ( components [ x ] ) ) ; } return sb . to string ( ) ; }	Concatenates the given component names separated by the delimiter character.
public static void main ( string [ ] args ) throws io { string output directory = args [ num ] ; for ( final api api version : api . values ( ) ) { if ( api version == api . v0 ) {	Generates the REST API documentation.
@ suppress warnings ( str ) public static < t extends specific record > type information < row > convert to type info ( class < t > avro class ) { preconditions . check not null ( avro class , str ) ;	Converts an Avro class into a nested row structure with deterministic field order and datatypes that are compatible with Flink's Table & SQL API.
@ suppress warnings ( str ) public static < t > type information < t > convert to type info ( string avro schema string ) { preconditions . check not null ( avro schema string , str ) ; final schema schema ; try { schema = new schema . parser ( ) . parse ( avro schema string ) ; } catch ( schema parse exception e ) { throw new illegal argument exception ( str , e ) ; } return ( type information < t > ) convert to type info ( schema ) ; }	Converts an Avro schema string into a nested row structure with deterministic field order and datatypes that are compatible with Flink's Table & SQL API.
public void register type with kryo serializer ( class < ? > type , class < ? extends serializer < ? > > serializer class ) { config . register type with kryo serializer ( type , serializer class ) ; }	Registers the given Serializer via its class as a serializer for the given type at the KryoSerializer.
public < x > data source < x > from collection ( collection < x > data , type information < x > type ) { return from collection ( data , type , utils . get call location name ( ) ) ; }	Creates a DataSet from the given non-empty collection.
private < x > data source < x > from parallel collection ( splittable iterator < x > iterator , type information < x > type , string call location name ) { return new data source < > ( this , new parallel iterator input format < > ( iterator ) , type , call location name ) ; }	private helper for passing different call location names.
protected void register cached files with plan ( plan p ) throws io { for ( tuple2 < string , distributed cache entry > entry : cache file ) { p . register cached file ( entry . f0 , entry . f1 ) ; } }	Registers all files that were registered at this execution environment's cache registry of thegiven plan's cache registry.
private void process event ( nfa nfa state , in event , long timestamp ) throws exception { try ( shared buffer accessor < in > shared buffer accessor = partial matches . get accessor ( ) ) { collection < map < string , list < in > > > patterns = nfa . process ( shared buffer accessor , nfa state , event , timestamp , after match skip strategy , cep timer service ) ; process matched sequences ( patterns , timestamp ) ; } }	Process the given event by giving it to the NFA and outputting the produced set of matchedevent sequences.
private static < in , out > single output stream operator < out > add operator ( data stream < in > in , async function < in , out > func , long timeout , int buf size , output mode mode ) { type information < out > out type info = type extractor . get unary operator return type ( func , async function . class , num , num , new int [ ] { num , num } , in . get type ( ) , utils . get call location name ( ) , bool ) ;	Add an AsyncWaitOperator.
public static < in , out > single output stream operator < out > unordered wait ( data stream < in > in , async function < in , out > func , long timeout , time unit time unit , int capacity ) { return add operator ( in , func , time unit . to millis ( timeout ) , capacity , output mode . unordered ) ; }	Add an AsyncWaitOperator. The order of output stream records may be reordered.
public static < in , out > single output stream operator < out > ordered wait ( data stream < in > in , async function < in , out > func , long timeout , time unit time unit , int capacity ) { return add operator ( in , func , time unit . to millis ( timeout ) , capacity , output mode . ordered ) ; }	Add an AsyncWaitOperator. The order to process input records is guaranteed to be the same asinput ones.
public string get job parameter ( string key , string default value ) { final global job parameters conf = context . get execution config ( ) . get global job parameters ( ) ; if ( conf != null && conf . to map ( ) . contains key ( key ) ) { return conf . to map ( ) . get ( key ) ; } else { return default value ; } }	Gets the global job parameter value associated with the given key as a string.
public static deadline from now ( duration duration ) { return new deadline ( math . add exact ( system . nano time ( ) , duration . to nanos ( ) ) ) ; }	Constructs a Deadline that is a given duration after now.
public void start ( resource manager id new resource manager id , executor new main thread executor , resource actions new resource actions ) { log . info ( str ) ; this . resource manager id = preconditions . check not null ( new resource manager id ) ; main thread executor = preconditions . check not null ( new main thread executor ) ; resource actions = preconditions . check not null ( new resource actions ) ; started = bool ; task manager timeout check = scheduled executor . schedule with fixed delay ( ( ) -> main thread executor . execute ( ( ) -> check task manager timeouts ( ) ) , num , task manager timeout . to milliseconds ( ) , time unit . milliseconds ) ; slot request timeout check = scheduled executor . schedule with fixed delay ( ( ) -> main thread executor . execute ( ( ) -> check slot request timeouts ( ) ) , num , slot request timeout . to milliseconds ( ) , time unit . milliseconds ) ; }	Starts the slot manager with the given leader id and resource manager actions.
public void suspend ( ) { log . info ( str ) ;	Suspends the component. This clears the internal state of the slot manager.
public boolean register slot request ( slot request slot request ) throws slot manager exception { check init ( ) ; if ( check duplicate request ( slot request . get allocation id ( ) ) ) { log . debug ( str , slot request . get allocation id ( ) ) ; return bool ; } else { pending slot request pending slot request = new pending slot request ( slot request ) ; pending slot requests . put ( slot request . get allocation id ( ) , pending slot request ) ; try { internal request slot ( pending slot request ) ; } catch ( resource manager exception e ) {	Requests a slot with the respective resource profile.
public boolean unregister slot request ( id allocation id ) { check init ( ) ; pending slot request pending slot request = pending slot requests . remove ( allocation id ) ; if ( null != pending slot request ) { log . debug ( str , allocation id ) ; cancel pending slot request ( pending slot request ) ; return bool ; } else { log . debug ( str , allocation id ) ; return bool ; } }	Cancels and removes a pending slot request with the given allocation id.
public void register task manager ( final task executor connection task executor connection , slot report initial slot report ) { check init ( ) ; log . debug ( str , task executor connection . get resource id ( ) , task executor connection . get instance id ( ) ) ;	Registers a new task manager at the slot manager.
public boolean unregister task manager ( id instance id ) { check init ( ) ; log . debug ( str , instance id ) ; task manager registration task manager registration = task manager registrations . remove ( instance id ) ; if ( null != task manager registration ) { internal unregister task manager ( task manager registration ) ; return bool ; } else { log . debug ( str , instance id ) ; return bool ; } }	Unregisters the task manager identified by the given instance id and its associated slotsfrom the slot manager.
public boolean report slot status ( id instance id , slot report slot report ) { check init ( ) ; log . debug ( str , instance id , slot report ) ; task manager registration task manager registration = task manager registrations . get ( instance id ) ; if ( null != task manager registration ) { for ( slot status slot status : slot report ) { update slot ( slot status . get slot id ( ) , slot status . get allocation id ( ) , slot status . get job id ( ) ) ; } return bool ; } else { log . debug ( str , instance id ) ; return bool ; } }	Reports the current slot allocations for a task manager identified by the given instance id.
public void free slot ( id slot id , id allocation id ) { check init ( ) ; task manager slot slot = slots . get ( slot id ) ; if ( null != slot ) { if ( slot . get state ( ) == task manager slot . state . allocated ) { if ( objects . equals ( allocation id , slot . get allocation id ( ) ) ) { task manager registration task manager registration = task manager registrations . get ( slot . get instance id ( ) ) ; if ( task manager registration == null ) { throw new illegal state exception ( str + slot . get instance id ( ) + str ) ; } update slot state ( slot , task manager registration , null , null ) ; } else { log . debug ( str + str , slot id , allocation id , slot . get allocation id ( ) ) ; } } else { log . debug ( str , allocation id ) ; } } else { log . debug ( str , slot id ) ; } }	Free the given slot from the given allocation.
protected pending slot request find matching request ( resource profile slot resource profile ) { for ( pending slot request pending slot request : pending slot requests . values ( ) ) { if ( ! pending slot request . is assigned ( ) && slot resource profile . is matching ( pending slot request . get resource profile ( ) ) ) { return pending slot request ; } } return null ; }	Finds a matching slot request for a given resource profile.
protected task manager slot find matching slot ( resource profile request resource profile ) { iterator < map . entry < id , task manager slot > > iterator = free slots . entry set ( ) . iterator ( ) ; while ( iterator . has next ( ) ) { task manager slot task manager slot = iterator . next ( ) . get value ( ) ;	Finds a matching slot for a given resource profile.
private void register slot ( id slot id , id allocation id , id job id , resource profile resource profile , task executor connection task manager connection ) { if ( slots . contains key ( slot id ) ) {	Registers a slot for the given task manager at the slot manager.
private boolean update slot ( id slot id , id allocation id , id job id ) { final task manager slot slot = slots . get ( slot id ) ; if ( slot != null ) { final task manager registration task manager registration = task manager registrations . get ( slot . get instance id ( ) ) ; if ( task manager registration != null ) { update slot state ( slot , task manager registration , allocation id , job id ) ; return bool ; } else { throw new illegal state exception ( str + slot . get instance id ( ) + str ) ; } } else { log . debug ( str , slot id ) ; return bool ; } }	Updates a slot with the given allocation id.
private void internal request slot ( pending slot request pending slot request ) throws resource manager exception { final resource profile resource profile = pending slot request . get resource profile ( ) ; task manager slot task manager slot = find matching slot ( resource profile ) ; if ( task manager slot != null ) { allocate slot ( task manager slot , pending slot request ) ; } else { optional < pending task manager slot > pending task manager slot optional = find free matching pending task manager slot ( resource profile ) ; if ( ! pending task manager slot optional . is present ( ) ) { pending task manager slot optional = allocate resource ( resource profile ) ; } pending task manager slot optional . if present ( pending task manager slot -> assign pending task manager slot ( pending slot request , pending task manager slot ) ) ; } }	Tries to allocate a slot for the given slot request.
private void allocate slot ( task manager slot task manager slot , pending slot request pending slot request ) { preconditions . check state ( task manager slot . get state ( ) == task manager slot . state . free ) ; task executor connection task executor connection = task manager slot . get task manager connection ( ) ; task executor gateway gateway = task executor connection . get task executor gateway ( ) ; final completable future < acknowledge > completable future = new completable future < > ( ) ; final id allocation id = pending slot request . get allocation id ( ) ; final id slot id = task manager slot . get slot id ( ) ; final id instance id = task manager slot . get instance id ( ) ; task manager slot . assign pending slot request ( pending slot request ) ; pending slot request . set request future ( completable future ) ; return pending task manager slot if assigned ( pending slot request ) ; task manager registration task manager registration = task manager registrations . get ( instance id ) ; if ( task manager registration == null ) { throw new illegal state exception ( str + instance id + str ) ; } task manager registration . mark used ( ) ;	Allocates the given slot for the given slot request.
private void handle free slot ( task manager slot free slot ) { preconditions . check state ( free slot . get state ( ) == task manager slot . state . free ) ; pending slot request pending slot request = find matching request ( free slot . get resource profile ( ) ) ; if ( null != pending slot request ) { allocate slot ( free slot , pending slot request ) ; } else { free slots . put ( free slot . get slot id ( ) , free slot ) ; } }	Handles a free slot.
private void remove slot ( id slot id ) { task manager slot slot = slots . remove ( slot id ) ; if ( null != slot ) { free slots . remove ( slot id ) ; if ( slot . get state ( ) == task manager slot . state . pending ) {	Removes the given slot from the slot manager.
private void remove slot request from slot ( id slot id , id allocation id ) { task manager slot task manager slot = slots . get ( slot id ) ; if ( null != task manager slot ) { if ( task manager slot . get state ( ) == task manager slot . state . pending && objects . equals ( allocation id , task manager slot . get assigned slot request ( ) . get allocation id ( ) ) ) { task manager registration task manager registration = task manager registrations . get ( task manager slot . get instance id ( ) ) ; if ( task manager registration == null ) { throw new illegal state exception ( str + task manager slot . get instance id ( ) + str ) ; }	Removes a pending slot request identified by the given allocation id from a slot identifiedby the given slot id.
private void handle failed slot request ( id slot id , id allocation id , throwable cause ) { pending slot request pending slot request = pending slot requests . get ( allocation id ) ; log . debug ( str , allocation id , slot id , cause ) ; if ( null != pending slot request ) { pending slot request . set request future ( null ) ; try { internal request slot ( pending slot request ) ; } catch ( resource manager exception e ) { pending slot requests . remove ( allocation id ) ; resource actions . notify allocation failure ( pending slot request . get job id ( ) , allocation id , e ) ; } } else { log . debug ( str , allocation id ) ; } }	Handles a failed slot request.
private void cancel pending slot request ( pending slot request pending slot request ) { completable future < acknowledge > request = pending slot request . get request future ( ) ; return pending task manager slot if assigned ( pending slot request ) ; if ( null != request ) { request . cancel ( bool ) ; } }	Cancels the given slot request.
protected flink kafka consumer base < row > get kafka consumer ( string topic , properties properties , deserialization schema < row > deserialization schema ) { flink kafka consumer base < row > kafka consumer = create kafka consumer ( topic , properties , deserialization schema ) ; switch ( startup mode ) { case earliest : kafka consumer . set start from earliest ( ) ; break ; case latest : kafka consumer . set start from latest ( ) ; break ; case group offsets : kafka consumer . set start from group offsets ( ) ; break ; case specific offsets : kafka consumer . set start from specific offsets ( specific startup offsets ) ; break ; } return kafka consumer ; }	Returns a version-specific Kafka consumer with the start position configured.
public void trigger checkpoint on barrier ( checkpoint meta data checkpoint meta data , checkpoint options checkpoint options , checkpoint metrics checkpoint metrics ) throws exception { throw new unsupported operation exception ( string . format ( str , this . get class ( ) . get name ( ) ) ) ; }	This method is called when a checkpoint is triggered as a result of receiving checkpointbarriers on all input streams.
public map < string , object > get all accumulator results ( ) { return accumulator results . entry set ( ) . stream ( ) . collect ( collectors . to map ( map . entry :: get key , entry -> entry . get value ( ) . get unchecked ( ) ) ) ; }	Gets all accumulators produced by the job.
@ deprecated @ public evolving public integer get int counter result ( string accumulator name ) { object result = this . accumulator results . get ( accumulator name ) . get unchecked ( ) ; if ( result == null ) { return null ; } if ( ! ( result instanceof integer ) ) { throw new class cast exception ( str + accumulator name + str + result . get class ( ) ) ; } return ( integer ) result ; }	Gets the accumulator with the given name as an integer.
protected two phase commit sink function < in , txn , context > set transaction timeout ( long transaction timeout ) { check argument ( transaction timeout >= num , str ) ; this . transaction timeout = transaction timeout ; return this ; }	Sets the transaction timeout.
public int hash ( ) { hash ^= num * count ; hash ^= hash > > > num ; hash *= num ; hash ^= hash > > > num ; hash *= num ; hash ^= hash > > > num ; return hash ; }	Finalize and return the MurmurHash output.
static set < string > extract port keys ( configuration config ) { final linked hash set < string > tm port keys = new linked hash set < > ( tm port keys ) ; final string port keys = config . get string ( port assignments ) ; if ( port keys != null ) { arrays . stream ( port keys . split ( str ) ) . map ( string :: trim ) . peek ( key -> log . debug ( str ) ) . for each ( tm port keys :: add ) ; } return collections . unmodifiable set ( tm port keys ) ; }	Get the port keys representing the TM's configured endpoints. This includes mandatory TM endpoints such asdata and rpc as well as optionally configured endpoints for services such as prometheus reporter.
static void configure artifact server ( mesos artifact server server , container specification container ) throws io {	Configures an artifact server to serve the artifacts associated with a container specification.
public circulant graph add range ( long offset , long length ) { preconditions . check argument ( offset >= minimum offset , str + minimum offset ) ; preconditions . check argument ( length <= vertex count - offset , str ) ; offset ranges . add ( new offset range ( offset , length ) ) ; return this ; }	Required configuration for each range of offsets in the graph.
public static curator framework use namespace and ensure path ( final curator framework client , final string path ) throws exception { preconditions . check not null ( client , str ) ; preconditions . check not null ( path , str ) ;	Returns a facade of the client that uses the specified namespace, and ensures that all nodesin the path exist.
public static full type info get full template type ( type type , int template position ) { if ( type instanceof parameterized type ) { return get full template type ( ( ( parameterized type ) type ) . get actual type arguments ( ) [ template position ] ) ; } else { throw new illegal argument exception ( ) ; } }	Extract the full template type information from the given type's template parameter at thegiven position.
public static full type info get full template type ( type type ) { if ( type instanceof parameterized type ) { parameterized type parameterized type = ( parameterized type ) type ; full type info [ ] template type infos = new full type info [ parameterized type . get actual type arguments ( ) . length ] ; for ( int i = num ; i < parameterized type . get actual type arguments ( ) . length ; i ++ ) { template type infos [ i ] = get full template type ( parameterized type . get actual type arguments ( ) [ i ] ) ; } return new full type info ( ( class < ? > ) parameterized type . get raw type ( ) , template type infos ) ; } else { return new full type info ( ( class < ? > ) type , null ) ; } }	Extract the full type information from the given type.
static string sql to regex like ( string sql pattern , char escape char ) { int i ; final int len = sql pattern . length ( ) ; final string builder java pattern = new string builder ( len + len ) ; for ( i = num ; i < len ; i ++ ) { char c = sql pattern . char at ( i ) ; if ( java regex specials . index of ( c ) >= num ) { java pattern . append ( str ) ; } if ( c == escape char ) { if ( i == ( sql pattern . length ( ) - num ) ) { throw invalid escape sequence ( sql pattern , i ) ; } char next char = sql pattern . char at ( i + num ) ; if ( ( next char == str ) || ( next char == str ) || ( next char == escape char ) ) { java pattern . append ( next char ) ; i ++ ; } else { throw invalid escape sequence ( sql pattern , i ) ; } } else if ( c == str ) { java pattern . append ( str ) ; } else if ( c == str ) { java pattern . append ( str ) ; } else { java pattern . append ( c ) ; } } return java pattern . to string ( ) ; }	Translates a SQL LIKE pattern to Java regex pattern.
static string sql to regex similar ( string sql pattern , char sequence escape str ) { final char escape char ; if ( escape str != null ) { if ( escape str . length ( ) != num ) { throw invalid escape character ( escape str . to string ( ) ) ; } escape char = escape str . char at ( num ) ; } else { escape char = num ; } return sql to regex similar ( sql pattern , escape char ) ; }	Translates a SQL SIMILAR pattern to Java regex pattern, with optionalescape string.
static string sql to regex similar ( string sql pattern , char escape char ) { similar escape rule checking ( sql pattern , escape char ) ; boolean inside character enumeration = bool ; final string builder java pattern = new string builder ( sql pattern . length ( ) * num ) ; final int len = sql pattern . length ( ) ; for ( int i = num ; i < len ; i ++ ) { char c = sql pattern . char at ( i ) ; if ( c == escape char ) { if ( i == ( len - num ) ) {	Translates SQL SIMILAR pattern to Java regex pattern.
private static string to html table ( final list < option with meta info > options ) { string builder html table = new string builder ( ) ; html table . append ( str ) ; html table . append ( str ) ; html table . append ( str ) ; html table . append ( str ) ; html table . append ( str ) ; html table . append ( str ) ; html table . append ( str ) ; html table . append ( str ) ; html table . append ( str ) ; for ( option with meta info option : options ) { html table . append ( to html string ( option ) ) ; } html table . append ( str ) ; html table . append ( str ) ; return html table . to string ( ) ; }	Transforms this configuration group into HTML formatted table.Options are sorted alphabetically by key.
private static string to html string ( final option with meta info option with meta info ) { config option < ? > option = option with meta info . option ; string default value = stringify default ( option with meta info ) ; return str + str + str + escape characters ( option . key ( ) ) + str + str + escape characters ( add word break opportunities ( default value ) ) + str + str + formatter . format ( option . description ( ) ) + str + str ; }	Transforms option to table row.
@ override public void dispose ( ) { if ( this . disposed ) { return ; } super . dispose ( ) ;	Should only be called by one thread, and only after all accesses to the DB happened.
public boolean starts with ( char sequence prefix , int start index ) { final char [ ] this chars = this . value ; final int p len = this . len ; final int s len = prefix . length ( ) ; if ( ( start index < num ) || ( start index > p len - s len ) ) { return bool ; } int s pos = num ; while ( s pos < s len ) { if ( this chars [ start index ++ ] != prefix . char at ( s pos ++ ) ) { return bool ; } } return bool ; }	Checks whether the substring, starting at the specified index, starts with the given prefix string.
private void grow ( int size ) { if ( this . value . length < size ) { char [ ] value = new char [ math . max ( this . value . length * num / num , size ) ] ; system . arraycopy ( this . value , num , value , num , this . len ) ; this . value = value ; } }	Grow and retain content.
@ deprecated public optional < table stats > get table stats ( ) { descriptor properties normalized props = new descriptor properties ( ) ; normalized props . put properties ( normalized props ) ; optional < long > row count = normalized props . get optional long ( statistics row count ) ; if ( row count . is present ( ) ) { map < string , column stats > column stats = read column stats ( normalized props , statistics columns ) ; return optional . of ( new table stats ( row count . get ( ) , column stats ) ) ; } else { return optional . empty ( ) ; } }	Reads table statistics from the descriptors properties.
@ override public int close ( ) throws io { if ( ! writer . is closed ( ) ) { int current position in segment = get current position in segment ( ) ;	Closes this OutputView, closing the underlying writer.
public void add broadcast set ( string name , data set < ? > data ) { this . bc vars . add ( new tuple2 < > ( name , data ) ) ; }	Adds a data set as a broadcast set to the compute function.
public optimized plan compile ( plan program ) throws compiler exception { final optimizer post pass post passer = get post pass from plan ( program ) ; return compile ( program , post passer ) ; }	Translates the given program to an OptimizedPlan, where all nodes have their local strategy assignedand all channels have a shipping strategy assigned.For more details on the optimization phase, see the comments for{.
public void register buffer pool ( buffer pool buffer pool ) { check argument ( buffer pool . get number of required memory segments ( ) >= get number of subpartitions ( ) , str ) ; check state ( this . buffer pool == null , str ) ; this . buffer pool = check not null ( buffer pool ) ; }	Registers a buffer pool with this result partition.
public void finish ( ) throws io { boolean success = bool ; try { check in produce state ( ) ; for ( result subpartition subpartition : subpartitions ) { subpartition . finish ( ) ; } success = bool ; } finally { if ( success ) { is finished = bool ; notify pipelined consumers ( ) ; } } }	Finishes the result partition.
public void release ( throwable cause ) { if ( is released . compare and set ( bool , bool ) ) { log . debug ( str , owning task name , this ) ;	Releases the result partition.
public result subpartition view create subpartition view ( int index , buffer availability listener availability listener ) throws io { int ref cnt = pending references . get ( ) ; check state ( ref cnt != - num , str ) ; check state ( ref cnt > num , str ) ; check element index ( index , subpartitions . length , str ) ; result subpartition view read view = subpartitions [ index ] . create read view ( availability listener ) ; log . debug ( str , read view ) ; return read view ; }	Returns the requested subpartition.
@ override public void release memory ( int to release ) throws io { check argument ( to release > num ) ; for ( result subpartition subpartition : subpartitions ) { to release -= subpartition . release memory ( ) ;	Releases buffers held by this result partition.
void pin ( ) { while ( bool ) { int ref cnt = pending references . get ( ) ; if ( ref cnt >= num ) { if ( pending references . compare and set ( ref cnt , ref cnt + subpartitions . length ) ) { break ; } } else { throw new illegal state exception ( str ) ; } } }	Pins the result partition.
void on consumed subpartition ( int subpartition index ) { if ( is released . get ( ) ) { return ; } int ref cnt = pending references . decrement and get ( ) ; if ( ref cnt == num ) { partition manager . on consumed partition ( this ) ; } else if ( ref cnt < num ) { throw new illegal state exception ( str ) ; } log . debug ( str , this , subpartition index , pending references ) ; }	Notification when a subpartition is released.
private void notify pipelined consumers ( ) { if ( send schedule or update consumers message && ! has notified pipelined consumers && partition type . is pipelined ( ) ) { partition consumable notifier . notify partition consumable ( job id , partition id , task actions ) ; has notified pipelined consumers = bool ; } }	Notifies pipelined consumers of this result partition once.
public r < t > set constants ( float a , float b , float c ) { preconditions . check argument ( a >= num && b >= num && c >= num && a + b + c <= num , str ) ; this . a = a ; this . b = b ; this . c = c ; return this ; }	The parameters for recursively subdividing the adjacency matrix.
public r < t > set noise ( boolean noise enabled , float noise ) { preconditions . check argument ( noise >= num && noise <= num , str ) ; this . noise enabled = noise enabled ; this . noise = noise ; return this ; }	Enable and configure noise.
@ suppress warnings ( { str , str } ) public data stream < t > close with ( data stream < t > feedback stream ) { collection < stream transformation < ? > > predecessors = feedback stream . get transformation ( ) . get transitive predecessors ( ) ; if ( ! predecessors . contains ( this . transformation ) ) { throw new unsupported operation exception ( str ) ; } ( ( feedback transformation ) get transformation ( ) ) . add feedback edge ( feedback stream . get transformation ( ) ) ; return feedback stream ; }	Closes the iteration. This method defines the end of the iterativeprogram part that will be fed back to the start of the iteration. A common usage pattern for streaming iterations is to use outputsplitting to send a part of the closing data stream to the head. Refer to{.
public void open ( ) { is running = bool ;	Opens the interactive CLI shell.
public static slot profile no locality ( resource profile resource profile ) { return new slot profile ( resource profile , collections . empty list ( ) , collections . empty list ( ) ) ; }	Returns a slot profile for the given resource profile, without any locality requirements.
public static slot profile preferred locality ( resource profile resource profile , collection < task manager location > preferred locations ) { return new slot profile ( resource profile , preferred locations , collections . empty list ( ) ) ; }	Returns a slot profile for the given resource profile and the preferred locations.
public static slot profile prior allocation ( resource profile resource profile , collection < id > prior allocations ) { return new slot profile ( resource profile , collections . empty list ( ) , prior allocations ) ; }	Returns a slot profile for the given resource profile and the prior allocations.
public final void send combined message ( message combined message ) { out value . f1 = either . right ( combined message ) ; out . collect ( out value ) ; }	Sends the combined message to the target vertex.
@ override public void aggregate ( t value ) { if ( value == null ) { null count ++ ; } else if ( is nan ( value ) ) { nan count ++ ; } else if ( is infinite ( value ) ) { infinity count ++ ; } else { non missing count ++ ; min . aggregate ( value ) ; max . aggregate ( value ) ; sum . aggregate ( value ) ; double double value = value . double value ( ) ; double delta = double value - mean . value ( ) ; mean = mean . add ( delta / non missing count ) ; m2 = m2 . add ( delta * ( double value - mean . value ( ) ) ) ; } }	Add a value to the current aggregation.
@ override public void combine ( aggregator < t , numeric column summary < t > > other same type ) { numeric summary aggregator < t > other = ( numeric summary aggregator < t > ) other same type ; null count += other . null count ; nan count += other . nan count ; infinity count += other . infinity count ; if ( non missing count == num ) { non missing count = other . non missing count ; min = other . min ; max = other . max ; sum = other . sum ; mean = other . mean ; m2 = other . m2 ; } else if ( other . non missing count != num ) { long combined count = non missing count + other . non missing count ; min . combine ( other . min ) ; max . combine ( other . max ) ; sum . combine ( other . sum ) ; double delta mean = other . mean . value ( ) - mean . value ( ) ; mean = mean . add ( delta mean * other . non missing count / combined count ) ; m2 = m2 . add ( other . m2 ) . add ( delta mean * delta mean * non missing count * other . non missing count / combined count ) ; non missing count = combined count ; } }	combine two aggregations.
public cassandra sink < in > name ( string name ) { if ( use data stream sink ) { get sink transformation ( ) . set name ( name ) ; } else { get stream transformation ( ) . set name ( name ) ; } return this ; }	Sets the name of this sink.
@ public evolving public cassandra sink < in > uid ( string uid ) { if ( use data stream sink ) { get sink transformation ( ) . set uid ( uid ) ; } else { get stream transformation ( ) . set uid ( uid ) ; } return this ; }	Sets an ID for this operator.
public cassandra sink < in > set parallelism ( int parallelism ) { if ( use data stream sink ) { get sink transformation ( ) . set parallelism ( parallelism ) ; } else { get stream transformation ( ) . set parallelism ( parallelism ) ; } return this ; }	Sets the parallelism for this sink.
public cassandra sink < in > slot sharing group ( string slot sharing group ) { if ( use data stream sink ) { get sink transformation ( ) . set slot sharing group ( slot sharing group ) ; } else { get stream transformation ( ) . set slot sharing group ( slot sharing group ) ; } return this ; }	Sets the slot sharing group of this operation.
public final v put ( k key , v value ) { final int hash = hash ( key ) ; final int slot = index of ( hash ) ;	Inserts the given value, mapped under the given key.
public v get ( k key ) { final int hash = hash ( key ) ; final int slot = index of ( hash ) ;	Looks up the value mapped under the given key.
@ override public iterator < entry < k , v > > iterator ( ) { return new iterator < entry < k , v > > ( ) { private final entry < k , v > [ ] tab = key map . this . table ; private entry < k , v > next entry ; private int next pos = num ; @ override public boolean has next ( ) { if ( next entry != null ) { return bool ; } else { while ( next pos < tab . length ) { entry < k , v > e = tab [ next pos ++ ] ; if ( e != null ) { next entry = e ; return bool ; } } return bool ; } } @ override public entry < k , v > next ( ) { if ( next entry != null || has next ( ) ) { entry < k , v > e = next entry ; next entry = next entry . next ; return e ; } else { throw new no such element exception ( ) ; } } @ override public void remove ( ) { throw new unsupported operation exception ( ) ; } } ; }	Creates an iterator over the entries of this map.
public void initialize buffer metrics ( task task ) { final metric group buffers = add group ( str ) ; buffers . gauge ( str , new input buffers gauge ( task ) ) ; buffers . gauge ( str , new output buffers gauge ( task ) ) ; buffers . gauge ( str , new input buffer pool usage gauge ( task ) ) ; buffers . gauge ( str , new output buffer pool usage gauge ( task ) ) ; }	Initialize Buffer Metrics for a task.
public key group range get intersection ( key group range other ) { int start = math . max ( start key group , other . start key group ) ; int end = math . min ( end key group , other . end key group ) ; return start <= end ? new key group range ( start , end ) : empty key group range ; }	Create a range that represent the intersection between this range and the given range.
public static key group range of ( int start key group , int end key group ) { return start key group <= end key group ? new key group range ( start key group , end key group ) : empty key group range ; }	Factory method that also handles creation of empty key-groups.
public static void terminate rpc service ( rpc service rpc service , time timeout ) throws interrupted exception , execution exception , timeout exception { rpc service . stop service ( ) . get ( timeout . to milliseconds ( ) , time unit . milliseconds ) ; }	Shuts the given rpc service down and waits for its termination.
public static void terminate rpc services ( time timeout , rpc service ... rpc services ) throws interrupted exception , execution exception , timeout exception { final collection < completable future < ? > > termination futures = new array list < > ( rpc services . length ) ; for ( rpc service service : rpc services ) { if ( service != null ) { termination futures . add ( service . stop service ( ) ) ; } } future utils . wait for all ( termination futures ) . get ( timeout . to milliseconds ( ) , time unit . milliseconds ) ; }	Shuts the given rpc services down and waits for their termination.
public void set broadcast inputs ( list < named channel > broadcast inputs ) { if ( broadcast inputs != null ) { this . broadcast inputs = broadcast inputs ;	Sets a list of all broadcast inputs attached to this node.
private void stop resources ( boolean wait for shutdown ) throws interrupted exception { emitter . stop ( ) ; emitter thread . interrupt ( ) ; executor . shutdown ( ) ; if ( wait for shutdown ) { try { if ( ! executor . await termination ( num , time unit . days ) ) { executor . shutdown now ( ) ; } } catch ( interrupted exception e ) { executor . shutdown now ( ) ; thread . current thread ( ) . interrupt ( ) ; } if ( thread . holds lock ( checkpointing lock ) ) { while ( emitter thread . is alive ( ) ) { checkpointing lock . wait ( num ) ; } } emitter thread . join ( ) ; } else { executor . shutdown now ( ) ; } }	Close the operator's resources.
private static void assert not end of input ( final json parser p , @ nullable final json token json token ) { check state ( json token != null , str , p . get current location ( ) ) ; }	Asserts that the provided JsonToken is not null, i.e., not at the end of the input.
@ deprecated public void set first inputs ( list < operator < i > > inputs ) { this . input1 = operator . create union cascade ( inputs ) ; }	Sets the first input to the union of the given operators.
@ deprecated public void set second inputs ( list < operator < i > > inputs ) { this . input2 = operator . create union cascade ( inputs ) ; }	Sets the second input to the union of the given operators.
private static void update job overview ( file web overview dir , file web dir ) { try ( json generator gen = jackson factory . create generator ( history server . create or get file ( web dir , jobs overview headers . url ) ) ) { file [ ] overviews = new file ( web overview dir . get path ( ) ) . list files ( ) ; if ( overviews != null ) { collection < job details > all jobs = new array list < > ( overviews . length ) ; for ( file overview : overviews ) { multiple jobs details sub jobs = mapper . read value ( overview , multiple jobs details . class ) ; all jobs . add all ( sub jobs . get jobs ( ) ) ; } mapper . write value ( gen , new multiple jobs details ( all jobs ) ) ; } } catch ( io ioe ) { log . error ( str , ioe ) ; } }	This method replicates the JSON response that would be given by the JobsOverviewHandler whenlisting both running and finished jobs.
public optional < tuple2 < string , uuid > > get leader now ( ) throws exception { completable future < tuple2 < string , uuid > > leader future = this . atomic leader future . get ( ) ; if ( leader future != null ) { if ( leader future . is done ( ) ) { return optional . of ( leader future . get ( ) ) ; } else { return optional . empty ( ) ; } } else { return optional . empty ( ) ; } }	Returns the current leader information if available.
public static < e extends enum < e > > type information < e > enum ( class < e > enum type ) { return new enum type info < > ( enum type ) ; }	Returns type information for Java enumerations.
public join operator sets predicate base where ( string ... fields ) { return new join operator sets predicate base ( new keys . expression keys < > ( fields , input1 . get type ( ) ) ) ; }	Continues a Join transformation. Defines the fields of the first join {.
public id register task manager ( task manager gateway task manager gateway , task manager location task manager location , hardware description resources , int number of slots ) { synchronized ( this . lock ) { if ( this . is shutdown ) { throw new illegal state exception ( str ) ; } instance prior = registered hosts by resource . get ( task manager location . get resource id ( ) ) ; if ( prior != null ) { throw new illegal state exception ( str + task manager location . address string ( ) + str + prior . get id ( ) ) ; } boolean was dead = this . dead hosts . remove ( task manager location . get resource id ( ) ) ; if ( was dead ) { log . info ( str + task manager location . address string ( ) + str ) ; } id instance id = new id ( ) ; instance host = new instance ( task manager gateway , task manager location , instance id , resources , number of slots ) ; registered hosts by id . put ( instance id , host ) ; registered hosts by resource . put ( task manager location . get resource id ( ) , host ) ; total number of alive task slots += number of slots ; if ( log . is info enabled ( ) ) { log . info ( string . format ( str + str + str , task manager location . get hostname ( ) , task manager gateway . get address ( ) , instance id , registered hosts by id . size ( ) , total number of alive task slots ) ) ; } host . report heart beat ( ) ;	Registers a task manager.
public void unregister all task managers ( ) { for ( instance instance : registered hosts by id . values ( ) ) { dead hosts . add ( instance . get task manager id ( ) ) ; instance . mark dead ( ) ; total number of alive task slots -= instance . get total number of slots ( ) ; notify dead instance ( instance ) ; } registered hosts by id . clear ( ) ; registered hosts by resource . clear ( ) ; }	Unregisters all currently registered TaskManagers from the InstanceManager.
public static row of ( object ... values ) { row row = new row ( values . length ) ; for ( int i = num ; i < values . length ; i ++ ) { row . set field ( i , values [ i ] ) ; } return row ; }	Creates a new Row and assigns the given values to the Row's fields.This is more convenient than using the constructor.
public static row copy ( row row ) { final row new row = new row ( row . fields . length ) ; system . arraycopy ( row . fields , num , new row . fields , num , row . fields . length ) ; return new row ; }	Creates a new Row which copied from another row.This method does not perform a deep copy.
public static row project ( row row , int [ ] fields ) { final row new row = new row ( fields . length ) ; for ( int i = num ; i < fields . length ; i ++ ) { new row . fields [ i ] = row . fields [ fields [ i ] ] ; } return new row ; }	Creates a new Row with projected fields from another row.This method does not perform a deep copy.
public static void copy to unsafe ( memory segment [ ] segments , int offset , object target , int pointer , int num bytes ) { if ( in first segment ( segments , offset , num bytes ) ) { segments [ num ] . copy to unsafe ( offset , target , pointer , num bytes ) ; } else { copy multi segments to unsafe ( segments , offset , target , pointer , num bytes ) ; } }	Copy segments to target unsafe pointer.
public static byte [ ] get bytes ( memory segment [ ] segments , int base offset , int size in bytes ) {	Maybe not copied, if want copy, please use copyTo.
public static int hash by words ( memory segment [ ] segments , int offset , int num bytes ) { if ( in first segment ( segments , offset , num bytes ) ) { return murmur hash util . hash bytes by words ( segments [ num ] , offset , num bytes ) ; } else { return hash multi seg by words ( segments , offset , num bytes ) ; } }	hash segments to int, numBytes must be aligned to 4 bytes.
public static int hash ( memory segment [ ] segments , int offset , int num bytes ) { if ( in first segment ( segments , offset , num bytes ) ) { return murmur hash util . hash bytes ( segments [ num ] , offset , num bytes ) ; } else { return hash multi seg ( segments , offset , num bytes ) ; } }	hash segments to int.
public static void bit un set ( memory segment segment , int base offset , int index ) { int offset = base offset + ( ( index & bit byte position mask ) > > > num ) ; byte current = segment . get ( offset ) ; current &= ~ ( num << ( index & bit byte index mask ) ) ; segment . put ( offset , current ) ; }	unset bit.
public static boolean bit get ( memory segment segment , int base offset , int index ) { int offset = base offset + ( ( index & bit byte position mask ) > > > num ) ; byte current = segment . get ( offset ) ; return ( current & ( num << ( index & bit byte index mask ) ) ) != num ; }	read bit.
public static void bit un set ( memory segment [ ] segments , int base offset , int index ) { if ( segments . length == num ) { memory segment segment = segments [ num ] ; int offset = base offset + ( ( index & bit byte position mask ) > > > num ) ; byte current = segment . get ( offset ) ; current &= ~ ( num << ( index & bit byte index mask ) ) ; segment . put ( offset , current ) ; } else { bit un set multi segments ( segments , base offset , index ) ; } }	unset bit from segments.
public static void bit set ( memory segment [ ] segments , int base offset , int index ) { if ( segments . length == num ) { int offset = base offset + ( ( index & bit byte position mask ) > > > num ) ; memory segment segment = segments [ num ] ; byte current = segment . get ( offset ) ; current |= ( num << ( index & bit byte index mask ) ) ; segment . put ( offset , current ) ; } else { bit set multi segments ( segments , base offset , index ) ; } }	set bit from segments.
public static boolean bit get ( memory segment [ ] segments , int base offset , int index ) { int offset = base offset + ( ( index & bit byte position mask ) > > > num ) ; byte current = get byte ( segments , offset ) ; return ( current & ( num << ( index & bit byte index mask ) ) ) != num ; }	read bit from segments.
public static boolean get boolean ( memory segment [ ] segments , int offset ) { if ( in first segment ( segments , offset , num ) ) { return segments [ num ] . get boolean ( offset ) ; } else { return get boolean multi segments ( segments , offset ) ; } }	get boolean from segments.
public static void set boolean ( memory segment [ ] segments , int offset , boolean value ) { if ( in first segment ( segments , offset , num ) ) { segments [ num ] . put boolean ( offset , value ) ; } else { set boolean multi segments ( segments , offset , value ) ; } }	set boolean from segments.
public static byte get byte ( memory segment [ ] segments , int offset ) { if ( in first segment ( segments , offset , num ) ) { return segments [ num ] . get ( offset ) ; } else { return get byte multi segments ( segments , offset ) ; } }	get byte from segments.
public static void set byte ( memory segment [ ] segments , int offset , byte value ) { if ( in first segment ( segments , offset , num ) ) { segments [ num ] . put ( offset , value ) ; } else { set byte multi segments ( segments , offset , value ) ; } }	set byte from segments.
public static int get int ( memory segment [ ] segments , int offset ) { if ( in first segment ( segments , offset , num ) ) { return segments [ num ] . get int ( offset ) ; } else { return get int multi segments ( segments , offset ) ; } }	get int from segments.
public static void set int ( memory segment [ ] segments , int offset , int value ) { if ( in first segment ( segments , offset , num ) ) { segments [ num ] . put int ( offset , value ) ; } else { set int multi segments ( segments , offset , value ) ; } }	set int from segments.
public static long get long ( memory segment [ ] segments , int offset ) { if ( in first segment ( segments , offset , num ) ) { return segments [ num ] . get long ( offset ) ; } else { return get long multi segments ( segments , offset ) ; } }	get long from segments.
public static void set long ( memory segment [ ] segments , int offset , long value ) { if ( in first segment ( segments , offset , num ) ) { segments [ num ] . put long ( offset , value ) ; } else { set long multi segments ( segments , offset , value ) ; } }	set long from segments.
public static short get short ( memory segment [ ] segments , int offset ) { if ( in first segment ( segments , offset , num ) ) { return segments [ num ] . get short ( offset ) ; } else { return get short multi segments ( segments , offset ) ; } }	get short from segments.
public static void set short ( memory segment [ ] segments , int offset , short value ) { if ( in first segment ( segments , offset , num ) ) { segments [ num ] . put short ( offset , value ) ; } else { set short multi segments ( segments , offset , value ) ; } }	set short from segments.
public static float get float ( memory segment [ ] segments , int offset ) { if ( in first segment ( segments , offset , num ) ) { return segments [ num ] . get float ( offset ) ; } else { return get float multi segments ( segments , offset ) ; } }	get float from segments.
public static void set float ( memory segment [ ] segments , int offset , float value ) { if ( in first segment ( segments , offset , num ) ) { segments [ num ] . put float ( offset , value ) ; } else { set float multi segments ( segments , offset , value ) ; } }	set float from segments.
public static double get double ( memory segment [ ] segments , int offset ) { if ( in first segment ( segments , offset , num ) ) { return segments [ num ] . get double ( offset ) ; } else { return get double multi segments ( segments , offset ) ; } }	get double from segments.
public static void set double ( memory segment [ ] segments , int offset , double value ) { if ( in first segment ( segments , offset , num ) ) { segments [ num ] . put double ( offset , value ) ; } else { set double multi segments ( segments , offset , value ) ; } }	set double from segments.
public static char get char ( memory segment [ ] segments , int offset ) { if ( in first segment ( segments , offset , num ) ) { return segments [ num ] . get char ( offset ) ; } else { return get char multi segments ( segments , offset ) ; } }	get char from segments.
public static void set char ( memory segment [ ] segments , int offset , char value ) { if ( in first segment ( segments , offset , num ) ) { segments [ num ] . put char ( offset , value ) ; } else { set char multi segments ( segments , offset , value ) ; } }	set char from segments.
public static int find ( memory segment [ ] segments1 , int offset1 , int num bytes1 , memory segment [ ] segments2 , int offset2 , int num bytes2 ) { if ( num bytes2 == num ) {	Find equal segments2 in segments1.
public boolean exists ( final path f ) throws io { try { return ( get file status ( f ) != null ) ; } catch ( file not found exception e ) { return bool ; } }	Check if exists.
private static file system factory load hadoop fs factory ( ) { final class loader cl = file system . class . get class loader ( ) ;	Utility loader for the Hadoop file system factory.We treat the Hadoop FS factory in a special way, because we use it as a catchall for file systems schemes not supported directly in Flink.
@ suppress warnings ( str ) public void start registration ( ) { if ( canceled ) {	This method resolves the target address to a callable gateway and starts theregistration after that.
@ suppress warnings ( str ) private void register ( final g gateway , final int attempt , final long timeout millis ) {	This method performs a registration attempt and triggers either a success notification or a retry,depending on the result.
@ override public void exception caught ( channel handler context ctx , throwable cause ) throws exception { if ( cause instanceof transport exception ) { notify all channels of error and close ( cause ) ; } else { final socket address remote addr = ctx . channel ( ) . remote address ( ) ; final transport exception tex ;	Called on exceptions in the client handler pipeline.
public static type extract type from lambda ( class < ? > base class , lambda executable exec , int [ ] lambda type argument indices , int param len , int base parameters len ) { type output = exec . get parameter types ( ) [ param len - base parameters len + lambda type argument indices [ num ] ] ; for ( int i = num ; i < lambda type argument indices . length ; i ++ ) { validate lambda type ( base class , output ) ; output = extract type argument ( output , lambda type argument indices [ i ] ) ; } validate lambda type ( base class , output ) ; return output ; }	Extracts type from given index from lambda.
public static type extract type argument ( type t , int index ) throws invalid types exception { if ( t instanceof parameterized type ) { type [ ] actual type arguments = ( ( parameterized type ) t ) . get actual type arguments ( ) ; if ( index < num || index >= actual type arguments . length ) { throw new invalid types exception ( str + index + str + actual type arguments . length + str ) ; } else { return actual type arguments [ index ] ; } } else { throw new invalid types exception ( str + t + str ) ; } }	This method extracts the n-th type argument from the given type.
public static list < method > get all declared methods ( class < ? > clazz ) { list < method > result = new array list < > ( ) ; while ( clazz != null ) { method [ ] methods = clazz . get declared methods ( ) ; collections . add all ( result , methods ) ; clazz = clazz . get superclass ( ) ; } return result ; }	Returns all declared methods of a class including methods of superclasses.
public static class < ? > type to class ( type t ) { if ( t instanceof class ) { return ( class < ? > ) t ; } else if ( t instanceof parameterized type ) { return ( ( class < ? > ) ( ( parameterized type ) t ) . get raw type ( ) ) ; } throw new illegal argument exception ( str ) ; }	Convert ParameterizedType or Class to a Class.
public static boolean same type vars ( type t1 , type t2 ) { return t1 instanceof type variable && t2 instanceof type variable && ( ( type variable < ? > ) t1 ) . get name ( ) . equals ( ( ( type variable < ? > ) t2 ) . get name ( ) ) && ( ( type variable < ? > ) t1 ) . get generic declaration ( ) . equals ( ( ( type variable < ? > ) t2 ) . get generic declaration ( ) ) ; }	Checks whether two types are type variables describing the same.
public static type get type hierarchy ( list < type > type hierarchy , type t , class < ? > stop at class ) { while ( ! ( is class type ( t ) && type to class ( t ) . equals ( stop at class ) ) ) { type hierarchy . add ( t ) ; t = type to class ( t ) . get generic superclass ( ) ; if ( t == null ) { break ; } } return t ; }	Traverses the type hierarchy of a type up until a certain stop class is found.
public static boolean has superclass ( class < ? > clazz , string super class name ) { list < type > hierarchy = new array list < > ( ) ; get type hierarchy ( hierarchy , clazz , object . class ) ; for ( type t : hierarchy ) { if ( is class type ( t ) && type to class ( t ) . get name ( ) . equals ( super class name ) ) { return bool ; } } return bool ; }	Returns true if the given class has a superclass of given name.
public static class < ? > get raw class ( type t ) { if ( is class type ( t ) ) { return type to class ( t ) ; } else if ( t instanceof generic array type ) { type component = ( ( generic array type ) t ) . get generic component type ( ) ; return array . new instance ( get raw class ( component ) , num ) . get class ( ) ; } return object . class ; }	Returns the raw class of both parameterized types and generic arrays.Returns java.lang.Object for all other types.
public static void validate lambda type ( class < ? > base class , type t ) { if ( ! ( t instanceof class ) ) { return ; } final class < ? > clazz = ( class < ? > ) t ; if ( clazz . get type parameters ( ) . length > num ) { throw new invalid types exception ( str + clazz . get simple name ( ) + str + str + str + base class . get name ( ) + str + str ) ; } }	Checks whether the given type has the generic parameters declared in the class definition.
public void clear ( ) { stream nodes = new hash map < > ( ) ; virtual select nodes = new hash map < > ( ) ; virtual side output nodes = new hash map < > ( ) ; virtual partition nodes = new hash map < > ( ) ; vertex i id = new hash map < > ( ) ; vertex i = new hash map < > ( ) ; iteration source sink pairs = new hash set < > ( ) ; sources = new hash set < > ( ) ; sinks = new hash set < > ( ) ; }	Remove all registered nodes etc.
public void add virtual select node ( integer original id , integer virtual id , list < string > selected names ) { if ( virtual select nodes . contains key ( virtual id ) ) { throw new illegal state exception ( str + virtual id ) ; } virtual select nodes . put ( virtual id , new tuple2 < integer , list < string > > ( original id , selected names ) ) ; }	Adds a new virtual node that is used to connect a downstream vertex to only the outputswith the selected names.
public void add virtual partition node ( integer original id , integer virtual id , stream partitioner < ? > partitioner ) { if ( virtual partition nodes . contains key ( virtual id ) ) { throw new illegal state exception ( str + virtual id ) ; } virtual partition nodes . put ( virtual id , new tuple2 < integer , stream partitioner < ? > > ( original id , partitioner ) ) ; }	Adds a new virtual node that is used to connect a downstream vertex to an input with acertain partitioning.
public string get slot sharing group ( integer id ) { if ( virtual side output nodes . contains key ( id ) ) { integer mapped id = virtual side output nodes . get ( id ) . f0 ; return get slot sharing group ( mapped id ) ; } else if ( virtual select nodes . contains key ( id ) ) { integer mapped id = virtual select nodes . get ( id ) . f0 ; return get slot sharing group ( mapped id ) ; } else if ( virtual partition nodes . contains key ( id ) ) { integer mapped id = virtual partition nodes . get ( id ) . f0 ; return get slot sharing group ( mapped id ) ; } else { stream node node = get stream node ( id ) ; return node . get slot sharing group ( ) ; } }	Determines the slot sharing group of an operation across virtual nodes.
public boolean is checkpointing enabled ( ) { if ( snapshot settings == null ) { return bool ; } long checkpoint interval = snapshot settings . get checkpoint coordinator configuration ( ) . get checkpoint interval ( ) ; return checkpoint interval > num && checkpoint interval < long . max value ; }	Checks if the checkpointing was enabled for this job graph.
public int get maximum parallelism ( ) { int max parallelism = - num ; for ( job vertex vertex : task vertices . values ( ) ) { max parallelism = math . max ( vertex . get parallelism ( ) , max parallelism ) ; } return max parallelism ; }	Gets the maximum parallelism of all operations in this job graph.
public void add jar ( path jar ) { if ( jar == null ) { throw new illegal argument exception ( ) ; } if ( ! user jars . contains ( jar ) ) { user jars . add ( jar ) ; } }	Adds the path of a JAR file required to run the job on a task manager.
public void add user artifact ( string name , distributed cache . distributed cache entry file ) { if ( file == null ) { throw new illegal argument exception ( ) ; } user artifacts . put if absent ( name , file ) ; }	Adds the path of a custom file required to run the job on a task manager.
public void add user jar blob key ( permanent blob key key ) { if ( key == null ) { throw new illegal argument exception ( ) ; } if ( ! user jar blob keys . contains ( key ) ) { user jar blob keys . add ( key ) ; } }	Adds the BLOB referenced by the key to the JobGraph's dependencies.
public static < c > program target descriptor of ( c cluster id , id job id , string web interface url ) { string cluster id string ; try {	Creates a program target description from deployment classes.
public key group range offsets get intersection ( key group range key group range ) { preconditions . check not null ( key group range ) ; key group range intersection = this . key group range . get intersection ( key group range ) ; long [ ] sub offsets = new long [ intersection . get number of key groups ( ) ] ; if ( sub offsets . length > num ) { system . arraycopy ( offsets , compute key group index ( intersection . get start key group ( ) ) , sub offsets , num , sub offsets . length ) ; } return new key group range offsets ( intersection , sub offsets ) ; }	Returns a key-group range with offsets which is the intersection of the internal key-group range with the givenkey-group range.
public static decimal from big decimal ( big decimal bd , int precision , int scale ) { bd = bd . set scale ( scale , rounding mode . half up ) ; if ( bd . precision ( ) > precision ) { return null ; } long long val = - num ; if ( precision <= max compact precision ) { long val = bd . move point right ( scale ) . long value exact ( ) ; } return new decimal ( precision , scale , long val , bd ) ; }	then `precision` is checked. if precision overflow, it will return `null`.
public decimal floor ( ) { big decimal bd = to big decimal ( ) . set scale ( num , rounding mode . floor ) ; return from big decimal ( bd , bd . precision ( ) , num ) ; }	note that result may exceed the original precision.
public static long cast to integral ( decimal dec ) { big decimal bd = dec . to big decimal ( ) ;	to cast to floats, overflow will not happen, because precision<=38.
@ public evolving public < acc , r > single output stream operator < r > aggregate ( aggregate function < t , acc , r > function , type information < acc > accumulator type , type information < r > result type ) { check not null ( function , str ) ; check not null ( accumulator type , str ) ; check not null ( result type , str ) ; if ( function instanceof rich function ) { throw new unsupported operation exception ( str ) ; } return aggregate ( function , new pass through window function < k , w , r > ( ) , accumulator type , result type ) ; }	Applies the given aggregation function to each window.
public void hand in ( string key , v obj ) { if ( ! retrieve shared queue ( key ) . offer ( obj ) ) { throw new runtime exception ( str ) ; } }	Hand in the object to share.
private final application status get yarn status ( application status status ) { if ( status == null ) { return final application status . undefined ; } else { switch ( status ) { case succeeded : return final application status . succeeded ; case failed : return final application status . failed ; case canceled : return final application status . killed ; default : return final application status . undefined ; } } }	Converts a Flink application status enum to a YARN application status enum.
protected void write leader information ( uuid leader session id ) {	Writes the current leader's address as well the given leader session ID to ZooKeeper.
public static mesos configuration create mesos scheduler configuration ( configuration flink config , string hostname ) { protos . framework info . builder framework info = protos . framework info . new builder ( ) . set hostname ( hostname ) ; protos . credential . builder credential = null ; if ( ! flink config . contains ( mesos options . master url ) ) { throw new illegal configuration exception ( mesos options . master url . key ( ) + str ) ; } string master url = flink config . get string ( mesos options . master url ) ; duration failover timeout = finite duration . apply ( flink config . get integer ( mesos options . failover timeout seconds ) , time unit . seconds ) ; framework info . set failover timeout ( failover timeout . to seconds ( ) ) ; framework info . set name ( flink config . get string ( mesos options . resourcemanager framework name ) ) ; framework info . set role ( flink config . get string ( mesos options . resourcemanager framework role ) ) ; framework info . set user ( flink config . get string ( mesos options . resourcemanager framework user ) ) ; if ( flink config . contains ( mesos options . resourcemanager framework principal ) ) { framework info . set principal ( flink config . get string ( mesos options . resourcemanager framework principal ) ) ; credential = protos . credential . new builder ( ) ; credential . set principal ( framework info . get principal ( ) ) ;	Loads and validates the Mesos scheduler configuration.
public static void apply overlays ( configuration configuration , container specification container spec ) throws io {	Generate a container specification as a TaskManager template.
public static configuration load configuration ( configuration dynamic properties , logger log ) { configuration configuration = global configuration . load configuration with dynamic properties ( dynamic properties ) ;	Loads the global configuration, adds the given dynamic properties configuration, and setsthe temp directory paths.
public static configuration load configuration ( final string config dir , @ nullable final configuration dynamic properties ) { if ( config dir == null ) { throw new illegal argument exception ( str ) ; } final file conf dir file = new file ( config dir ) ; if ( ! ( conf dir file . exists ( ) ) ) { throw new illegal configuration exception ( str + config dir + str + conf dir file . get absolute path ( ) + str ) ; }	Loads the configuration files from the specified directory.
public static configuration load configuration with dynamic properties ( configuration dynamic properties ) { final string config dir = system . getenv ( config constants . env flink conf dir ) ; if ( config dir == null ) { return new configuration ( dynamic properties ) ; } return load configuration ( config dir , dynamic properties ) ; }	Loads the global configuration and adds the given dynamic propertiesconfiguration.
public static boolean is sensitive ( string key ) { preconditions . check not null ( key , str ) ; final string key in lower = key . to lower case ( ) ; for ( string hide key : sensitive keys ) { if ( key in lower . length ( ) >= hide key . length ( ) && key in lower . contains ( hide key ) ) { return bool ; } } return bool ; }	Check whether the key is a hidden key.
public boolean close ( ) { lock . lock ( ) ; try { if ( open ) { if ( elements . is empty ( ) ) { open = bool ; non empty . signal all ( ) ; return bool ; } else { return bool ; } } else {	Tries to close the queue.
public boolean add if open ( e element ) { require non null ( element ) ; lock . lock ( ) ; try { if ( open ) { elements . add last ( element ) ; if ( elements . size ( ) == num ) { non empty . signal all ( ) ; } } return open ; } finally { lock . unlock ( ) ; } }	Tries to add an element to the queue, if the queue is still open.
public void add ( e element ) throws illegal state exception { require non null ( element ) ; lock . lock ( ) ; try { if ( open ) { elements . add last ( element ) ; if ( elements . size ( ) == num ) { non empty . signal all ( ) ; } } else { throw new illegal state exception ( str ) ; } } finally { lock . unlock ( ) ; } }	Adds the element to the queue, or fails with an exception, if the queue is closed.Checking whether the queue is open and adding the element is one atomic operation.
public e peek ( ) { lock . lock ( ) ; try { if ( open ) { if ( elements . size ( ) > num ) { return elements . get first ( ) ; } else { return null ; } } else { throw new illegal state exception ( str ) ; } } finally { lock . unlock ( ) ; } }	Returns the queue's next element without removing it, if the queue is non-empty.Otherwise, returns null.
public e poll ( ) { lock . lock ( ) ; try { if ( open ) { if ( elements . size ( ) > num ) { return elements . remove first ( ) ; } else { return null ; } } else { throw new illegal state exception ( str ) ; } } finally { lock . unlock ( ) ; } }	Returns the queue's next element and removes it, the queue is non-empty.Otherwise, this method returns null.
public list < e > poll batch ( ) { lock . lock ( ) ; try { if ( open ) { if ( elements . size ( ) > num ) { array list < e > result = new array list < > ( elements ) ; elements . clear ( ) ; return result ; } else { return null ; } } else { throw new illegal state exception ( str ) ; } } finally { lock . unlock ( ) ; } }	Returns all of the queue's current elements in a list, if the queue is non-empty.Otherwise, this method returns null.
public e get element blocking ( ) throws interrupted exception { lock . lock ( ) ; try { while ( open && elements . is empty ( ) ) { non empty . await ( ) ; } if ( open ) { return elements . remove first ( ) ; } else { throw new illegal state exception ( str ) ; } } finally { lock . unlock ( ) ; } }	Returns the next element in the queue.
private < t > void add entry ( stream element queue entry < t > stream element queue entry ) { assert ( lock . is held by current thread ( ) ) ; if ( stream element queue entry . is watermark ( ) ) { last set = new hash set < > ( capacity ) ; if ( first set . is empty ( ) ) { first set . add ( stream element queue entry ) ; } else { set < stream element queue entry < ? > > watermark set = new hash set < > ( num ) ; watermark set . add ( stream element queue entry ) ; uncompleted queue . offer ( watermark set ) ; } uncompleted queue . offer ( last set ) ; } else { last set . add ( stream element queue entry ) ; } stream element queue entry . on complete ( ( stream element queue entry < t > value ) -> { try { on complete handler ( value ) ; } catch ( interrupted exception e ) {	Add the given stream element queue entry to the current last set if it is not a watermark.If it is a watermark, then stop adding to the current last set, insert the watermark into itsown set and add a new last set.
private stream graph generate internal ( list < stream transformation < ? > > transformations ) { for ( stream transformation < ? > transformation : transformations ) { transform ( transformation ) ; } return stream graph ; }	This starts the actual transformation, beginning from the sinks.
private string determine slot sharing group ( string specified group , collection < integer > input ids ) { if ( specified group != null ) { return specified group ; } else { string input group = null ; for ( int id : input ids ) { string input group candidate = stream graph . get slot sharing group ( id ) ; if ( input group == null ) { input group = input group candidate ; } else if ( ! input group . equals ( input group candidate ) ) { return str ; } } return input group == null ? str : input group ; } }	Determines the slot sharing group for an operation based on the slot sharing group set bythe user and the slot sharing groups of the inputs.
public static void install as shutdown hook ( logger logger , long delay millis ) { check argument ( delay millis >= num , str ) ;	Installs the safeguard shutdown hook.
public void start ( job leader id actions initial job leader id actions ) throws exception { if ( is started ( ) ) { clear ( ) ; } this . job leader id actions = preconditions . check not null ( initial job leader id actions ) ; }	Start the service with the given job leader actions.
public void clear ( ) throws exception { exception exception = null ; for ( job leader id listener listener : job leader id listeners . values ( ) ) { try { listener . stop ( ) ; } catch ( exception e ) { exception = exception utils . first or suppressed ( e , exception ) ; } } if ( exception != null ) { exception utils . rethrow exception ( exception , str + job leader id service . class . get simple name ( ) + str ) ; } job leader id listeners . clear ( ) ; }	Stop and clear the currently registered job leader id listeners.
public void add job ( id job id ) throws exception { preconditions . check not null ( job leader id actions ) ; log . debug ( str , job id ) ; if ( ! job leader id listeners . contains key ( job id ) ) { leader retrieval service leader retrieval service = high availability services . get job manager leader retriever ( job id ) ; job leader id listener job id listener = new job leader id listener ( job id , job leader id actions , leader retrieval service ) ; job leader id listeners . put ( job id , job id listener ) ; } }	Add a job to be monitored to retrieve the job leader id.
public void remove job ( id job id ) throws exception { log . debug ( str , job id ) ; job leader id listener listener = job leader id listeners . remove ( job id ) ; if ( listener != null ) { listener . stop ( ) ; } }	Remove the given job from being monitored by the service.
public static configuration generate task manager configuration ( configuration base config , string job manager hostname , int job manager port , int num slots , finite duration registration timeout ) { configuration cfg = clone configuration ( base config ) ; if ( job manager hostname != null && ! job manager hostname . is empty ( ) ) { cfg . set string ( job manager options . address , job manager hostname ) ; } if ( job manager port > num ) { cfg . set integer ( job manager options . port , job manager port ) ; } cfg . set string ( task manager options . registration timeout , registration timeout . to string ( ) ) ; if ( num slots != - num ) { cfg . set integer ( task manager options . num task slots , num slots ) ; } return cfg ; }	Generate a task manager configuration.
public static void write configuration ( configuration cfg , file file ) throws io { try ( file writer fwrt = new file writer ( file ) ; print writer out = new print writer ( fwrt ) ) { for ( string key : cfg . key set ( ) ) { string value = cfg . get string ( key , null ) ; out . print ( key ) ; out . print ( str ) ; out . println ( value ) ; } } }	Writes a Flink YAML config file from a Flink Configuration object.
public static void substitute deprecated config key ( configuration config , string deprecated , string designated ) {	Sets the value of a new config key to the value of a deprecated config key.
public static void substitute deprecated config prefix ( configuration config , string deprecated prefix , string designated prefix ) {	Sets the value of a new config key to the value of a deprecated config key.
public static string get task manager shell command ( configuration flink config , containered task manager parameters tm params , string config directory , string log directory , boolean has logback , boolean has log4j , boolean has krb5 , class < ? > main class ) { final map < string , string > start command values = new hash map < > ( ) ; start command values . put ( str , str ) ; array list < string > params = new array list < > ( ) ; params . add ( string . format ( str , tm params . task manager heap size mb ( ) ) ) ; params . add ( string . format ( str , tm params . task manager heap size mb ( ) ) ) ; if ( tm params . task manager direct memory limit mb ( ) >= num ) { params . add ( string . format ( str , tm params . task manager direct memory limit mb ( ) ) ) ; } start command values . put ( str , string utils . join ( params , str ) ) ; string java opts = flink config . get string ( core options . flink jvm options ) ; if ( flink config . get string ( core options . flink tm jvm options ) . length ( ) > num ) { java opts += str + flink config . get string ( core options . flink tm jvm options ) ; }	Generates the shell command to start a task manager.
public static configuration clone configuration ( configuration configuration ) { final configuration cloned configuration = new configuration ( configuration ) ; if ( cloned configuration . get boolean ( use local default tmp dirs ) ) { cloned configuration . remove config ( core options . tmp dirs ) ; cloned configuration . remove config ( use local default tmp dirs ) ; } return cloned configuration ; }	Clones the given configuration and resets instance specific config options.
public boolean read buffer from file channel ( buffer buffer ) throws io { check argument ( file channel . size ( ) - file channel . position ( ) > num ) ;	Reads data from the object's file channel into the given buffer.
@ override public void collect ( final key key , final value val ) throws io { this . out tuple . f0 = key ; this . out tuple . f1 = val ; this . flink collector . collect ( out tuple ) ; }	Use the wrapped Flink collector to collect a key-value pair for Flink.
public boolean is matching topic ( string topic ) { if ( is fixed topics ( ) ) { return get fixed topics ( ) . contains ( topic ) ; } else { return topic pattern . matcher ( topic ) . matches ( ) ; } }	Check if the input topic matches the topics described by this KafkaTopicDescriptor.
public static offset commit mode from configuration ( boolean enable auto commit , boolean enable commit on checkpoint , boolean enable checkpointing ) { if ( enable checkpointing ) {	Determine the offset commit mode using several configuration values.
void assign exclusive segments ( list < memory segment > segments ) { check state ( this . initial credit == num , str + str ) ; check not null ( segments ) ; check argument ( segments . size ( ) > num , str ) ; this . initial credit = segments . size ( ) ; this . num required buffers = segments . size ( ) ; synchronized ( buffer queue ) { for ( memory segment segment : segments ) { buffer queue . add exclusive buffer ( new network buffer ( segment , this ) , num required buffers ) ; } } }	Assigns exclusive buffers to this input channel, and this method should be called only onceafter this input channel is created.
@ visible for testing @ override public void request subpartition ( int subpartition index ) throws io , interrupted exception { if ( partition request client == null ) {	Requests a remote subpartition.
void retrigger subpartition request ( int subpartition index ) throws io , interrupted exception { check state ( partition request client != null , str ) ; if ( increase backoff ( ) ) { partition request client . request subpartition ( partition id , subpartition index , this , get current backoff ( ) ) ; } else { fail partition request ( ) ; } }	Retriggers a remote subpartition request.
@ override public void recycle ( memory segment segment ) { int num added buffers ; synchronized ( buffer queue ) {	Exclusive buffer is recycled to this input channel directly and it may trigger return extrafloating buffer and notify increased credit to the producer.
void on sender backlog ( int backlog ) throws io { int num requested buffers = num ; synchronized ( buffer queue ) {	Receives the backlog from the producer's buffer response.
public void register ongoing operation ( final k operation key , final completable future < r > operation result future ) { final result access tracker < r > in progress = result access tracker . in progress ( ) ; registered operation triggers . put ( operation key , in progress ) ; operation result future . when complete ( ( result , error ) -> { if ( error == null ) { completed operations . put ( operation key , in progress . finish operation ( either . right ( result ) ) ) ; } else { completed operations . put ( operation key , in progress . finish operation ( either . left ( error ) ) ) ; } registered operation triggers . remove ( operation key ) ; } ) ; }	Registers an ongoing operation with the cache.
@ nullable public inet socket address get server address ( ) { synchronized ( lock ) { preconditions . check state ( state != state . created , str ) ; channel server = this . server channel ; if ( server != null ) { try { return ( ( inet socket address ) server . local address ( ) ) ; } catch ( exception e ) { log . error ( str , e ) ; } } return null ; } }	Returns the address on which this endpoint is accepting requests.
@ visible for testing static void create upload dir ( final path upload dir , final logger log , final boolean initial creation ) throws io { if ( ! files . exists ( upload dir ) ) { if ( initial creation ) { log . info ( str + upload dir ) ; } else { log . warn ( str + str , upload dir ) ; } check and create upload dir ( upload dir , log ) ; } }	Creates the upload dir if needed.
public match iterator get ( long key , int hash code ) { int bucket = hash code & num buckets mask ; int bucket offset = bucket << num ; memory segment segment = buckets [ bucket offset > > > segment size bits ] ; int seg offset = bucket offset & segment size mask ; while ( bool ) { long address = segment . get long ( seg offset + num ) ; if ( address != invalid address ) { if ( segment . get long ( seg offset ) == key ) { return value iter ( address ) ; } else { bucket = ( bucket + num ) & num buckets mask ; if ( seg offset + num < segment size ) { seg offset += num ; } else { bucket offset = bucket << num ; seg offset = bucket offset & segment size mask ; segment = buckets [ bucket offset > > > segment size bits ] ; } } } else { return value iter ( invalid address ) ; } } }	Returns an iterator for all the values for the given key, or null if no value found.
private void update index ( long key , int hash code , long address , int size , memory segment data segment , int current position in segment ) throws io { assert ( num keys <= num buckets / num ) ; int bucket id = hash code & num buckets mask ;	Update the address in array for given key.
public void register listener ( id job id , kv state registry listener listener ) { final kv state registry listener previous value = listeners . put if absent ( job id , listener ) ; if ( previous value != null ) { throw new illegal state exception ( str + job id + str ) ; } }	Registers a listener with the registry.
public id register kv state ( id job id , id job vertex id , key group range key group range , string registration name , internal kv state < ? , ? , ? > kv state ) { id kv state id = new id ( ) ; if ( registered kv states . put if absent ( kv state id , new kv state entry < > ( kv state ) ) == null ) { final kv state registry listener listener = get kv state registry listener ( job id ) ; if ( listener != null ) { listener . notify kv state registered ( job id , job vertex id , key group range , registration name , kv state id ) ; } return kv state id ; } else { throw new illegal state exception ( str + registration name + str + kv state id + str ) ; } }	Registers the KvState instance and returns the assigned ID.
public void unregister kv state ( id job id , id job vertex id , key group range key group range , string registration name , id kv state id ) { kv state entry < ? , ? , ? > entry = registered kv states . remove ( kv state id ) ; if ( entry != null ) { entry . clear ( ) ; final kv state registry listener listener = get kv state registry listener ( job id ) ; if ( listener != null ) { listener . notify kv state unregistered ( job id , job vertex id , key group range , registration name ) ; } } }	Unregisters the KvState instance identified by the given KvStateID.
public binary merge iterator < entry > get merging iterator ( list < channel with meta > channel i , list < io > open channels ) throws io {	Returns an iterator that iterates over the merged result from all given channels.
public list < channel with meta > merge channel list ( list < channel with meta > channel i ) throws io {	Merges the given sorted runs to a smaller number of sorted runs.
private channel with meta merge channels ( list < channel with meta > channel i ) throws io {	Merges the sorted runs described by the given Channel IDs into a single sorted run.
public boolean is met by ( local properties other ) { if ( this . ordering != null ) {	Checks, if this set of properties, as interesting properties, is met by the givenproperties.
public void parameterize channel ( channel channel ) { local properties current = channel . get local properties ( ) ; if ( is met by ( current ) ) {	Parametrizes the local strategy fields of a channel such that the channel produces the desired local properties.
public boolean get ( ) { switch ( state ) { case unset : return value if unset ; case false : return bool ; case true : return bool ; case conflicting : return value if conflicting ; default : throw new runtime exception ( str ) ; } }	Get the boolean state.
public boolean conflicts with ( optional boolean other ) { return state == state . conflicting || other . state == state . conflicting || ( state == state . true && other . state == state . false ) || ( state == state . false && other . state == state . true ) ; }	The conflicting states are true with false and false with true.
public void merge with ( optional boolean other ) { if ( state == other . state ) {	State transitions.- if the states are the same then no change- if either state is unset then change to the other state- if the states are conflicting then set to the conflicting state.
@ public evolving @ suppress warnings ( str ) public restart strategies . restart strategy configuration get restart strategy ( ) { if ( restart strategy configuration instanceof restart strategies . fallback restart strategy configuration ) {	Returns the restart strategy which has been set for the current job.
@ suppress warnings ( str ) public void register type with kryo serializer ( class < ? > type , class < ? extends serializer > serializer class ) { if ( type == null || serializer class == null ) { throw new null pointer exception ( str ) ; } @ suppress warnings ( str ) class < ? extends serializer < ? > > casted serializer class = ( class < ? extends serializer < ? > > ) serializer class ; registered types with kryo serializer classes . put ( type , casted serializer class ) ; }	Registers the given Serializer via its class as a serializer for the given type at the KryoSerializer.
public linked hash set < class < ? > > get registered kryo types ( ) { if ( is force kryo enabled ( ) ) {	Returns the registered Kryo types.
public completable future < acknowledge > start ( final job master id new job master id ) throws exception {	Start the rpc service and begin to run the job.
@ override public completable future < void > on stop ( ) { log . info ( str , job graph . get name ( ) , job graph . get job id ( ) ) ;	Suspend the job and shutdown all other services including rpc.
@ override public completable future < acknowledge > update task execution state ( final task execution state task execution state ) { check not null ( task execution state , str ) ; if ( execution graph . update state ( task execution state ) ) { return completable future . completed future ( acknowledge . get ( ) ) ; } else { return future utils . completed exceptionally ( new execution graph exception ( str + task execution state . get id ( ) + str ) ) ; } }	Updates the task execution state for a given task.
public option default value ( string default value ) throws required parameters exception { if ( this . choices . is empty ( ) ) { return this . set default value ( default value ) ; } else { if ( this . choices . contains ( default value ) ) { return this . set default value ( default value ) ; } else { throw new required parameters exception ( str + default value + str + this . long name ) ; } } }	Define a default value for the option.
public option choices ( string ... choices ) throws required parameters exception { if ( this . default value != null ) { if ( arrays . as list ( choices ) . contains ( default value ) ) { collections . add all ( this . choices , choices ) ; } else { throw new required parameters exception ( str + this . long name + str + default value ) ; } } else { collections . add all ( this . choices , choices ) ; } return this ; }	Restrict the list of possible values of the parameter.
public static int get int ( properties config , string key , int default value ) { string val = config . get property ( key ) ; if ( val == null ) { return default value ; } else { try { return integer . parse int ( val ) ; } catch ( number format exception nfe ) { throw new illegal argument exception ( str + key + str + str + val + str + default value + str ) ; } } }	Get integer from properties.This method throws an exception if the integer is not valid.
public static long get long ( properties config , string key , long default value ) { string val = config . get property ( key ) ; if ( val == null ) { return default value ; } else { try { return long . parse long ( val ) ; } catch ( number format exception nfe ) { throw new illegal argument exception ( str + key + str + str + val + str + default value + str ) ; } } }	Get long from properties.This method throws an exception if the long is not valid.
public static long get long ( properties config , string key , long default value , logger logger ) { try { return get long ( config , key , default value ) ; } catch ( illegal argument exception iae ) { logger . warn ( iae . get message ( ) ) ; return default value ; } }	Get long from properties.This method only logs if the long is not valid.
checkpoint stats history create snapshot ( ) { if ( read only ) { throw new unsupported operation exception ( str ) ; } list < abstract checkpoint stats > checkpoints history ; map < long , abstract checkpoint stats > checkpoints by id ; checkpoints by id = new hash map < > ( checkpoints array . length ) ; if ( max size == num ) { checkpoints history = collections . empty list ( ) ; } else { abstract checkpoint stats [ ] new checkpoints array = new abstract checkpoint stats [ checkpoints array . length ] ; system . arraycopy ( checkpoints array , next pos , new checkpoints array , num , checkpoints array . length - next pos ) ; system . arraycopy ( checkpoints array , num , new checkpoints array , checkpoints array . length - next pos , next pos ) ; checkpoints history = arrays . as list ( new checkpoints array ) ;	Creates a snapshot of the current state.
void add in progress checkpoint ( pending checkpoint stats pending ) { if ( read only ) { throw new unsupported operation exception ( str ) ; } if ( max size == num ) { return ; } check not null ( pending , str ) ;	Adds an in progress checkpoint to the checkpoint history.
boolean replace pending checkpoint by id ( abstract checkpoint stats completed or failed ) { check argument ( ! completed or failed . get status ( ) . is in progress ( ) , str ) ; if ( read only ) { throw new unsupported operation exception ( str ) ; }	Searches for the in progress checkpoint with the given ID and replacesit with the given completed or failed checkpoint.
public static message type to parquet type ( type information < ? > type information , boolean legacy mode ) { return ( message type ) convert field ( null , type information , type . repetition . optional , legacy mode ) ; }	Converts Flink Internal Type to Parquet schema.
public static int hash unsafe bytes by words ( object base , long offset , int length in bytes ) { return hash unsafe bytes by words ( base , offset , length in bytes , default seed ) ; }	Hash unsafe bytes, length must be aligned to 4 bytes.
public static int hash unsafe bytes ( object base , long offset , int length in bytes ) { return hash unsafe bytes ( base , offset , length in bytes , default seed ) ; }	Hash unsafe bytes.
public static int hash bytes by words ( memory segment segment , int offset , int length in bytes ) { return hash bytes by words ( segment , offset , length in bytes , default seed ) ; }	Hash bytes in MemorySegment, length must be aligned to 4 bytes.
public static int hash bytes ( memory segment segment , int offset , int length in bytes ) { return hash bytes ( segment , offset , length in bytes , default seed ) ; }	Hash bytes in MemorySegment.
@ nullable public serialized value < job information > get serialized job information ( ) { if ( serialized job information instanceof non offloaded ) { non offloaded < job information > job information = ( non offloaded < job information > ) serialized job information ; return job information . serialized value ; } else { throw new illegal state exception ( str ) ; } }	Return the sub task's serialized job information.
@ nullable public serialized value < task information > get serialized task information ( ) { if ( serialized task information instanceof non offloaded ) { non offloaded < task information > task information = ( non offloaded < task information > ) serialized task information ; return task information . serialized value ; } else { throw new illegal state exception ( str ) ; } }	Return the sub task's serialized task information.
@ override public void accept ( visitor < plan node > visitor ) { for ( sink plan node node : this . data sinks ) { node . accept ( visitor ) ; } }	Applies the given visitor top down to all nodes, starting at the sinks.
boolean insert to bucket ( int hash code , int pointer , boolean spilling allowed , boolean size add and check resize ) throws io { final int pos hash code = find bucket ( hash code ) ;	Insert into bucket by hashCode and pointer.
boolean append record and insert ( binary row record , int hash code ) throws io { final int pos hash code = find bucket ( hash code ) ;	Append record and insert to bucket.
private boolean find first same build row ( memory segment bucket , int search hash code , int bucket in segment offset , binary row build row to insert ) { int pos in segment = bucket in segment offset + bucket header length ; int count in bucket = bucket . get short ( bucket in segment offset + header count offset ) ; int num in bucket = num ; random access input view view = partition . get build state input view ( ) ; while ( count in bucket != num ) { while ( num in bucket < count in bucket ) { final int this code = bucket . get int ( pos in segment ) ; pos in segment += hash code len ; if ( this code == search hash code ) { final int pointer = bucket . get int ( bucket in segment offset + bucket pointer start offset + ( num in bucket * pointer len ) ) ; num in bucket ++ ; try { view . set read position ( pointer ) ; binary row row = table . binary build side serializer . map from pages ( table . reuse build row , view ) ; if ( build row to insert . equals ( row ) ) { return bool ; } } catch ( io e ) { throw new runtime exception ( str + e . get message ( ) , e ) ; } } else { num in bucket ++ ; } }	For distinct build.
void start lookup ( int hash code ) { final int pos hash code = find bucket ( hash code ) ;	Probe start lookup joined build rows.
private static boolean rep step ( buffered reader in , boolean read console input ) throws io , interrupted exception {	Read-Evaluate-Print step for the REPL.
public static list < input channel deployment descriptor > from edges ( list < execution edge > edges , boolean allow lazy deployment ) { return edges . stream ( ) . map ( edge -> from edge and validate ( allow lazy deployment , edge ) ) . collect ( collectors . to list ( ) ) ; }	Creates an input channel deployment descriptor for each partition.
long refresh and get total ( ) { long total = num ; for ( input channel channel : input gate . get input channels ( ) . values ( ) ) { if ( channel instanceof remote input channel ) { remote input channel rc = ( remote input channel ) channel ; total += rc . unsynchronized get number of queued buffers ( ) ; } } return total ; }	Iterates over all input channels and collects the total number of queued buffers in abest-effort way.
int refresh and get min ( ) { int min = integer . max value ; collection < input channel > channels = input gate . get input channels ( ) . values ( ) ; for ( input channel channel : channels ) { if ( channel instanceof remote input channel ) { remote input channel rc = ( remote input channel ) channel ; int size = rc . unsynchronized get number of queued buffers ( ) ; min = math . min ( min , size ) ; } } if ( min == integer . max value ) {	Iterates over all input channels and collects the minimum number of queued buffers in achannel in a best-effort way.
int refresh and get max ( ) { int max = num ; for ( input channel channel : input gate . get input channels ( ) . values ( ) ) { if ( channel instanceof remote input channel ) { remote input channel rc = ( remote input channel ) channel ; int size = rc . unsynchronized get number of queued buffers ( ) ; max = math . max ( max , size ) ; } } return max ; }	Iterates over all input channels and collects the maximum number of queued buffers in achannel in a best-effort way.
float refresh and get avg ( ) { long total = num ; int count = num ; for ( input channel channel : input gate . get input channels ( ) . values ( ) ) { if ( channel instanceof remote input channel ) { remote input channel rc = ( remote input channel ) channel ; int size = rc . unsynchronized get number of queued buffers ( ) ; total += size ; ++ count ; } } return count == num ? num : total / ( float ) count ; }	Iterates over all input channels and collects the average number of queued buffers in achannel in a best-effort way.
static jar file with entry class find only entry class ( iterable < file > jar files ) throws io { list < jar file with entry class > jars with entry classes = new array list < > ( ) ; for ( file jar file : jar files ) { find entry class ( jar file ) . if present ( entry class -> jars with entry classes . add ( new jar file with entry class ( jar file , entry class ) ) ) ; } int size = jars with entry classes . size ( ) ; if ( size == num ) { throw new no such element exception ( str ) ; } if ( size == num ) { return jars with entry classes . get ( num ) ; }	Returns a JAR file with its entry class as specified in the manifest.
@ visible for testing static optional < string > find entry class ( file jar file ) throws io { return find first manifest attribute ( jar file , packaged program . manifest attribute assembler class , packaged program . manifest attribute main class ) ; }	Returns the entry class as specified in the manifest of the provided JAR file. The following manifest attributes are checked in order to find the entry class: {.
private static optional < string > find first manifest attribute ( file jar file , string ... attributes ) throws io { if ( attributes . length == num ) { return optional . empty ( ) ; } try ( jar file f = new jar file ( jar file ) ) { return find first manifest attribute ( f , attributes ) ; } }	Returns the value of the first manifest attribute found in the provided JAR file.
@ override public final type serializer < t > restore serializer ( ) { if ( serializer == null ) { throw new illegal state exception ( str + str ) ; } else if ( serializer instanceof unloadable dummy type serializer ) { throwable original error = ( ( unloadable dummy type serializer < ? > ) serializer ) . get original error ( ) ; throw new illegal state exception ( str + str + get class ( ) . get name ( ) + str + str + str , original error ) ; } else { return this . serializer ; } }	Creates a serializer using this configuration, that is capable of reading datawritten by the serializer described by this configuration.
private static < v , r extends serializable > accumulator < v , r > merge single ( accumulator < ? , ? > target , accumulator < ? , ? > to merge ) { @ suppress warnings ( str ) accumulator < v , r > typed target = ( accumulator < v , r > ) target ; @ suppress warnings ( str ) accumulator < v , r > typed to merge = ( accumulator < v , r > ) to merge ; typed target . merge ( typed to merge ) ; return typed target ; }	Workaround method for type safety.
public static map < string , optional failure < object > > to result map ( map < string , accumulator < ? , ? > > accumulators ) { map < string , optional failure < object > > result map = new hash map < > ( ) ; for ( map . entry < string , accumulator < ? , ? > > entry : accumulators . entry set ( ) ) { result map . put ( entry . get key ( ) , wrap unchecked ( entry . get key ( ) , ( ) -> entry . get value ( ) . get local value ( ) ) ) ; } return result map ; }	Transform the Map with accumulators into a Map containing only theresults.
public static map < string , optional failure < object > > deserialize accumulators ( map < string , serialized value < optional failure < object > > > serialized accumulators , class loader loader ) throws io , class not found exception { if ( serialized accumulators == null || serialized accumulators . is empty ( ) ) { return collections . empty map ( ) ; } map < string , optional failure < object > > accumulators = new hash map < > ( serialized accumulators . size ( ) ) ; for ( map . entry < string , serialized value < optional failure < object > > > entry : serialized accumulators . entry set ( ) ) { optional failure < object > value = null ; if ( entry . get value ( ) != null ) { value = entry . get value ( ) . deserialize value ( loader ) ; } accumulators . put ( entry . get key ( ) , value ) ; } return accumulators ; }	Takes the serialized accumulator results and tries to deserialize them using the providedclass loader.
public void prepare and commit offsets ( map < kafka topic partition , long > internal offsets ) throws exception { for ( map . entry < kafka topic partition , long > entry : internal offsets . entry set ( ) ) { kafka topic partition tp = entry . get key ( ) ; long last processed offset = entry . get value ( ) ; if ( last processed offset != null && last processed offset >= num ) { set offset in zoo keeper ( curator client , group id , tp . get topic ( ) , tp . get partition ( ) , last processed offset + num ) ; } } }	Commits offsets for Kafka partitions to ZooKeeper.
public void publish ( task event event ) { synchronized ( listeners ) { for ( event listener < task event > listener : listeners . get ( event . get class ( ) ) ) { listener . on event ( event ) ; } } }	Publishes the task event to all subscribed event listeners.
public static void register ( final logger log ) { synchronized ( signal handler . class ) { if ( registered ) { return ; } registered = bool ; final string [ ] signals = operating system . is windows ( ) ? new string [ ] { str , str } : new string [ ] { str , str , str } ; string builder bld = new string builder ( ) ; bld . append ( str ) ; string separator = str ; for ( string signal name : signals ) { try { new handler ( signal name , log ) ; bld . append ( separator ) ; bld . append ( signal name ) ; separator = str ; } catch ( exception e ) { log . info ( str , e ) ; } } bld . append ( str ) ; log . info ( bld . to string ( ) ) ; } }	Register some signal handlers.
private static < x extends copyable value < x > > copyable value serializer < x > create copyable value serializer ( class < x > clazz ) { return new copyable value serializer < x > ( clazz ) ; }	utility method to summon the necessary bound.
private static void register factory ( type t , class < ? extends type info factory > factory ) { preconditions . check not null ( t , str ) ; preconditions . check not null ( factory , str ) ; if ( ! type info factory . class . is assignable from ( factory ) ) { throw new illegal argument exception ( str ) ; } if ( registered type info factories . contains key ( t ) ) { throw new invalid types exception ( str + t + str ) ; } registered type info factories . put ( t , factory ) ; }	Registers a type information factory globally for a certain type.
@ suppress warnings ( str ) private < i , i , out > type information < out > create type info from factory ( type t , array list < type > type hierarchy , type information < i > in1 type , type information < i > in2 type ) { final array list < type > factory hierarchy = new array list < > ( type hierarchy ) ; final type info factory < ? super out > factory = get closest factory ( factory hierarchy , t ) ; if ( factory == null ) { return null ; } final type factory defining type = factory hierarchy . get ( factory hierarchy . size ( ) - num ) ;	Creates type information using a factory if for this type or super types.
@ internal public static < out > type info factory < out > get type info factory ( type t ) { final class < ? > factory class ; if ( registered type info factories . contains key ( t ) ) { factory class = registered type info factories . get ( t ) ; } else { if ( ! is class type ( t ) || ! type to class ( t ) . is annotation present ( type info . class ) ) { return null ; } final type info type info annotation = type to class ( t ) . get annotation ( type info . class ) ; factory class = type info annotation . value ( ) ;	Returns the type information factory for a type using the factory registry or annotations.
private static < out > type info factory < ? super out > get closest factory ( array list < type > type hierarchy , type t ) { type info factory factory = null ; while ( factory == null && is class type ( t ) && ! ( type to class ( t ) . equals ( object . class ) ) ) { type hierarchy . add ( t ) ; factory = get type info factory ( t ) ; t = type to class ( t ) . get generic superclass ( ) ; if ( t == null ) { break ; } } return factory ; }	Traverses the type hierarchy up until a type information factory can be found.
public static boolean is proper class ( class < ? > clazz ) { int mods = clazz . get modifiers ( ) ; return ! ( modifier . is abstract ( mods ) || modifier . is interface ( mods ) || modifier . is native ( mods ) ) ; }	Checks, whether the class is a proper class, i.e.
private void try dense mode ( ) { if ( num spill files != num ) { return ; } long min key = long . max value ; long max key = long . min value ; long record count = num ; for ( long hash partition p : this . partitions being built ) { long partition records = p . get build side record count ( ) ; record count += partition records ; if ( partition records > num ) { if ( p . get min key ( ) < min key ) { min key = p . get min key ( ) ; } if ( p . get max key ( ) > max key ) { max key = p . get max key ( ) ; } } } if ( build spill ret buffer numbers != num ) { throw new runtime exception ( str + build spill ret buffer numbers ) ; } long range = max key - min key + num ; if ( range <= record count * num || range <= segment size / num ) {	After build end, try to use dense mode.
@ override public db restore ( ) throws io , state migration exception , db { open db ( ) ; for ( keyed state handle keyed state handle : restore state handles ) { if ( keyed state handle != null ) { if ( ! ( keyed state handle instanceof key groups state handle ) ) { throw new illegal state exception ( str + str + key groups state handle . class + str + keyed state handle . get class ( ) ) ; } this . current key groups state handle = ( key groups state handle ) keyed state handle ; restore key groups in state handle ( ) ; } } return new db ( this . db , default column family handle , native metric monitor , - num , null , null ) ; }	Restores all key-groups data that is referenced by the passed state handles.
private void restore key groups in state handle ( ) throws io , state migration exception , db { try { current state handle in stream = current key groups state handle . open input stream ( ) ; cancel stream registry . register closeable ( current state handle in stream ) ; current state handle in view = new data input view stream wrapper ( current state handle in stream ) ; restore kv ( ) ; restore kv ( ) ; } finally { if ( cancel stream registry . unregister closeable ( current state handle in stream ) ) { io . close quietly ( current state handle in stream ) ; } } }	Restore one key groups state handle.
public void run detached ( job graph job ) throws job execution exception , interrupted exception { check not null ( job , str ) ; final completable future < job submission result > submission future = submit job ( job ) ; try { submission future . get ( ) ; } catch ( execution exception e ) { throw new job execution exception ( job . get job id ( ) , exception utils . strip execution exception ( e ) ) ; } }	This method executes a job in detached mode. The method returns immediately after the jobhas been added to the.
@ override public job execution result execute job blocking ( job graph job ) throws job execution exception , interrupted exception { check not null ( job , str ) ; final completable future < job submission result > submission future = submit job ( job ) ; final completable future < job result > job result future = submission future . then compose ( ( job submission result ignored ) -> request job result ( job . get job id ( ) ) ) ; final job result job result ; try { job result = job result future . get ( ) ; } catch ( execution exception e ) { throw new job execution exception ( job . get job id ( ) , str , exception utils . strip execution exception ( e ) ) ; } try { return job result . to job execution result ( thread . current thread ( ) . get context class loader ( ) ) ; } catch ( io | class not found exception e ) { throw new job execution exception ( job . get job id ( ) , e ) ; } }	This method runs a job in blocking mode.
protected metric registry impl create metric registry ( configuration config ) { return new metric registry impl ( metric registry configuration . from configuration ( config ) , reporter setup . from configuration ( config ) ) ; }	Factory method to create the metric registry for the mini cluster.
protected rpc service create rpc service ( akka rpc service configuration akka rpc service config , boolean remote enabled , string bind address ) { final config akka config ; if ( remote enabled ) { akka config = akka utils . get akka config ( akka rpc service config . get configuration ( ) , bind address , num ) ; } else { akka config = akka utils . get akka config ( akka rpc service config . get configuration ( ) ) ; } final config effective akka config = akka utils . test dispatcher config ( ) . with fallback ( akka config ) ; final actor system actor system = akka utils . create actor system ( effective akka config ) ; return new akka rpc service ( actor system , akka rpc service config ) ; }	Factory method to instantiate the RPC service.
private boolean generate node hash ( stream node node , hash function hash function , map < integer , byte [ ] > hashes , boolean is chaining enabled , stream graph stream graph ) {	Generates a hash for the node and returns whether the operation wassuccessful.
private byte [ ] generate user specified hash ( stream node node , hasher hasher ) { hasher . put string ( node . get transformation uid ( ) , charset . for name ( str ) ) ; return hasher . hash ( ) . as bytes ( ) ; }	Generates a hash from a user-specified ID.
private byte [ ] generate deterministic hash ( stream node node , hasher hasher , map < integer , byte [ ] > hashes , boolean is chaining enabled , stream graph stream graph ) {	Generates a deterministic hash from node-local properties and input andoutput edges.
public static int murmur hash ( int code ) { code *= num ; code = integer . rotate left ( code , num ) ; code *= num ; code = integer . rotate left ( code , num ) ; code = code * num + num ; code ^= num ; code = bit mix ( code ) ; if ( code >= num ) { return code ; } else if ( code != integer . min value ) { return - code ; } else { return num ; } }	This function hashes an integer value.
public static int round up to power of two ( int x ) { x = x - num ; x |= x > > num ; x |= x > > num ; x |= x > > num ; x |= x > > num ; x |= x > > num ; return x + num ; }	Round the given number to the next power of two.
public completable future < list < stack trace element [ ] > > request stack trace sample ( final stack trace sampleable task task , @ nonnegative final int num samples , final time delay between samples , final int max stack trace depth ) { check not null ( task , str ) ; check argument ( num samples > num , str ) ; check not null ( delay between samples , str ) ; return request stack trace sample ( task , num samples , delay between samples , max stack trace depth , new array list < > ( num samples ) , new completable future < > ( ) ) ; }	Returns a future that completes with a given number of stack trace samples of a task thread.
@ override protected int require ( int required ) throws kryo exception { if ( required > capacity ) { throw new kryo exception ( str + capacity + str + str + required ) ; } position = num ; int bytes read = num ; int count ; while ( bool ) { count = fill ( buffer , bytes read , required - bytes read ) ; if ( count == - num ) { throw new kryo exception ( new eof ( str ) ) ; } bytes read += count ; if ( bytes read == required ) { break ; } } limit = required ; return required ; }	Require makes sure that at least required number of bytes are kept in the buffer.
@ override protected final void acknowledge i ( long checkpoint id , set < u > unique ids ) { log . debug ( str , checkpoint id ) ; iterator < tuple2 < long , list < session id > > > iterator = session ids per snapshot . iterator ( ) ; while ( iterator . has next ( ) ) { final tuple2 < long , list < session id > > next = iterator . next ( ) ; long id = next . f0 ; if ( id <= checkpoint id ) { acknowledge session i ( next . f1 ) ;	Acknowledges the session ids.
public final string function identifier ( ) { final string md5 = encoding utils . hex ( encoding utils . md5 ( encoding utils . encode object to string ( this ) ) ) ; return get class ( ) . get canonical name ( ) . replace ( str , str ) . concat ( str ) . concat ( md5 ) ; }	Returns a unique, serialized representation for this function.
public static id from hex string ( string hex string ) { try { return new id ( string utils . hex string to byte ( hex string ) ) ; } catch ( exception e ) { throw new illegal argument exception ( str + hex string + str + str , e ) ; } }	Parses a JobID from the given string.
protected long init rank end ( base row row ) throws exception { if ( is constant rank end ) { return rank end ; } else { long rank end value = rank end state . value ( ) ; long cur rank end = rank end fetcher . apply ( row ) ; if ( rank end value == null ) { rank end = cur rank end ; rank end state . update ( rank end ) ; return rank end ; } else { rank end = rank end value ; if ( rank end != cur rank end ) {	Initialize rank end.
protected boolean check sort key in buffer range ( base row sort key , n buffer ) { comparator < base row > comparator = buffer . get sort key comparator ( ) ; map . entry < base row , collection < base row > > worst entry = buffer . last entry ( ) ; if ( worst entry == null ) {	Checks whether the record should be put into the buffer.
public void open ( runtime context cep runtime context , configuration conf ) throws exception { for ( state < t > state : get states ( ) ) { for ( state transition < t > transition : state . get state transitions ( ) ) { iterative condition condition = transition . get condition ( ) ; function utils . set function runtime context ( condition , cep runtime context ) ; function utils . open function ( condition , conf ) ; } } }	Initialization method for the NFA.
public void close ( ) throws exception { for ( state < t > state : get states ( ) ) { for ( state transition < t > transition : state . get state transitions ( ) ) { iterative condition condition = transition . get condition ( ) ; function utils . close function ( condition ) ; } } }	Tear-down method for the NFA.
private map < string , list < event id > > extract current matches ( final shared buffer accessor < t > shared buffer accessor , final computation state computation state ) throws exception { if ( computation state . get previous buffer entry ( ) == null ) { return new hash map < > ( ) ; } list < map < string , list < event id > > > paths = shared buffer accessor . extract patterns ( computation state . get previous buffer entry ( ) , computation state . get version ( ) ) ; if ( paths . is empty ( ) ) { return new hash map < > ( ) ; }	Extracts all the sequences of events from the start to the given computation state.
@ suppress warnings ( str ) public static < t > type information < t > convert ( string json schema ) { preconditions . check not null ( json schema , str ) ; final object mapper mapper = new object mapper ( ) ; mapper . get factory ( ) . enable ( json parser . feature . allow comments ) . enable ( json parser . feature . allow unquoted field names ) . enable ( json parser . feature . allow single quotes ) ; final json node node ; try { node = mapper . read tree ( json schema ) ; } catch ( io e ) { throw new illegal argument exception ( str , e ) ; } return ( type information < t > ) convert type ( str , node , node ) ; }	Converts a JSON schema into Flink's type information.
public throwable get error ( class loader user code classloader ) { if ( this . throwable == null ) { return null ; } else { return this . throwable . deserialize error ( user code classloader ) ; } }	Gets the attached exception, which is in serialized form.
protected char [ ] get password from credential providers ( string name ) throws io { char [ ] pass = null ; try { list < credential provider > providers = credential provider factory . get providers ( this ) ; if ( providers != null ) { for ( credential provider provider : providers ) { try { credential entry entry = provider . get credential entry ( name ) ; if ( entry != null ) { pass = entry . get credential ( ) ; break ; } } catch ( io ioe ) { throw new io ( str + name + str + str + provider . get class ( ) . get name ( ) + str , ioe ) ; } } } } catch ( io ioe ) { throw new io ( str , ioe ) ; } return pass ; }	Try and resolve the provided element name as a credential provideralias.
public data set < t > close with ( data set < t > iteration result ) { return new bulk iteration result set < t > ( get execution environment ( ) , get type ( ) , this , iteration result ) ; }	Closes the iteration. This method defines the end of the iterative program part.
public byte [ ] get bytes ( ) { byte [ ] bytes = new byte [ size ] ; long to byte array ( lower part , bytes , num ) ; long to byte array ( upper part , bytes , size of long ) ; return bytes ; }	Gets the bytes underlying this ID.
public final string to hex string ( ) { if ( this . hex string == null ) { final byte [ ] ba = new byte [ size ] ; long to byte array ( this . lower part , ba , num ) ; long to byte array ( this . upper part , ba , size of long ) ; this . hex string = string utils . byte to hex string ( ba ) ; } return this . hex string ; }	Returns pure String representation of the ID in hexadecimal.
private static long byte array to long ( byte [ ] ba , int offset ) { long l = num ; for ( int i = num ; i < size of long ; ++ i ) { l |= ( ba [ offset + size of long - num - i ] & num ) << ( i << num ) ; } return l ; }	Converts the given byte array to a long.
private static void long to byte array ( long l , byte [ ] ba , int offset ) { for ( int i = num ; i < size of long ; ++ i ) { final int shift = i << num ;	Converts a long to a byte array.
public static < k , vv , ev , m > gather sum apply iteration < k , vv , ev , m > with edges ( data set < edge < k , ev > > edges , gather function < vv , ev , m > gather , sum function < vv , ev , m > sum , apply function < k , vv , m > apply , int maximum number of iterations ) { return new gather sum apply iteration < > ( gather , sum , apply , edges , maximum number of iterations ) ; }	Creates a new gather-sum-apply iteration operator for graphs.
public int put ( base row sort key , base row value ) { current top num += num ;	Appends a record into the buffer.
void remove all ( base row sort key ) { collection < base row > list = tree map . get ( sort key ) ; if ( list != null ) { current top num -= list . size ( ) ; tree map . remove ( sort key ) ; } }	Removes all record list from the buffer under the sortKey.
base row remove last ( ) { map . entry < base row , collection < base row > > last = tree map . last entry ( ) ; base row last element = null ; if ( last != null ) { collection < base row > list = last . get value ( ) ; last element = get last element ( list ) ; if ( last element != null ) { if ( list . remove ( last element ) ) { current top num -= num ; } if ( list . size ( ) == num ) { tree map . remove ( last . get key ( ) ) ; } } } return last element ; }	Removes the last record of the last Entry in the buffer.
base row get element ( int rank ) { int cur rank = num ; iterator < map . entry < base row , collection < base row > > > iter = tree map . entry set ( ) . iterator ( ) ; while ( iter . has next ( ) ) { map . entry < base row , collection < base row > > entry = iter . next ( ) ; collection < base row > list = entry . get value ( ) ; iterator < base row > list iter = list . iterator ( ) ; while ( list iter . has next ( ) ) { base row elem = list iter . next ( ) ; cur rank += num ; if ( cur rank == rank ) { return elem ; } } } return null ; }	Gets record which rank is given value.
public < k > data stream < t > partition custom ( partitioner < k > partitioner , int field ) { keys . expression keys < t > out expression keys = new keys . expression keys < > ( new int [ ] { field } , get type ( ) ) ; return partition custom ( partitioner , out expression keys ) ; }	Partitions a tuple DataStream on the specified key fields using a custom partitioner.This method takes the key position to partition on, and a partitioner that accepts the key type.
public < k > data stream < t > partition custom ( partitioner < k > partitioner , key selector < t , k > key selector ) { return set connection type ( new custom partitioner wrapper < > ( clean ( partitioner ) , clean ( key selector ) ) ) ; }	Partitions a DataStream on the key returned by the selector, using a custom partitioner.This method takes the key selector to get the key to partition on, and a partitioner thataccepts the key type.
private < k > data stream < t > partition custom ( partitioner < k > partitioner , keys < t > keys ) { key selector < t , k > key selector = key selector util . get selector for one key ( keys , partitioner , get type ( ) , get execution config ( ) ) ; return set connection type ( new custom partitioner wrapper < > ( clean ( partitioner ) , clean ( key selector ) ) ) ; }	private helper method for custom partitioning.
@ deprecated public single output stream operator < t > assign timestamps ( timestamp extractor < t > extractor ) {	Extracts a timestamp from an element and assigns it as the internal timestamp of that element.The internal timestamps are, for example, used to to event-time window operations.
@ suppress warnings ( str ) @ public evolving public < x extends tuple > data stream sink < t > write as csv ( string path , write mode write mode , string row delimiter , string field delimiter ) { preconditions . check argument ( get type ( ) . is tuple type ( ) , str ) ; csv output format < x > of = new csv output format < > ( new path ( path ) , row delimiter , field delimiter ) ; if ( write mode != null ) { of . set write mode ( write mode ) ; } return write using output format ( ( output format < t > ) of ) ; }	Writes a DataStream to the file specified by the path parameter.
@ public evolving public data stream sink < t > write using output format ( output format < t > format ) { return add sink ( new output format sink function < > ( format ) ) ; }	Writes the dataStream into an output, described by an OutputFormat.
@ public evolving public < r > single output stream operator < r > transform ( string operator name , type information < r > out type info , one input stream operator < t , r > operator ) {	Method for passing user defined operators along with the typeinformation that will transform the DataStream.
protected data stream < t > set connection type ( stream partitioner < t > partitioner ) { return new data stream < > ( this . get execution environment ( ) , new partition transformation < > ( this . get transformation ( ) , partitioner ) ) ; }	Internal function for setting the partitioner for the DataStream.
public event next ( int min ip , int max ip ) { final double p = rnd . next double ( ) ; if ( p * num >= states . size ( ) ) {	Creates a new random event.
private static long get size of physical memory for windows ( ) { buffered reader bi = null ; try { process proc = runtime . get runtime ( ) . exec ( str ) ; bi = new buffered reader ( new input stream reader ( proc . get input stream ( ) ) ) ; string line = bi . read line ( ) ; if ( line == null ) { return - num ; } if ( ! line . starts with ( str ) ) { return - num ; } long size of phyiscal memory = num ; while ( ( line = bi . read line ( ) ) != null ) { if ( line . is empty ( ) ) { continue ; } line = line . replace all ( str , str ) ; size of phyiscal memory += long . parse long ( line ) ; } return size of phyiscal memory ; } catch ( throwable t ) { log . error ( str + str , t ) ; return - num ; } finally { if ( bi != null ) { try { bi . close ( ) ; } catch ( throwable ignored ) { } } } }	Returns the size of the physical memory in bytes on Windows.
public < r > map operator < t , r > map ( map function < t , r > mapper ) { if ( mapper == null ) { throw new null pointer exception ( str ) ; } string call location = utils . get call location name ( ) ; type information < r > result type = type extractor . get map return types ( mapper , get type ( ) , call location , bool ) ; return new map operator < > ( this , result type , clean ( mapper ) , call location ) ; }	Applies a Map transformation on this DataSet. The transformation calls a {.
public list < t > collect ( ) throws exception { final string id = new id ( ) . to string ( ) ; final type serializer < t > serializer = get type ( ) . create serializer ( get execution environment ( ) . get config ( ) ) ; this . output ( new utils . collect helper < > ( id , serializer ) ) . name ( str ) ; job execution result res = get execution environment ( ) . execute ( ) ; array list < byte [ ] > acc result = res . get accumulator result ( id ) ; if ( acc result != null ) { try { return serialized list accumulator . deserialize list ( acc result , serializer ) ; } catch ( class not found exception e ) { throw new runtime exception ( str , e ) ; } catch ( io e ) { throw new runtime exception ( str , e ) ; } } else { throw new runtime exception ( str ) ; } }	Convenience method to get the elements of a DataSet as a List.As DataSet can contain a lot of data, this method should be used with caution.
public union operator < t > union ( data set < t > other ) { return new union operator < > ( this , other , utils . get call location name ( ) ) ; }	Creates a union of this DataSet with an other DataSet.
public partition operator < t > partition by hash ( int ... fields ) { return new partition operator < > ( this , partition method . hash , new keys . expression keys < > ( fields , get type ( ) ) , utils . get call location name ( ) ) ; }	Hash-partitions a DataSet on the specified key fields.
public < k extends comparable < k > > partition operator < t > partition by hash ( key selector < t , k > key extractor ) { final type information < k > key type = type extractor . get key selector types ( key extractor , get type ( ) ) ; return new partition operator < > ( this , partition method . hash , new keys . selector function keys < > ( clean ( key extractor ) , this . get type ( ) , key type ) , utils . get call location name ( ) ) ; }	Partitions a DataSet using the specified KeySelector.
public partition operator < t > partition by range ( int ... fields ) { return new partition operator < > ( this , partition method . range , new keys . expression keys < > ( fields , get type ( ) ) , utils . get call location name ( ) ) ; }	Range-partitions a DataSet on the specified key fields.
public < k > partition operator < t > partition custom ( partitioner < k > partitioner , int field ) { return new partition operator < > ( this , new keys . expression keys < > ( new int [ ] { field } , get type ( ) ) , clean ( partitioner ) , utils . get call location name ( ) ) ; }	Partitions a tuple DataSet on the specified key fields using a custom partitioner.This method takes the key position to partition on, and a partitioner that accepts the key type.
public < k extends comparable < k > > partition operator < t > partition custom ( partitioner < k > partitioner , key selector < t , k > key extractor ) { final type information < k > key type = type extractor . get key selector types ( key extractor , get type ( ) ) ; return new partition operator < > ( this , new keys . selector function keys < > ( key extractor , get type ( ) , key type ) , clean ( partitioner ) , utils . get call location name ( ) ) ; }	Partitions a DataSet on the key returned by the selector, using a custom partitioner.This method takes the key selector to get the key to partition on, and a partitioner thataccepts the key type.
public < k > sort partition operator < t > sort partition ( key selector < t , k > key extractor , order order ) { final type information < k > key type = type extractor . get key selector types ( key extractor , get type ( ) ) ; return new sort partition operator < > ( this , new keys . selector function keys < > ( clean ( key extractor ) , get type ( ) , key type ) , order , utils . get call location name ( ) ) ; }	Locally sorts the partitions of the DataSet on the extracted key in the specified order.The DataSet can be sorted on multiple values by returning a tuple from the KeySelector.
public t get default value ( ) { if ( default value != null ) { if ( serializer != null ) { return serializer . copy ( default value ) ; } else { throw new illegal state exception ( str ) ; } } else { return null ; } }	Returns the default value.
public void set queryable ( string queryable state name ) { preconditions . check argument ( ttl config . get update type ( ) == state ttl config . update type . disabled , str ) ; if ( this . queryable state name == null ) { this . queryable state name = preconditions . check not null ( queryable state name , str ) ; } else { throw new illegal state exception ( str ) ; } }	Sets the name for queries of state created from this descriptor.
public void initialize serializer unless set ( execution config execution config ) { if ( serializer == null ) { check state ( type info != null , str ) ;	Initializes the serializer, unless it has been initialized before.
@ override public row next record ( row row ) throws io { try { if ( ! has next ) { return null ; } for ( int pos = num ; pos < row . get arity ( ) ; pos ++ ) { row . set field ( pos , result set . get object ( pos + num ) ) ; }	Stores the next resultSet row in a tuple.
private catalog table validate partition spec ( object path table path , catalog partition spec partition spec ) throws table not exist exception , table not partitioned exception , partition spec invalid exception { catalog table table = validate partitioned table ( table path ) ; list < string > partition keys = table . get partition keys ( ) ; map < string , string > spec = partition spec . get partition spec ( ) ;	Validate the partitioned table and partitionSpec.
private catalog table validate partitioned table ( object path table path ) throws table not exist exception , table not partitioned exception { catalog base table base table = get table ( table path ) ; if ( ! ( base table instanceof catalog table ) ) { throw new catalog exception ( string . format ( str , table path . get full name ( ) , catalog name ) ) ; } catalog table table = ( catalog table ) base table ; if ( ! table . is partitioned ( ) ) { throw new table not partitioned exception ( catalog name , table path ) ; } return table ; }	Validate the partitioned table.
protected void increase buffers in backlog ( buffer consumer buffer ) { assert thread . holds lock ( buffers ) ; if ( buffer != null && buffer . is buffer ( ) ) { buffers in backlog ++ ; } }	Increases the number of non-event buffers by one after adding a non-eventbuffer into this subpartition.
void lookup select hints ( sql select select , sql parser pos pos , collection < sql moniker > hint list ) { id info info = id positions . get ( pos . to string ( ) ) ; if ( ( info == null ) || ( info . scope == null ) ) { sql node from node = select . get from ( ) ; final sql validator scope from scope = get from scope ( select ) ; lookup from hints ( from node , from scope , pos , hint list ) ; } else { lookup name completion hints ( info . scope , info . id . names , info . id . get parser position ( ) , hint list ) ; } }	Looks up completion hints for a syntactically correct select SQL that hasbeen parsed into an expression tree.
public final void lookup name completion hints ( sql validator scope scope , list < string > names , sql parser pos pos , collection < sql moniker > hint list ) {	Populates a list of all the valid alternatives for an identifier.
protected void validate namespace ( final sql validator namespace namespace , rel data type target row type ) { namespace . validate ( target row type ) ; if ( namespace . get node ( ) != null ) { set validated node type ( namespace . get node ( ) , namespace . get type ( ) ) ; } }	Validates a namespace.
protected sql select create source select for update ( sql update call ) { final sql node list select list = new sql node list ( sql parser pos . zero ) ; select list . add ( sql identifier . star ( sql parser pos . zero ) ) ; int ordinal = num ; for ( sql node exp : call . get source expression list ( ) ) {	Creates the SELECT statement that putatively feeds rows into an UPDATEstatement to be updated.
protected sql select create source select for delete ( sql delete call ) { final sql node list select list = new sql node list ( sql parser pos . zero ) ; select list . add ( sql identifier . star ( sql parser pos . zero ) ) ; sql node source table = call . get target table ( ) ; if ( call . get alias ( ) != null ) { source table = sql validator util . add alias ( source table , call . get alias ( ) . get simple ( ) ) ; } return new sql select ( sql parser pos . zero , null , select list , source table , call . get condition ( ) , null , null , null , null , null , null ) ; }	Creates the SELECT statement that putatively feeds rows into a DELETEstatement to be deleted.
rel data type get table constructor row type ( sql call values , sql validator scope scope ) { final list < sql node > rows = values . get operand list ( ) ; assert rows . size ( ) >= num ; final list < rel data type > row types = new array list < > ( ) ; for ( final sql node row : rows ) { assert row . get kind ( ) == sql kind . row ; sql call row constructor = ( sql call ) row ;	Returns null if there is no common type.
rel data type derive type impl ( sql validator scope scope , sql node operand ) { derive type visitor v = new derive type visitor ( scope ) ; final rel data type type = operand . accept ( v ) ; return objects . require non null ( scope . nullify type ( operand , type ) ) ; }	Derives the type of a node, never null.
protected void add to select list ( list < sql node > list , set < string > aliases , list < map . entry < string , rel data type > > field list , sql node exp , sql validator scope scope , final boolean include system vars ) { string alias = sql validator util . get alias ( exp , - num ) ; string unique alias = sql validator util . uniquify ( alias , aliases , sql validator util . expr suggester ) ; if ( ! alias . equals ( unique alias ) ) { exp = sql validator util . add alias ( exp , unique alias ) ; } field list . add ( pair . of ( unique alias , derive type ( scope , exp ) ) ) ; list . add ( exp ) ; }	Adds an expression to a select list, ensuring that its alias does notclash with any existing expressions on the list.
protected void register namespace ( sql validator scope using scope , string alias , sql validator namespace ns , boolean force nullable ) { namespaces . put ( ns . get node ( ) , ns ) ; if ( using scope != null ) { using scope . add child ( ns , alias , force nullable ) ; } }	Registers a new namespace, and adds it as a child of its parent scope.Derived class can override this method to tinker with namespaces as theyare created.
private sql node get agg ( sql select select ) { final select scope select scope = get raw select scope ( select ) ; if ( select scope != null ) { final list < sql node > select list = select scope . get expanded select list ( ) ; if ( select list != null ) { return agg finder . find agg ( select list ) ; } } return agg finder . find agg ( select . get select list ( ) ) ; }	If there is at least one call to an aggregate function, returns thefirst.
private void register operand sub queries ( sql validator scope parent scope , sql call call , int operand ordinal ) { sql node operand = call . operand ( operand ordinal ) ; if ( operand == null ) { return ; } if ( operand . get kind ( ) . belongs to ( sql kind . query ) && call . get operator ( ) . argument must be scalar ( operand ordinal ) ) { operand = sql std operator table . scalar query . create call ( operand . get parser position ( ) , operand ) ; call . set operand ( operand ordinal , operand ) ; } register sub queries ( parent scope , operand ) ; }	Registers any sub-queries inside a given call operand, and converts theoperand to a scalar sub-query if the operator requires it.
private void validate no aggs ( agg finder agg finder , sql node node , string clause ) { final sql call agg = agg finder . find agg ( node ) ; if ( agg == null ) { return ; } final sql operator op = agg . get operator ( ) ; if ( op == sql std operator table . over ) { throw new validation error ( agg , resource . windowed aggregate illegal in clause ( clause ) ) ; } else if ( op . is group ( ) || op . is group auxiliary ( ) ) { throw new validation error ( agg , resource . group function must appear in group by clause ( op . get name ( ) ) ) ; } else { throw new validation error ( agg , resource . aggregate illegal in clause ( clause ) ) ; } }	Throws an error if there is an aggregate or windowed aggregate in thegiven clause.
protected void validate select ( sql select select , rel data type target row type ) { assert target row type != null ;	Validates a SELECT statement.
private boolean is rolled up column allowed in agg ( sql identifier identifier , sql validator scope scope , sql call agg call , sql node parent ) { pair < string , string > pair = find table column pair ( identifier , scope ) ; if ( pair == null ) { return bool ; } string table alias = pair . left ; string column name = pair . right ; table table = find table ( table alias ) ; if ( table != null ) { return table . rolled up column valid inside agg ( column name , agg call , parent , catalog reader . get config ( ) ) ; } return bool ; }	Returns true iff the given column is valid inside the given aggCall.
private boolean is rolled up column ( sql identifier identifier , sql validator scope scope ) { pair < string , string > pair = find table column pair ( identifier , scope ) ; if ( pair == null ) { return bool ; } string table alias = pair . left ; string column name = pair . right ; table table = find table ( table alias ) ; if ( table != null ) { return table . is rolled up ( column name ) ; } return bool ; }	Returns true iff the given column is actually rolled up.
private void validate modality ( sql node query ) { final sql modality modality = deduce modality ( query ) ; if ( query instanceof sql select ) { final sql select select = ( sql select ) query ; validate modality ( select , modality , bool ) ; } else if ( query . get kind ( ) == sql kind . values ) { switch ( modality ) { case stream : throw new validation error ( query , static . resource . cannot stream values ( ) ) ; } } else { assert query . is a ( sql kind . set query ) ; final sql call call = ( sql call ) query ; for ( sql node operand : call . get operand list ( ) ) { if ( deduce modality ( operand ) != modality ) { throw new validation error ( operand , static . resource . stream set op inconsistent inputs ( ) ) ; } validate modality ( operand ) ; } } }	Validates that a query can deliver the modality it promises.
private sql modality deduce modality ( sql node query ) { if ( query instanceof sql select ) { sql select select = ( sql select ) query ; return select . get modifier node ( sql select keyword . stream ) != null ? sql modality . stream : sql modality . relation ; } else if ( query . get kind ( ) == sql kind . values ) { return sql modality . relation ; } else { assert query . is a ( sql kind . set query ) ; final sql call call = ( sql call ) query ; return deduce modality ( call . get operand list ( ) . get ( num ) ) ; } }	Return the intended modality of a SELECT or set-op.
private boolean has sorted prefix ( select scope scope , sql node list order list ) { return is sort compatible ( scope , order list . get ( num ) , bool ) ; }	Returns whether the prefix is sorted.
protected void validate order list ( sql select select ) {	Validates the ORDER BY clause of a SELECT statement.
private void validate group by item ( sql select select , sql node group by item ) { final sql validator scope group by scope = get group scope ( select ) ; group by scope . validate expr ( group by item ) ; }	Validates an item in the GROUP BY clause of a SELECT statement.
private void validate order item ( sql select select , sql node order item ) { switch ( order item . get kind ( ) ) { case descending : validate feature ( resource . s ql ( ) , order item . get parser position ( ) ) ; validate order item ( select , ( ( sql call ) order item ) . operand ( num ) ) ; return ; } final sql validator scope order scope = get order scope ( select ) ; validate expr ( order item , order scope ) ; }	Validates an item in the ORDER BY clause of a SELECT statement.
protected void validate group clause ( sql select select ) { sql node list group list = select . get group ( ) ; if ( group list == null ) { return ; } final string clause = str ; validate no aggs ( agg or over finder , group list , clause ) ; final sql validator scope group scope = get group scope ( select ) ; infer unknown types ( unknown type , group scope , group list ) ;	Validates the GROUP BY clause of a SELECT statement.
private void handle scalar sub query ( sql select parent select , sql select select item , list < sql node > expanded select items , set < string > alias list , list < map . entry < string , rel data type > > field list ) {	Processes SubQuery found in Select list.
protected rel data type create target row type ( sql validator table table , sql node list target column list , boolean append ) { rel data type base row type = table . get row type ( ) ; if ( target column list == null ) { return base row type ; } list < rel data type field > target fields = base row type . get field list ( ) ; final list < map . entry < string , rel data type > > fields = new array list < > ( ) ; if ( append ) { for ( rel data type field target field : target fields ) { fields . add ( pair . of ( sql util . derive alias from ordinal ( fields . size ( ) ) , target field . get type ( ) ) ) ; } } final set < integer > assigned fields = new hash set < > ( ) ; final rel opt table rel opt table = table instanceof rel opt table ? ( ( rel opt table ) table ) : null ; for ( sql node node : target column list ) { sql identifier id = ( sql identifier ) node ; rel data type field target field = sql validator util . get target field ( base row type , type factory , id , catalog reader , rel opt table ) ; if ( target field == null ) { throw new validation error ( id , resource . unknown target column ( id . to string ( ) ) ) ; } if ( ! assigned fields . add ( target field . get index ( ) ) ) { throw new validation error ( id , resource . duplicate target column ( target field . get name ( ) ) ) ; } fields . add ( target field ) ; } return type factory . create struct type ( fields ) ; }	Derives a row-type for INSERT and UPDATE operations.
private void check constraint ( sql validator table validator table , sql node source , rel data type target row type ) { final modifiable view table modifiable view table = validator table . unwrap ( modifiable view table . class ) ; if ( modifiable view table != null && source instanceof sql call ) { final table table = modifiable view table . unwrap ( table . class ) ; final rel data type table row type = table . get row type ( type factory ) ; final list < rel data type field > table fields = table row type . get field list ( ) ;	Validates insert values against the constraint of a modifiable view.
private void check constraint ( sql validator table validator table , sql update update , rel data type target row type ) { final modifiable view table modifiable view table = validator table . unwrap ( modifiable view table . class ) ; if ( modifiable view table != null ) { final table table = modifiable view table . unwrap ( table . class ) ; final rel data type table row type = table . get row type ( type factory ) ; final map < integer , rex node > project map = rel opt util . get column constraints ( modifiable view table , target row type , type factory ) ; final map < string , integer > name to index = sql validator util . map name to index ( table row type . get field list ( ) ) ;	Validates updates against the constraint of a modifiable view.
private sql node get nth expr ( sql node query , int ordinal , int source count ) { if ( query instanceof sql insert ) { sql insert insert = ( sql insert ) query ; if ( insert . get target column list ( ) != null ) { return insert . get target column list ( ) . get ( ordinal ) ; } else { return get nth expr ( insert . get source ( ) , ordinal , source count ) ; } } else if ( query instanceof sql update ) { sql update update = ( sql update ) query ; if ( update . get target column list ( ) != null ) { return update . get target column list ( ) . get ( ordinal ) ; } else if ( update . get source expression list ( ) != null ) { return update . get source expression list ( ) . get ( ordinal ) ; } else { return get nth expr ( update . get source select ( ) , ordinal , source count ) ; } } else if ( query instanceof sql select ) { sql select select = ( sql select ) query ; if ( select . get select list ( ) . size ( ) == source count ) { return select . get select list ( ) . get ( ordinal ) ; } else { return query ;	Locates the n'th expression in an INSERT or UPDATE query.
private void validate access ( sql node node , sql validator table table , sql access enum required access ) { if ( table != null ) { sql access type access = table . get allowed access ( ) ; if ( ! access . allows access ( required access ) ) { throw new validation error ( node , resource . access not allowed ( required access . name ( ) , table . get qualified name ( ) . to string ( ) ) ) ; } } }	Validates access to a table.
protected void validate values ( sql call node , rel data type target row type , final sql validator scope scope ) { assert node . get kind ( ) == sql kind . values ; final list < sql node > operands = node . get operand list ( ) ; for ( sql node operand : operands ) { if ( ! ( operand . get kind ( ) == sql kind . row ) ) { throw util . need to implement ( str ) ; } sql call row constructor = ( sql call ) operand ; if ( conformance . is insert subset columns allowed ( ) && target row type . is struct ( ) && row constructor . operand count ( ) < target row type . get field count ( ) ) { target row type = type factory . create struct type ( target row type . get field list ( ) . sub list ( num , row constructor . operand count ( ) ) ) ; } else if ( target row type . is struct ( ) && row constructor . operand count ( ) != target row type . get field count ( ) ) { return ; } infer unknown types ( target row type , scope , row constructor ) ; if ( target row type . is struct ( ) ) { for ( pair < sql node , rel data type field > pair : pair . zip ( row constructor . get operand list ( ) , target row type . get field list ( ) ) ) { if ( ! pair . right . get type ( ) . is nullable ( ) && sql util . is null literal ( pair . left , bool ) ) { throw new validation error ( node , resource . column not nullable ( pair . right . get name ( ) ) ) ; } } } } for ( sql node operand : operands ) { operand . validate ( this , scope ) ; }	Validates a VALUES clause.
private static string alias ( sql node item ) { assert item instanceof sql call ; assert item . get kind ( ) == sql kind . as ; final sql identifier identifier = ( ( sql call ) item ) . operand ( num ) ; return identifier . get simple ( ) ; }	Returns the alias of a "expr AS alias" expression.
public static output stream and path create entropy aware ( file system fs , path path , write mode write mode ) throws io {	Handles entropy injection across regular and entropy-aware file systems. If the given file system is entropy-aware (a implements {.
@ override public fs state backend configure ( configuration config , class loader class loader ) { return new fs state backend ( this , config , class loader ) ; }	Creates a copy of this state backend that uses the values defined in the configurationfor fields where that were not specified in this state backend.
public static list < resolver rule > get resolver rules ( ) { return arrays . as list ( resolver rules . lookup call by name , resolver rules . flatten star reference , resolver rules . expand column functions , resolver rules . over windows , resolver rules . field resolve , resolver rules . flatten call , resolver rules . resolve call by arguments , resolver rules . verify no more unresolved expressions ) ; }	List of rules that will be applied during expression resolution.
public logical window resolve group window ( group window window ) { expression alias = window . get alias ( ) ; if ( ! ( alias instanceof unresolved reference expression ) ) { throw new validation exception ( str ) ; } final string window name = ( ( unresolved reference expression ) alias ) . get name ( ) ; list < expression > resolved time field expression = prepare expressions ( collections . singleton list ( window . get time field ( ) ) ) ; if ( resolved time field expression . size ( ) != num ) { throw new validation exception ( str ) ; } planner expression time field = resolved time field expression . get ( num ) . accept ( bridge converter ) ;	Converts an API class to a logical window for planning with expressions already resolved.
public void start ( slot actions initial slot actions ) { this . slot actions = preconditions . check not null ( initial slot actions ) ; timer service . start ( this ) ; started = bool ; }	Start the task slot table with the given slot actions.
public boolean allocate slot ( int index , id job id , id allocation id , time slot timeout ) { check init ( ) ; task slot task slot = task slots . get ( index ) ; boolean result = task slot . allocate ( job id , allocation id ) ; if ( result ) {	Allocate the slot with the given index for the given job and allocation id.
public int free slot ( id allocation id , throwable cause ) throws slot not found exception { check init ( ) ; task slot task slot = get task slot ( allocation id ) ; if ( task slot != null ) { if ( log . is debug enabled ( ) ) { log . debug ( str , task slot , cause ) ; } else { log . info ( str , task slot ) ; } final id job id = task slot . get job id ( ) ; if ( task slot . mark free ( ) ) {	Tries to free the slot.
public boolean is valid timeout ( id allocation id , uuid ticket ) { check init ( ) ; return timer service . is valid ( allocation id , ticket ) ; }	Check whether the timeout with ticket is valid for the given allocation id.
public boolean is allocated ( int index , id job id , id allocation id ) { task slot task slot = task slots . get ( index ) ; return task slot . is allocated ( job id , allocation id ) ; }	Check whether the slot for the given index is allocated for the given job and allocation id.
public boolean try mark slot active ( id job id , id allocation id ) { task slot task slot = get task slot ( allocation id ) ; if ( task slot != null && task slot . is allocated ( job id , allocation id ) ) { return task slot . mark active ( ) ; } else { return bool ; } }	Try to mark the specified slot as active if it has been allocated by the given job.
public boolean add task ( task task ) throws slot not found exception , slot not active exception { preconditions . check not null ( task ) ; task slot task slot = get task slot ( task . get allocation id ( ) ) ; if ( task slot != null ) { if ( task slot . is active ( task . get job id ( ) , task . get allocation id ( ) ) ) { if ( task slot . add ( task ) ) { task slot mappings . put ( task . get execution id ( ) , new task slot mapping ( task , task slot ) ) ; return bool ; } else { return bool ; } } else { throw new slot not active exception ( task . get job id ( ) , task . get allocation id ( ) ) ; } } else { throw new slot not found exception ( task . get allocation id ( ) ) ; } }	Add the given task to the slot identified by the task's allocation id.
public task remove task ( id execution attempt id ) { check init ( ) ; task slot mapping task slot mapping = task slot mappings . remove ( execution attempt id ) ; if ( task slot mapping != null ) { task task = task slot mapping . get task ( ) ; task slot task slot = task slot mapping . get task slot ( ) ; task slot . remove ( task . get execution id ( ) ) ; if ( task slot . is releasing ( ) && task slot . is empty ( ) ) { slot actions . free slot ( task slot . get allocation id ( ) ) ; } return task ; } else { return null ; } }	Remove the task with the given execution attempt id from its task slot.
public task get task ( id execution attempt id ) { task slot mapping task slot mapping = task slot mappings . get ( execution attempt id ) ; if ( task slot mapping != null ) { return task slot mapping . get task ( ) ; } else { return null ; } }	Get the task for the given execution attempt id.
public kafka properties ( properties properties ) { preconditions . check not null ( properties ) ; if ( this . kafka properties == null ) { this . kafka properties = new hash map < > ( ) ; } this . kafka properties . clear ( ) ; properties . for each ( ( k , v ) -> this . kafka properties . put ( ( string ) k , ( string ) v ) ) ; return this ; }	Sets the configuration properties for the Kafka consumer.
public kafka property ( string key , string value ) { preconditions . check not null ( key ) ; preconditions . check not null ( value ) ; if ( this . kafka properties == null ) { this . kafka properties = new hash map < > ( ) ; } kafka properties . put ( key , value ) ; return this ; }	Adds a configuration properties for the Kafka consumer.
public kafka start from specific offsets ( map < integer , long > specific offsets ) { this . startup mode = startup mode . specific offsets ; this . specific offsets = preconditions . check not null ( specific offsets ) ; return this ; }	Configures to start reading partitions from specific offsets, set independently for each partition.Resets previously set offsets.
public kafka start from specific offset ( int partition , long specific offset ) { this . startup mode = startup mode . specific offsets ; if ( this . specific offsets == null ) { this . specific offsets = new hash map < > ( ) ; } this . specific offsets . put ( partition , specific offset ) ; return this ; }	Configures to start reading partitions from specific offsets and specifies the given offset forthe given partition.
public kafka sink partitioner custom ( class < ? extends flink kafka partitioner > partitioner class ) { sink partitioner type = connector sink partitioner value custom ; sink partitioner class = preconditions . check not null ( partitioner class ) ; return this ; }	Configures how to partition records from Flink's partitions into Kafka's partitions. This strategy allows for a custom partitioner by providing an implementationof {.
public static int assign key to parallel operator ( object key , int max parallelism , int parallelism ) { return compute operator index for key group ( max parallelism , parallelism , assign to key group ( key , max parallelism ) ) ; }	Assigns the given key to a parallel operator index.
public static int compute default max parallelism ( int operator parallelism ) { check parallelism preconditions ( operator parallelism ) ; return math . min ( math . max ( math utils . round up to power of two ( operator parallelism + ( operator parallelism / num ) ) , default lower bound max parallelism ) , upper bound max parallelism ) ; }	Computes a default maximum parallelism from the operator parallelism.
@ override public job execution result execute plan ( plan plan ) throws exception { if ( plan == null ) { throw new illegal argument exception ( str ) ; } synchronized ( this . lock ) {	Executes the given program on a local runtime and waits for the job to finish.
public static job execution result execute ( program pa , string ... args ) throws exception { return execute ( pa . get plan ( args ) ) ; }	Executes the given program.
public static string optimizer plan as json ( plan plan ) throws exception { final int parallelism = plan . get default parallelism ( ) == execution config . parallelism default ? num : plan . get default parallelism ( ) ; optimizer pc = new optimizer ( new data statistics ( ) , new configuration ( ) ) ; pc . set default parallelism ( parallelism ) ; optimized plan op = pc . compile ( plan ) ; return new json ( ) . get optimizer plan as json ( op ) ; }	Creates a JSON representation of the given dataflow's execution plan.
public static string get plan as json ( plan plan ) { list < data sink node > sinks = optimizer . create pre optimized plan ( plan ) ; return new json ( ) . get pact plan as json ( sinks ) ; }	Creates a JSON representation of the given dataflow plan.
public final void resolve ( x value ) { preconditions . check state ( ! resolved , str ) ; this . value = preconditions . check not null ( value ) ; this . resolved = bool ; }	Resolves this parameter for the given value.
protected shard consumer create shard consumer ( integer subscribed shard state index , stream shard handle subscribed shard , sequence number last sequence num , shard metrics reporter shard metrics reporter ) { return new shard consumer < > ( this , subscribed shard state index , subscribed shard , last sequence num , this . kinesis proxy factory . create ( config props ) , shard metrics reporter ) ; }	Create a new shard consumer.Override this method to customize shard consumer behavior in subclasses.
public hash map < stream shard metadata , sequence number > snapshot state ( ) {	Creates a snapshot of the current last processed sequence numbers of each subscribed shard.
public void advance last discovered shard of stream ( string stream , string shard id ) { string last seen shard id of stream = this . subscribed streams to last discovered shard ids . get ( stream ) ;	Updates the last discovered shard of a subscribed stream; only updates if the update is valid.
public int register new subscribed shard state ( kinesis stream shard state new subscribed shard state ) { synchronized ( checkpoint lock ) { subscribed shards state . add ( new subscribed shard state ) ;	Register a new subscribed shard state.
@ visible for testing protected void emit watermark ( ) { log . debug ( str , index of this consumer subtask , get current time millis ( ) ) ; long potential watermark = long . max value ; long idle time = ( shard idle interval millis > num ) ? get current time millis ( ) - shard idle interval millis : long . max value ; for ( map . entry < integer , shard watermark state > e : shard watermarks . entry set ( ) ) {	Called periodically to emit a watermark.
public graph analytic base < k , vv , ev , t > set parallelism ( int parallelism ) { preconditions . check argument ( parallelism > num || parallelism == parallelism default , str ) ; this . parallelism = parallelism ; return this ; }	Set the parallelism for this analytic's operators.
public static string create random name ( string prefix ) { preconditions . check not null ( prefix , str ) ; long name offset ;	Creates a random name of the form prefix_X, where X is an increasing number.
@ override public void dispose ( ) throws exception { exception exception = null ; stream task < ? , ? > containing task = get containing task ( ) ; closeable registry task closeable registry = containing task != null ? containing task . get cancelables ( ) : null ; try { if ( task closeable registry == null || task closeable registry . unregister closeable ( operator state backend ) ) { operator state backend . close ( ) ; } } catch ( exception e ) { exception = e ; } try { if ( task closeable registry == null || task closeable registry . unregister closeable ( keyed state backend ) ) { keyed state backend . close ( ) ; } } catch ( exception e ) { exception = exception utils . first or suppressed ( e , exception ) ; } try { if ( operator state backend != null ) { operator state backend . dispose ( ) ; } } catch ( exception e ) { exception = exception utils . first or suppressed ( e , exception ) ; } try { if ( keyed state backend != null ) { keyed state backend . dispose ( ) ; } } catch ( exception e ) { exception = exception utils . first or suppressed ( e , exception ) ; } if ( exception != null ) { throw exception ; } }	This method is called at the very end of the operator's life, both in the case of a successfulcompletion of the operation, and in the case of a failure and canceling.
public void snapshot state ( state snapshot context context ) throws exception { final keyed state backend < ? > keyed state backend = get keyed state backend ( ) ;	Stream operators with state, which want to participate in a snapshot need to override this hook method.
public void put properties ( map < string , string > properties ) { for ( map . entry < string , string > property : properties . entry set ( ) ) { put ( property . get key ( ) , property . get value ( ) ) ; } }	Adds a set of properties.
public void put properties ( descriptor properties other properties ) { for ( map . entry < string , string > other property : other properties . properties . entry set ( ) ) { put ( other property . get key ( ) , other property . get value ( ) ) ; } }	Adds a set of descriptor properties.
public void put class ( string key , class < ? > clazz ) { check not null ( key ) ; check not null ( clazz ) ; final string error = instantiation util . check for instantiation error ( clazz ) ; if ( error != null ) { throw new validation exception ( str + clazz . get name ( ) + str + error ) ; } put ( key , clazz . get name ( ) ) ; }	Adds a class under the given key.
public void put string ( string key , string str ) { check not null ( key ) ; check not null ( str ) ; put ( key , str ) ; }	Adds a string under the given key.
public void put boolean ( string key , boolean b ) { check not null ( key ) ; put ( key , boolean . to string ( b ) ) ; }	Adds a boolean under the given key.
public void put long ( string key , long l ) { check not null ( key ) ; put ( key , long . to string ( l ) ) ; }	Adds a long under the given key.
public void put int ( string key , int i ) { check not null ( key ) ; put ( key , integer . to string ( i ) ) ; }	Adds an integer under the given key.
public void put character ( string key , char c ) { check not null ( key ) ; put ( key , character . to string ( c ) ) ; }	Adds a character under the given key.
public void put table schema ( string key , table schema schema ) { check not null ( key ) ; check not null ( schema ) ; final string [ ] field names = schema . get field names ( ) ; final type information < ? > [ ] field types = schema . get field types ( ) ; final list < list < string > > values = new array list < > ( ) ; for ( int i = num ; i < schema . get field count ( ) ; i ++ ) { values . add ( arrays . as list ( field names [ i ] , type string utils . write type info ( field types [ i ] ) ) ) ; } put indexed fixed properties ( key , arrays . as list ( table schema name , table schema type ) , values ) ; }	Adds a table schema under the given key.
public void put indexed variable properties ( string key , list < map < string , string > > sub key values ) { check not null ( key ) ; check not null ( sub key values ) ; for ( int idx = num ; idx < sub key values . size ( ) ; idx ++ ) { final map < string , string > values = sub key values . get ( idx ) ; for ( map . entry < string , string > value : values . entry set ( ) ) { put ( key + str + idx + str + value . get key ( ) , value . get value ( ) ) ; } } }	Adds an indexed mapping of properties under a common key.
public intermediate result partition get partition by id ( id result partition id ) {	Returns the partition with the given ID.
public string get ( string key ) { add to defaults ( key , null ) ; unrequested parameters . remove ( key ) ; return data . get ( key ) ; }	Returns the String value for the given key.If the key does not exist it will return null.
public string get ( string key , string default value ) { add to defaults ( key , default value ) ; string value = get ( key ) ; if ( value == null ) { return default value ; } else { return value ; } }	Returns the String value for the given key.If the key does not exist it will return the given default value.
public boolean has ( string value ) { add to defaults ( value , null ) ; unrequested parameters . remove ( value ) ; return data . contains key ( value ) ; }	Check if value is set.
public int get int ( string key ) { add to defaults ( key , null ) ; string value = get required ( key ) ; return integer . parse int ( value ) ; }	Returns the Integer value for the given key.The method fails if the key does not exist or the value is not an Integer.
public long get long ( string key ) { add to defaults ( key , null ) ; string value = get required ( key ) ; return long . parse long ( value ) ; }	Returns the Long value for the given key.The method fails if the key does not exist.
public float get float ( string key ) { add to defaults ( key , null ) ; string value = get required ( key ) ; return float . value of ( value ) ; }	Returns the Float value for the given key.The method fails if the key does not exist.
public float get float ( string key , float default value ) { add to defaults ( key , float . to string ( default value ) ) ; string value = get ( key ) ; if ( value == null ) { return default value ; } else { return float . value of ( value ) ; } }	Returns the Float value for the given key.
public double get double ( string key ) { add to defaults ( key , null ) ; string value = get required ( key ) ; return double . value of ( value ) ; }	Returns the Double value for the given key.The method fails if the key does not exist.
public double get double ( string key , double default value ) { add to defaults ( key , double . to string ( default value ) ) ; string value = get ( key ) ; if ( value == null ) { return default value ; } else { return double . value of ( value ) ; } }	Returns the Double value for the given key.
public boolean get boolean ( string key ) { add to defaults ( key , null ) ; string value = get required ( key ) ; return boolean . value of ( value ) ; }	Returns the Boolean value for the given key.The method fails if the key does not exist.
public boolean get boolean ( string key , boolean default value ) { add to defaults ( key , boolean . to string ( default value ) ) ; string value = get ( key ) ; if ( value == null ) { return default value ; } else { return boolean . value of ( value ) ; } }	Returns the Boolean value for the given key.
public short get short ( string key ) { add to defaults ( key , null ) ; string value = get required ( key ) ; return short . value of ( value ) ; }	Returns the Short value for the given key.The method fails if the key does not exist.
public short get short ( string key , short default value ) { add to defaults ( key , short . to string ( default value ) ) ; string value = get ( key ) ; if ( value == null ) { return default value ; } else { return short . value of ( value ) ; } }	Returns the Short value for the given key.
public byte get byte ( string key ) { add to defaults ( key , null ) ; string value = get required ( key ) ; return byte . value of ( value ) ; }	Returns the Byte value for the given key.The method fails if the key does not exist.
public byte get byte ( string key , byte default value ) { add to defaults ( key , byte . to string ( default value ) ) ; string value = get ( key ) ; if ( value == null ) { return default value ; } else { return byte . value of ( value ) ; } }	Returns the Byte value for the given key.
private static string get fqdn host name ( inet address inet address ) { string fqdn host name ; try { fqdn host name = inet address . get canonical host name ( ) ; } catch ( throwable t ) { log . warn ( str + str ) ; log . debug ( str , t ) ; fqdn host name = inet address . get host address ( ) ; } return fqdn host name ; }	Gets the fully qualified hostname of the TaskManager based on the network address.
public static string get host name ( inet address inet address ) { string host name ; string fqdn host name = get fqdn host name ( inet address ) ; if ( fqdn host name . equals ( inet address . get host address ( ) ) ) {	Gets the hostname of the TaskManager based on the network address.
public jdbc set parameter types ( type information < ? > ... types ) { int [ ] ty = new int [ types . length ] ; for ( int i = num ; i < types . length ; ++ i ) { ty [ i ] = jdbc . type information to sql type ( types [ i ] ) ; } this . parameter types = ty ; return this ; }	Specify the type of the rows that the sink will be accepting.
public jdbc build ( ) { preconditions . check not null ( parameter types , str + str ) ; jdbc format = jdbc . build jdbc ( ) . set username ( username ) . set password ( password ) . set db ( db url ) . set query ( query ) . set drivername ( driver name ) . set batch interval ( batch size ) . set sql types ( parameter types ) . finish ( ) ; return new jdbc ( format ) ; }	Finalizes the configuration and checks validity.
@ suppress warnings ( { str , str } ) private static < t > type serializer snapshot < t > configure for backwards compatibility ( type serializer snapshot < ? > snapshot , type serializer < ? > serializer ) { type serializer snapshot < t > typed snapshot = ( type serializer snapshot < t > ) snapshot ; type serializer < t > typed serializer = ( type serializer < t > ) serializer ; if ( snapshot instanceof type serializer config snapshot ) { ( ( type serializer config snapshot < t > ) typed snapshot ) . set prior serializer ( typed serializer ) ; } return typed snapshot ; }	Utility method to bind the serializer and serializer snapshot to a commongeneric type variable.
public static < t > serialized checkpoint data [ ] from deque ( array deque < tuple2 < long , set < t > > > checkpoints , type serializer < t > serializer ) throws io { return from deque ( checkpoints , serializer , new data output serializer ( num ) ) ; }	Converts a list of checkpoints with elements into an array of SerializedCheckpointData.
public static < t > serialized checkpoint data [ ] from deque ( array deque < tuple2 < long , set < t > > > checkpoints , type serializer < t > serializer , data output serializer output buffer ) throws io { serialized checkpoint data [ ] serialized checkpoints = new serialized checkpoint data [ checkpoints . size ( ) ] ; int pos = num ; for ( tuple2 < long , set < t > > checkpoint : checkpoints ) { output buffer . clear ( ) ; set < t > checkpoint ids = checkpoint . f1 ; for ( t id : checkpoint ids ) { serializer . serialize ( id , output buffer ) ; } serialized checkpoints [ pos ++ ] = new serialized checkpoint data ( checkpoint . f0 , output buffer . get copy of buffer ( ) , checkpoint ids . size ( ) ) ; } return serialized checkpoints ; }	Converts a list of checkpoints into an array of SerializedCheckpointData.
public static < t > array deque < tuple2 < long , set < t > > > to deque ( serialized checkpoint data [ ] data , type serializer < t > serializer ) throws io { array deque < tuple2 < long , set < t > > > deque = new array deque < > ( data . length ) ; data input deserializer deser = null ; for ( serialized checkpoint data checkpoint : data ) { byte [ ] serialized data = checkpoint . get serialized data ( ) ; if ( deser == null ) { deser = new data input deserializer ( serialized data , num , serialized data . length ) ; } else { deser . set buffer ( serialized data ) ; } final set < t > ids = new hash set < > ( checkpoint . get num ids ( ) ) ; final int num ids = checkpoint . get num ids ( ) ; for ( int i = num ; i < num ids ; i ++ ) { ids . add ( serializer . deserialize ( deser ) ) ; } deque . add last ( new tuple2 < long , set < t > > ( checkpoint . checkpoint id , ids ) ) ; } return deque ; }	De-serializes an array of SerializedCheckpointData back into an ArrayDeque of element checkpoints.
private static file generate default config file ( ) { final file jaas conf file ; try { path jaas conf path = files . create temp file ( str , str ) ; try ( input stream resource stream = jaas module . class . get class loader ( ) . get resource as stream ( jaas conf resource name ) ) { files . copy ( resource stream , jaas conf path , standard copy option . replace existing ) ; } jaas conf file = jaas conf path . to file ( ) ; jaas conf file . delete on exit ( ) ; } catch ( io e ) { throw new runtime exception ( str , e ) ; } return jaas conf file ; }	Generate the default JAAS config file.
public static list < expression > rename columns ( list < string > input fields , list < expression > new aliases ) { linked hash map < string , expression > final fields = new linked hash map < > ( ) ; input fields . for each ( field -> final fields . put ( field , new unresolved reference expression ( field ) ) ) ; new aliases . for each ( expr -> { string name = expr . accept ( rename column extractor ) ; final fields . put ( name , expr ) ; } ) ; return new array list < > ( final fields . values ( ) ) ; }	Creates a projection list that renames existing columns to new names.
public static list < expression > drop fields ( list < string > input fields , list < expression > drop expressions ) { set < string > columns to drop = drop expressions . stream ( ) . map ( expr -> expr . accept ( drop columns extractor ) ) . collect ( collectors . to set ( ) ) ; columns to drop . for each ( c -> { if ( ! input fields . contains ( c ) ) { throw new validation exception ( format ( str , c ) ) ; } } ) ; return input fields . stream ( ) . filter ( old name -> ! columns to drop . contains ( old name ) ) . map ( unresolved reference expression :: new ) . collect ( collectors . to list ( ) ) ; }	Creates a projection list that removes given columns.
public static < t > data set < tuple2 < integer , long > > count elements per partition ( data set < t > input ) { return input . map partition ( new rich map partition function < t , tuple2 < integer , long > > ( ) { @ override public void map partition ( iterable < t > values , collector < tuple2 < integer , long > > out ) throws exception { long counter = num ; for ( t value : values ) { counter ++ ; } out . collect ( new tuple2 < > ( get runtime context ( ) . get index of this subtask ( ) , counter ) ) ; } } ) ; }	Method that goes over all the elements in each partition in order to retrievethe total number of elements.
public static < t > partition operator < t > partition by range ( data set < t > input , data distribution distribution , int ... fields ) { return new partition operator < > ( input , partition operator base . partition method . range , new keys . expression keys < > ( fields , input . get type ( ) , bool ) , distribution , utils . get call location name ( ) ) ; }	Range-partitions a DataSet on the specified tuple field positions.
public static < t , k extends comparable < k > > partition operator < t > partition by range ( data set < t > input , data distribution distribution , key selector < t , k > key extractor ) { final type information < k > key type = type extractor . get key selector types ( key extractor , input . get type ( ) ) ; return new partition operator < > ( input , partition operator base . partition method . range , new keys . selector function keys < > ( input . clean ( key extractor ) , input . get type ( ) , key type ) , distribution , utils . get call location name ( ) ) ; }	Range-partitions a DataSet using the specified key selector function.
public static < r extends tuple , t extends tuple > r summarize ( data set < t > input ) throws exception { if ( ! input . get type ( ) . is tuple type ( ) ) { throw new illegal argument exception ( str ) ; } final tuple type info base < ? > in type = ( tuple type info base < ? > ) input . get type ( ) ; data set < tuple summary aggregator < r > > result = input . map partition ( new map partition function < t , tuple summary aggregator < r > > ( ) { @ override public void map partition ( iterable < t > values , collector < tuple summary aggregator < r > > out ) throws exception { tuple summary aggregator < r > aggregator = summary aggregator factory . create ( in type ) ; for ( tuple value : values ) { aggregator . aggregate ( value ) ; } out . collect ( aggregator ) ; } } ) . reduce ( new reduce function < tuple summary aggregator < r > > ( ) { @ override public tuple summary aggregator < r > reduce ( tuple summary aggregator < r > agg1 , tuple summary aggregator < r > agg2 ) throws exception { agg1 . combine ( agg2 ) ; return agg1 ; } } ) ; return result . collect ( ) . get ( num ) . result ( ) ; }	Summarize a DataSet of Tuples by collecting single pass statistics for all columns. Example usage: {.
public csv reader ignore comments ( string comment prefix ) { if ( comment prefix == null || comment prefix . length ( ) == num ) { throw new illegal argument exception ( str ) ; } this . comment prefix = comment prefix ; return this ; }	Configures the string that starts comments.By default comments will be treated as invalid lines.This function only recognizes comments which start at the beginning of the line!.
public < t > data source < t > pojo type ( class < t > pojo type , string ... pojo fields ) { preconditions . check not null ( pojo type , str ) ; preconditions . check not null ( pojo fields , str ) ; final type information < t > ti = type extractor . create type info ( pojo type ) ; if ( ! ( ti instanceof pojo type info ) ) { throw new illegal argument exception ( str + ti ) ; } final pojo type info < t > pti = ( pojo type info < t > ) ti ; csv input format < t > input format = new pojo csv input format < t > ( path , this . line delimiter , this . field delimiter , pti , pojo fields , this . included mask ) ; configure input format ( input format ) ; return new data source < t > ( execution context , input format , pti , utils . get call location name ( ) ) ; }	Configures the reader to read the CSV data and parse it to the given type.
public static text element code ( string text ) { text element element = text ( text ) ; element . text styles . add ( text style . code ) ; return element ; }	Creates a block of text formatted as code.
public void set any partitioning ( field set partitioned fields ) { if ( partitioned fields == null ) { throw new null pointer exception ( ) ; } this . partitioning = partitioning property . any partitioning ; this . partitioning fields = partitioned fields ; this . ordering = null ; }	Sets these properties to request some partitioning on the given fields.
public void reset ( ) { this . partitioning = partitioning property . random partitioned ; this . ordering = null ; this . partitioning fields = null ; this . data distribution = null ; this . custom partitioner = null ; }	This method resets the properties to a state where no properties are given.
public void parameterize channel ( channel channel , boolean global dop change , execution mode exchange mode , boolean break pipeline ) {	Parametrizes the ship strategy fields of a channel such that the channel producesthe desired global properties.
public void add app configuration entry ( string name , app configuration entry ... entry ) { final app configuration entry [ ] existing = dynamic entries . get ( name ) ; final app configuration entry [ ] updated ; if ( existing == null ) { updated = arrays . copy of ( entry , entry . length ) ; } else { updated = merge ( existing , entry ) ; } dynamic entries . put ( name , updated ) ; }	Add entries for the given application name.
@ suppress warnings ( str ) public final char get char ( int index ) { final long pos = address + index ; if ( index >= num && pos <= address limit - num ) { return unsafe . get char ( heap memory , pos ) ; } else if ( address > address limit ) { throw new illegal state exception ( str ) ; } else {	Reads a char value from the given position, in the system's native byte order.
@ suppress warnings ( str ) public final void put char ( int index , char value ) { final long pos = address + index ; if ( index >= num && pos <= address limit - num ) { unsafe . put char ( heap memory , pos , value ) ; } else if ( address > address limit ) { throw new illegal state exception ( str ) ; } else {	Writes a char value to the given position, in the system's native byte order.
public final void put short ( int index , short value ) { final long pos = address + index ; if ( index >= num && pos <= address limit - num ) { unsafe . put short ( heap memory , pos , value ) ; } else if ( address > address limit ) { throw new illegal state exception ( str ) ; } else {	Writes the given short value into this buffer at the given position, usingthe native byte order of the system.
public final int compare ( memory segment seg2 , int offset1 , int offset2 , int len ) { while ( len >= num ) { long l1 = this . get long big endian ( offset1 ) ; long l2 = seg2 . get long big endian ( offset2 ) ; if ( l1 != l2 ) { return ( l1 < l2 ) ^ ( l1 < num ) ^ ( l2 < num ) ? - num : num ; } offset1 += num ; offset2 += num ; len -= num ; } while ( len > num ) { int b1 = this . get ( offset1 ) & num ; int b2 = seg2 . get ( offset2 ) & num ; int cmp = b1 - b2 ; if ( cmp != num ) { return cmp ; } offset1 ++ ; offset2 ++ ; len -- ; } return num ; }	Compares two memory segment regions.
public final void swap bytes ( byte [ ] temp buffer , memory segment seg2 , int offset1 , int offset2 , int len ) { if ( ( offset1 | offset2 | len | ( temp buffer . length - len ) ) >= num ) { final long this pos = this . address + offset1 ; final long other pos = seg2 . address + offset2 ; if ( this pos <= this . address limit - len && other pos <= seg2 . address limit - len ) {	Swaps bytes between two memory segments, using the given auxiliary buffer.
public final boolean equal to ( memory segment seg2 , int offset1 , int offset2 , int length ) { int i = num ;	Equals two memory segment regions.
@ public evolving public list < flat field descriptor > get flat fields ( string field expression ) { list < flat field descriptor > result = new array list < flat field descriptor > ( ) ; this . get flat fields ( field expression , num , result ) ; return result ; }	Returns the flat field descriptors for the given field expression.
@ public evolving public type comparator < t > create comparator ( int [ ] logical key fields , boolean [ ] orders , int logical field offset , execution config config ) { type comparator builder < t > builder = create type comparator builder ( ) ; builder . initialize type comparator builder ( logical key fields . length ) ; for ( int logical key field index = num ; logical key field index < logical key fields . length ; logical key field index ++ ) { int logical key field = logical key fields [ logical key field index ] ; int logical field = logical field offset ;	Generic implementation of the comparator creation. Composite types are supplying the infrastructureto create the actual comparators.
private path get yarn files dir ( final application id app id ) throws io { final file system file system = file system . get ( yarn configuration ) ; final path home dir = file system . get home directory ( ) ; return new path ( home dir , str + app id + str ) ; }	Returns the Path where the YARN application files should be uploaded to.
private void fail session during deployment ( yarn client yarn client , yarn client application yarn application ) { log . info ( str ) ; try { yarn client . kill application ( yarn application . get new application response ( ) . get application id ( ) ) ; } catch ( exception e ) {	Kills YARN application and stops YARN client. Use this method to kill the App before it has been properly deployed.
private data stream sink < t > set resources ( resource spec min resources , resource spec preferred resources ) { preconditions . check not null ( min resources , str ) ; preconditions . check not null ( preferred resources , str ) ; preconditions . check argument ( min resources . is valid ( ) && preferred resources . is valid ( ) && min resources . less than or equal ( preferred resources ) , str ) ; transformation . set resources ( min resources , preferred resources ) ; return this ; }	Sets the minimum and preferred resources for this sink, and the lower and upper resource limits willbe considered in resource resize feature for future plan.
public void shutdown ( ) { synchronized ( lock ) {	Shuts down the file cache by cancelling all.
public future < path > create tmp file ( string name , distributed cache entry entry , id job id , id execution id ) throws exception { synchronized ( lock ) { map < string , future < path > > job entries = entries . compute if absent ( job id , k -> new hash map < > ( ) ) ;	If the file doesn't exists locally, retrieve the file from the blob-service.
public static network environment configuration from configuration ( configuration configuration , long max jvm heap memory , boolean local task manager communication , inet address task manager address ) { final int dataport = get dataport ( configuration ) ; final int page size = get page size ( configuration ) ; final int number of network buffers = calculate number of network buffers ( configuration , max jvm heap memory ) ; final netty config netty config = create netty config ( configuration , local task manager communication , task manager address , dataport ) ; int initial request backoff = configuration . get integer ( task manager options . network request backoff initial ) ; int max request backoff = configuration . get integer ( task manager options . network request backoff max ) ; int buffers per channel = configuration . get integer ( task manager options . network buffers per channel ) ; int extra buffers per gate = configuration . get integer ( task manager options . network extra buffers per gate ) ; boolean is credit based = netty config != null && configuration . get boolean ( task manager options . network credit model ) ; return new network environment configuration ( number of network buffers , page size , initial request backoff , max request backoff , buffers per channel , extra buffers per gate , is credit based , netty config ) ; }	Utility method to extract network related parameters from the configuration and tosanity check them.
@ suppress warnings ( str ) @ visible for testing public static boolean has new network config ( final configuration config ) { return config . contains ( task manager options . network buffers memory fraction ) || config . contains ( task manager options . network buffers memory min ) || config . contains ( task manager options . network buffers memory max ) || ! config . contains ( task manager options . network num buffers ) ; }	Returns whether the new network buffer memory configuration is present in the configurationobject, i.e.
@ suppress warnings ( str ) private static int calculate number of network buffers ( configuration configuration , long max jvm heap memory ) { final int number of network buffers ; if ( ! has new network config ( configuration ) ) {	Calculates the number of network buffers based on configuration and jvm heap size.
public static int get page size ( configuration configuration ) { final int page size = checked down cast ( memory size . parse ( configuration . get string ( task manager options . memory segment size ) ) . get bytes ( ) ) ;	Parses the configuration to get the page size and validates the value.
public split data properties < t > splits ordered by ( int [ ] order fields , order [ ] orders ) { if ( order fields == null || orders == null ) { throw new invalid program exception ( str ) ; } else if ( order fields . length == num ) { throw new invalid program exception ( str ) ; } else if ( orders . length == num ) { throw new invalid program exception ( str ) ; } else if ( order fields . length != orders . length ) { throw new invalid program exception ( str ) ; } if ( this . split group keys != null ) { throw new invalid program exception ( str ) ; } this . split ordering = new ordering ( ) ; for ( int i = num ; i < order fields . length ; i ++ ) { int pos = order fields [ i ] ; int [ ] flat keys = this . get all flat keys ( new int [ ] { pos } ) ; for ( int key : flat keys ) {	Defines that the data within an input split is sorted on the fields defined by the field positionsin the specified orders.All records of an input split must be emitted by the input format in the defined order.
public split data properties < t > splits ordered by ( string order fields , order [ ] orders ) { if ( order fields == null || orders == null ) { throw new invalid program exception ( str ) ; } string [ ] order keys a = order fields . split ( str ) ; if ( order keys a . length == num ) { throw new invalid program exception ( str ) ; } else if ( orders . length == num ) { throw new invalid program exception ( str ) ; } else if ( order keys a . length != orders . length ) { throw new invalid program exception ( str ) ; } if ( this . split group keys != null ) { throw new invalid program exception ( str ) ; } this . split ordering = new ordering ( ) ; for ( int i = num ; i < order keys a . length ; i ++ ) { string key exp = order keys a [ i ] ; keys . expression keys < t > ek = new keys . expression keys < > ( key exp , this . type ) ; int [ ] flat keys = ek . compute logical key positions ( ) ; for ( int key : flat keys ) {	Defines that the data within an input split is sorted on the fields defined by the field expressionsin the specified orders.
public static < t > t copy ( t from , kryo kryo , type serializer < t > serializer ) { try { return kryo . copy ( from ) ; } catch ( kryo exception ke ) {	Tries to copy the given record from using the provided Kryo instance.
@ override public void open ( input split ignored ) throws io { this . session = cluster . connect ( ) ; this . result set = session . execute ( query ) ; }	Opens a Session and executes the query.
private void redistribute buffers ( ) throws io { assert thread . holds lock ( factory lock ) ;	Must be called from synchronized block.
public void close ( ) throws io {	Closes all structures and deletes all temporary files.Even in the presence of failures, this method will try and continue closingfiles and deleting temporary files.
private aggregate operator < t > aggregate ( aggregations agg , int field , string call location name ) { return new aggregate operator < t > ( this , agg , field , call location name ) ; }	private helper that allows to set a different call location name.
public array list < memory segment > reset overflow buckets ( ) { this . num overflow segments = num ; this . next overflow bucket = num ; array list < memory segment > result = new array list < memory segment > ( this . overflow segments . length ) ; for ( int i = num ; i < this . overflow segments . length ; i ++ ) { if ( this . overflow segments [ i ] != null ) { result . add ( this . overflow segments [ i ] ) ; } } this . overflow segments = new memory segment [ num ] ; return result ; }	resets overflow bucket counters and returns freed memory and should only be used for resizing.
public final long append record ( t record ) throws io { long pointer = this . write view . get current pointer ( ) ; try { this . serializer . serialize ( record , this . write view ) ; this . record counter ++ ; return pointer ; } catch ( eof e ) {	Inserts the given object into the current buffer.
@ deprecated public void overwrite record at ( long pointer , t record ) throws io { long tmp pointer = this . write view . get current pointer ( ) ; this . write view . reset to ( pointer ) ; this . serializer . serialize ( record , this . write view ) ; this . write view . reset to ( tmp pointer ) ; }	UNSAFE!! overwrites recordcauses inconsistency or data loss for overwriting everything but records of the exact same size.
public void allocate segments ( int number of segments ) { while ( get block count ( ) < number of segments ) { memory segment next = this . available memory . next segment ( ) ; if ( next != null ) { this . partition pages . add ( next ) ; } else { return ; } } }	attempts to allocate specified number of segments and should only be used by compaction partitionfails silently if not enough segments are available since next compaction could still succeed.
public static binary in memory sort buffer create buffer ( normalized key computer normalized key computer , abstract row serializer < base row > input serializer , binary row serializer serializer , record comparator comparator , list < memory segment > memory ) throws io { check argument ( memory . size ( ) >= min required buffers ) ; int total num buffers = memory . size ( ) ; list memory segment pool pool = new list memory segment pool ( memory ) ; array list < memory segment > record buffer segments = new array list < > ( num ) ; return new binary in memory sort buffer ( normalized key computer , input serializer , serializer , comparator , record buffer segments , new simple collecting output view ( record buffer segments , pool , pool . page size ( ) ) , pool , total num buffers ) ; }	Create a memory sorter in `insert` way.
@ override public void serialize record ( t record ) throws io { if ( checked ) { if ( data buffer . has remaining ( ) ) { throw new illegal state exception ( str ) ; } } serialization buffer . clear ( ) ; length buffer . clear ( ) ;	Serializes the complete record to an intermediate data serialization buffer.
@ override public serialization result copy to buffer builder ( buffer builder target buffer ) { target buffer . append ( length buffer ) ; target buffer . append ( data buffer ) ; target buffer . commit ( ) ; return get serialization result ( target buffer ) ; }	Copies an intermediate data serialization buffer into the target BufferBuilder.
private void enqueue available reader ( final network sequence view reader reader ) throws exception { if ( reader . is registered as available ( ) || ! reader . is available ( ) ) { return ; }	Try to enqueue the reader once receiving credit notification from the consumer or receivingnon-empty reader notification from the producer.
@ override public void invoke ( in value ) { try { byte [ ] msg = schema . serialize ( value ) ; if ( publish options == null ) { channel . basic publish ( str , queue name , null , msg ) ; } else { boolean mandatory = publish options . compute mandatory ( value ) ; boolean immediate = publish options . compute immediate ( value ) ; preconditions . check state ( ! ( return listener == null && ( mandatory || immediate ) ) , str ) ; string rk = publish options . compute routing key ( value ) ; string exchange = publish options . compute exchange ( value ) ; channel . basic publish ( exchange , rk , mandatory , immediate , publish options . compute properties ( value ) , msg ) ; } } catch ( io e ) { if ( log failures only ) { log . error ( str , queue name , rmq connection config . get host ( ) , e ) ; } else { throw new runtime exception ( str + queue name + str + rmq connection config . get host ( ) , e ) ; } } }	Called when new data arrives to the sink, and forwards it to RMQ.
@ override public void set input type ( type information < ? > type , execution config execution config ) { if ( ! type . is tuple type ( ) ) { throw new invalid program exception ( str + scala csv output format . class . get simple name ( ) + str ) ; } }	The purpose of this method is solely to check whether the data type to be processedis in fact a tuple type.
public static < x > pattern < x , x > begin ( final string name ) { return new pattern < > ( name , null , consuming strategy . strict , after match skip strategy . no skip ( ) ) ; }	Starts a new pattern sequence.
public < s extends f > pattern < t , s > subtype ( final class < s > subtype class ) { preconditions . check not null ( subtype class , str ) ; if ( condition == null ) { this . condition = new subtype condition < f > ( subtype class ) ; } else { this . condition = new rich and condition < > ( condition , new subtype condition < f > ( subtype class ) ) ; } @ suppress warnings ( str ) pattern < t , s > result = ( pattern < t , s > ) this ; return result ; }	Applies a subtype constraint on the current pattern.
public pattern < t , f > until ( iterative condition < f > until condition ) { preconditions . check not null ( until condition , str ) ; if ( this . until condition != null ) { throw new malformed pattern exception ( str ) ; } if ( ! quantifier . has property ( quantifier . quantifier property . looping ) ) { throw new malformed pattern exception ( str ) ; } closure cleaner . clean ( until condition , bool ) ; this . until condition = until condition ; return this ; }	Applies a stop condition for a looping state.
public pattern < t , f > within ( time window time ) { if ( window time != null ) { this . window time = window time ; } return this ; }	Defines the maximum time interval in which a matching pattern has to be completed inorder to be considered valid.
public pattern < t , t > next ( final string name ) { return new pattern < > ( name , this , consuming strategy . strict , after match skip strategy ) ; }	Appends a new pattern to the existing one.
public pattern < t , t > not next ( final string name ) { if ( quantifier . has property ( quantifier . quantifier property . optional ) ) { throw new unsupported operation exception ( str + str + str ) ; } return new pattern < > ( name , this , consuming strategy . not next , after match skip strategy ) ; }	Appends a new pattern to the existing one.
public pattern < t , t > not followed by ( final string name ) { if ( quantifier . has property ( quantifier . quantifier property . optional ) ) { throw new unsupported operation exception ( str + str + str ) ; } return new pattern < > ( name , this , consuming strategy . not follow , after match skip strategy ) ; }	Appends a new pattern to the existing one.
public pattern < t , f > times ( int times ) { check if no not pattern ( ) ; check if quantifier applied ( ) ; preconditions . check argument ( times > num , str ) ; this . quantifier = quantifier . times ( quantifier . get consuming strategy ( ) ) ; this . times = times . of ( times ) ; return this ; }	Specifies exact number of times that this pattern should be matched.
public pattern < t , f > times ( int from , int to ) { check if no not pattern ( ) ; check if quantifier applied ( ) ; this . quantifier = quantifier . times ( quantifier . get consuming strategy ( ) ) ; if ( from == num ) { this . quantifier . optional ( ) ; from = num ; } this . times = times . of ( from , to ) ; return this ; }	Specifies that the pattern can occur between from and to times.
public pattern < t , f > times or more ( int times ) { check if no not pattern ( ) ; check if quantifier applied ( ) ; this . quantifier = quantifier . looping ( quantifier . get consuming strategy ( ) ) ; this . times = times . of ( times ) ; return this ; }	Specifies that this pattern can occur the specified times at least.This means at least the specified times and at most infinite number of events canbe matched to this pattern.
public static < t , f extends t > group pattern < t , f > begin ( final pattern < t , f > group , final after match skip strategy after match skip strategy ) { return new group pattern < > ( null , group , consuming strategy . strict , after match skip strategy ) ; }	Starts a new pattern sequence.
public group pattern < t , f > next ( pattern < t , f > group ) { return new group pattern < > ( this , group , consuming strategy . strict , after match skip strategy ) ; }	Appends a new group pattern to the existing one.
public void replace ( string path in zoo keeper , int expected version , t state ) throws exception { check not null ( path in zoo keeper , str ) ; check not null ( state , str ) ; final string path = normalize path ( path in zoo keeper ) ; retrievable state handle < t > old state handle = get ( path , bool ) ; retrievable state handle < t > new state handle = storage . store ( state ) ; boolean success = bool ; try {	Replaces a state handle in ZooKeeper and discards the old state handle.
public collection < string > get all paths ( ) throws exception { final string path = str ; while ( bool ) { stat stat = client . check exists ( ) . for path ( path ) ; if ( stat == null ) { return collections . empty list ( ) ; } else { try { return client . get children ( ) . for path ( path ) ; } catch ( keeper exception . no node exception ignored ) {	Return a list of all valid paths for state handles.
@ suppress warnings ( str ) public list < tuple2 < retrievable state handle < t > , string > > get all and lock ( ) throws exception { final list < tuple2 < retrievable state handle < t > , string > > state handles = new array list < > ( ) ; boolean success = bool ; retry : while ( ! success ) { state handles . clear ( ) ; stat stat = client . check exists ( ) . for path ( str ) ; if ( stat == null ) { break ;	Gets all available state handles from ZooKeeper and locks the respective state nodes.
public void release and try remove all ( ) throws exception { collection < string > children = get all paths ( ) ; exception exception = null ; for ( string child : children ) { try { release and try remove ( str + child ) ; } catch ( exception e ) { exception = exception utils . first or suppressed ( e , exception ) ; } } if ( exception != null ) { throw new exception ( str , exception ) ; } }	Releases all lock nodes of this ZooKeeperStateHandleStores and tries to remove all state nodes whichare not locked anymore. The delete operation is executed asynchronously.
public void release ( string path in zoo keeper ) throws exception { final string path = normalize path ( path in zoo keeper ) ; try { client . delete ( ) . for path ( get lock path ( path ) ) ; } catch ( keeper exception . no node exception ignored ) {	Releases the lock from the node under the given ZooKeeper path.
public void release all ( ) throws exception { collection < string > children = get all paths ( ) ; exception exception = null ; for ( string child : children ) { try { release ( child ) ; } catch ( exception e ) { exception = exception utils . first or suppressed ( e , exception ) ; } } if ( exception != null ) { throw new exception ( str , exception ) ; } }	Releases all lock nodes of this ZooKeeperStateHandleStore.
public void delete children ( ) throws exception { final string path = str + client . get namespace ( ) ; log . info ( str , path ) ; zk . delete children ( client . get zookeeper client ( ) . get zoo keeper ( ) , path , bool ) ; }	Recursively deletes all children.
@ suppress warnings ( str ) private retrievable state handle < t > get ( string path in zoo keeper , boolean lock ) throws exception { check not null ( path in zoo keeper , str ) ; final string path = normalize path ( path in zoo keeper ) ; if ( lock ) {	Gets a state handle from ZooKeeper and optionally locks it.
public void set ( int index ) { preconditions . check argument ( index < bit length && index >= num ) ; int byte index = ( index & byte position mask ) > > > num ; byte current = memory segment . get ( offset + byte index ) ; current |= ( num << ( index & byte index mask ) ) ; memory segment . put ( offset + byte index , current ) ; }	Sets the bit at specified index.
@ override public final void co group ( iterable < i > first , iterable < i > second , collector < out > out ) throws exception { streamer . stream buffer with groups ( first . iterator ( ) , second . iterator ( ) , out ) ; }	Calls the external python function.
void register column family ( string column family name , column family handle handle ) { metric group group = metric group . add group ( column family name ) ; for ( string property : options . get properties ( ) ) { db gauge = new db ( handle , property ) ; group . gauge ( property , gauge ) ; } }	Register gauges to pull native metrics for the column family.
private void set property ( column family handle handle , string property , db metric view ) { if ( metric view . is closed ( ) ) { return ; } try { synchronized ( lock ) { if ( rocks db != null ) { long value = rocks db . get long property ( handle , property ) ; metric view . set value ( value ) ; } } } catch ( db e ) { metric view . close ( ) ; log . warn ( str , property , e ) ; } }	Updates the value of metricView if the reference is still valid.
@ override public string get message ( ) { string ret = super . get message ( ) ; if ( file name != null ) { ret += ( str + file name ) ; if ( line number != - num ) { ret += str + line number ; } if ( column number != - num ) { ret += str + column number ; } } return ret ; }	Returns a message containing the String passed to a constructor as well as line and column numbers and filename if any of these are known.
public static int bernstein ( string key ) { int hash = num ; int i ; for ( i = num ; i < key . length ( ) ; ++ i ) { hash = num * hash + key . char at ( i ) ; } return hash ; }	Bernstein's hash.
public string next to ( string delimiters ) throws json { char c ; string builder sb = new string builder ( ) ; for ( ; ; ) { c = this . next ( ) ; if ( delimiters . index of ( c ) >= num || c == num || c == str || c == str ) { if ( c != num ) { this . back ( ) ; } return sb . to string ( ) . trim ( ) ; } sb . append ( c ) ; } }	Get the text up but not including one of the specified delimiter characters or the end of line, whichever comes first.
private static void append digits ( final appendable buffer , final int value ) throws io { buffer . append ( ( char ) ( value / num + str ) ) ; buffer . append ( ( char ) ( value % num + str ) ) ; }	Appends two digits to the given buffer.
private static void append full digits ( final appendable buffer , int value , int min field width ) throws io {	Appends all digits to the given buffer.
private void read object ( final object input stream in ) throws io , class not found exception { in . default read object ( ) ; final calendar defining calendar = calendar . get instance ( time zone , locale ) ; init ( defining calendar ) ; }	Create the object after serialization.
private static map < string , integer > append display names ( final calendar cal , final locale locale , final int field , final string builder regex ) { final map < string , integer > values = new hash map < > ( ) ; final map < string , integer > display names = cal . get display names ( field , calendar . all styles , locale ) ; final tree set < string > sorted = new tree set < > ( longer first lowercase ) ; for ( final map . entry < string , integer > display name : display names . entry set ( ) ) { final string key = display name . get key ( ) . to lower case ( locale ) ; if ( sorted . add ( key ) ) { values . put ( key , display name . get value ( ) ) ; } } for ( final string symbol : sorted ) { simple quote ( regex , symbol ) . append ( str ) ; } return values ; }	Get the short and long values displayed for a field.
private strategy get strategy ( final char f , final int width , final calendar defining calendar ) { switch ( f ) { default : throw new illegal argument exception ( str + f + str ) ; case str : return day of year strategy ; case str : return get locale specific strategy ( calendar . day of week , defining calendar ) ; case str : return day of week in month strategy ; case str : return get locale specific strategy ( calendar . era , defining calendar ) ; case str :	Obtain a Strategy given a field from a SimpleDateFormat pattern.
private static concurrent map < locale , strategy > get cache ( final int field ) { synchronized ( caches ) { if ( caches [ field ] == null ) { caches [ field ] = new concurrent hash map < > ( num ) ; } return caches [ field ] ; } }	Get a cache of Strategies for a particular field.
private strategy get locale specific strategy ( final int field , final calendar defining calendar ) { final concurrent map < locale , strategy > cache = get cache ( field ) ; strategy strategy = cache . get ( locale ) ; if ( strategy == null ) { strategy = field == calendar . zone offset ? new time zone strategy ( locale ) : new case insensitive text strategy ( field , defining calendar , locale ) ; final strategy in cache = cache . put if absent ( locale , strategy ) ; if ( in cache != null ) { return in cache ; } } return strategy ; }	Construct a Strategy that parses a Text field.
private f get date time instance ( final integer date style , final integer time style , final time zone time zone , locale locale ) { if ( locale == null ) { locale = locale . get default ( ) ; } final string pattern = get pattern for style ( date style , time style , locale ) ; return get instance ( pattern , time zone , locale ) ; }	This must remain private, see LANG-884.
public object next meta ( ) throws json { char c ; char q ; do { c = next ( ) ; } while ( character . is whitespace ( c ) ) ; switch ( c ) { case num : throw syntax error ( str ) ; case str : return xml . lt ; case str : return xml . gt ; case str : return xml . slash ; case str : return xml . eq ; case str : return xml . bang ; case str : return xml . quest ; case str : case str : q = c ; for ( ; ; ) { c = next ( ) ; if ( c == num ) { throw syntax error ( str ) ; } if ( c == q ) { return boolean . true ; } } default : for ( ; ; ) { c = next ( ) ; if ( character . is whitespace ( c ) ) { return boolean . true ; } switch ( c ) { case num : case str : case str : case str : case str : case str : case str : case str : case str : back ( ) ; return boolean . true ; } } } }	Returns the next XML meta token.
public boolean skip past ( string to ) throws json { boolean b ; char c ; int i ; int j ; int offset = num ; int length = to . length ( ) ; char [ ] circle = new char [ length ] ; for ( i = num ; i < length ; i += num ) { c = next ( ) ; if ( c == num ) { return bool ; } circle [ i ] = c ; } for ( ; ; ) { j = offset ; b = bool ; for ( i = num ; i < length ; i += num ) { if ( circle [ j ] != to . char at ( i ) ) { b = bool ; break ; } j += num ; if ( j >= length ) { j -= length ; } } if ( b ) { return bool ; } c = next ( ) ; if ( c == num ) { return bool ; } circle [ offset ] = c ; offset += num ; if ( offset >= length ) { offset -= length ; } } }	Skip characters until past the requested string.
public string filter ( final string input ) { reset ( ) ; string s = input ; debug ( str ) ; debug ( str + input ) ; s = escape comments ( s ) ; debug ( str + s ) ; s = balance html ( s ) ; debug ( str + s ) ; s = check tags ( s ) ; debug ( str + s ) ; s = process remove blanks ( s ) ; debug ( str + s ) ; s = validate entities ( s ) ; debug ( str + s ) ; debug ( str ) ; return s ; }	given a user submitted input String, filter out any invalid or restricted html.
@ override public void dump request ( map < string , object > result ) { exchange . get query parameters ( ) . for each ( ( k , v ) -> { if ( config . get request filtered query parameters ( ) . contains ( k ) ) {	impl of dumping request query parameter to result.
@ override protected void put dump info to ( map < string , object > result ) { if ( this . query parameters map . size ( ) > num ) { result . put ( dump constants . query parameters , query parameters map ) ; } }	put queryParametersMap to result.
public void send mail ( string to , string subject , string content ) throws messaging exception { properties props = new properties ( ) ; props . put ( str , email confg . get user ( ) ) ; props . put ( str , email confg . get host ( ) ) ; props . put ( str , email confg . get port ( ) ) ; props . put ( str , str ) ; props . put ( str , email confg . get debug ( ) ) ; props . put ( str , email confg . get auth ( ) ) ; props . put ( str , email confg . host ) ; smtp auth = new smtp ( email confg . get user ( ) , ( string ) secret . get ( secret constants . email password ) ) ; session session = session . get instance ( props , auth ) ; mime message message = new mime message ( session ) ; message . set from ( new internet address ( email confg . get user ( ) ) ) ; message . add recipient ( message . recipient type . to , new internet address ( to ) ) ; message . set subject ( subject ) ; message . set content ( content , str ) ;	Send email with a string content.
public void send mail with attachment ( string to , string subject , string content , string filename ) throws messaging exception { properties props = new properties ( ) ; props . put ( str , email confg . get user ( ) ) ; props . put ( str , email confg . get host ( ) ) ; props . put ( str , email confg . get port ( ) ) ; props . put ( str , str ) ; props . put ( str , email confg . get debug ( ) ) ; props . put ( str , email confg . get auth ( ) ) ; props . put ( str , email confg . host ) ; smtp auth = new smtp ( email confg . get user ( ) , ( string ) secret . get ( secret constants . email password ) ) ; session session = session . get instance ( props , auth ) ; mime message message = new mime message ( session ) ; message . set from ( new internet address ( email confg . get user ( ) ) ) ; message . add recipient ( message . recipient type . to , new internet address ( to ) ) ; message . set subject ( subject ) ;	Send email with a string content and attachment.
private static void handle singleton class ( string key , string value ) throws exception { object object = handle value ( value ) ; if ( key . contains ( str ) ) { string [ ] interfaces = key . split ( str ) ; for ( string an interface : interfaces ) { service map . put ( an interface , object ) ; } } else { service map . put ( key , object ) ; } }	For each singleton definition, create object with the initializer class and method,and push it into the service map with the key of the class name.
private static void handle singleton list ( string key , list < object > value ) throws exception { list < string > interface classes = new array list ( ) ; if ( key . contains ( str ) ) { string [ ] interfaces = key . split ( str ) ; interface classes . add all ( arrays . as list ( interfaces ) ) ; } else { interface classes . add ( key ) ; }	For each singleton definition, create object for the interface with the implementation class,and push it into the service map with key and implemented object.
public static < t > t get bean ( class < t > interface class , class type class ) { object object = service map . get ( interface class . get name ( ) + str + type class . get name ( ) + str ) ; if ( object == null ) return null ; if ( object instanceof object [ ] ) { return ( t ) array . get ( object , num ) ; } else { return ( t ) object ; } }	Get a cached singleton object from service map by interface class and generic type class.The serviceMap is constructed from service.yml which defines interface and generic typeto implementation mapping.
public static < t > t [ ] get beans ( class < t > interface class ) { object object = service map . get ( interface class . get name ( ) ) ; if ( object == null ) return null ; if ( object instanceof object [ ] ) { return ( t [ ] ) object ; } else { object array = array . new instance ( interface class , num ) ; array . set ( array , num , object ) ; return ( t [ ] ) array ; } }	Get a list of cached singleton objects from service map by interface class.
public byte [ ] to byte array ( ) { final byte [ ] b = new byte [ this . len ] ; if ( this . len > num ) { system . arraycopy ( this . array , num , b , num , this . len ) ; } return b ; }	Converts the content of this buffer to an array of bytes.
public string get last path segment ( ) { if ( string utils . is blank ( path ) ) { return string utils . empty ; } string segment = path ; segment = string utils . substring after last ( segment , str ) ; return segment ; }	Gets the last URL path segment without the query string.If there are segment to return,an empty string will be returned instead.
public boolean is port default ( ) { return ( protocol https . equals ignore case ( protocol ) && port == default https port ) || ( protocol http . equals ignore case ( protocol ) && port == default http port ) ; }	Whether this URL uses the default port for the protocol.
public static string to absolute ( string base url , string relative url ) { string rel url = relative url ;	Converts a relative URL to an absolute one, based on the suppliedbase URL.
public static string generate random code verifier ( secure random entropy source , int entropy bytes ) { byte [ ] random bytes = new byte [ entropy bytes ] ; entropy source . next bytes ( random bytes ) ; return base64 . get url encoder ( ) . without padding ( ) . encode to string ( random bytes ) ; }	Generates a random code verifier string using the provided entropy source and the specifiednumber of bytes of entropy.
public static void add providers to path handler ( path resource provider [ ] path resource providers , path handler path handler ) { if ( path resource providers != null && path resource providers . length > num ) { for ( path resource provider path resource provider : path resource providers ) { if ( path resource provider . is prefix path ( ) ) { path handler . add prefix path ( path resource provider . get path ( ) , new resource handler ( path resource provider . get resource manager ( ) ) ) ; } else { path handler . add exact path ( path resource provider . get path ( ) , new resource handler ( path resource provider . get resource manager ( ) ) ) ; } } } }	Helper to add given PathResourceProviders to a PathHandler.
public static list < predicated handler > get predicated handlers ( predicated handlers provider [ ] predicated handlers providers ) { list < predicated handler > predicated handlers = new array list < > ( ) ; if ( predicated handlers providers != null && predicated handlers providers . length > num ) { for ( predicated handlers provider predicated handlers provider : predicated handlers providers ) { predicated handlers . add all ( predicated handlers provider . get predicated handlers ( ) ) ; } } return predicated handlers ; }	Helper for retrieving all PredicatedHandlers from the given list of PredicatedHandlersProviders.
public static boolean is resource path ( string request path , path resource provider [ ] path resource providers ) { boolean is resource path = bool ; if ( path resource providers != null && path resource providers . length > num ) { for ( path resource provider path resource provider : path resource providers ) { if ( ( path resource provider . is prefix path ( ) && request path . starts with ( path resource provider . get path ( ) ) ) || ( ! path resource provider . is prefix path ( ) && request path . equals ( path resource provider . get path ( ) ) ) ) { is resource path = bool ; } } } return is resource path ; }	Helper to check if a given requestPath could resolve to a PathResourceProvider.
public static result < token response > get token result ( token request token request , string env tag ) { final atomic reference < result < token response > > reference = new atomic reference < > ( ) ; final http2 client client = http2 client . get instance ( ) ; final count down latch latch = new count down latch ( num ) ; final client connection connection ; try { if ( token request . get server url ( ) != null ) { connection = client . connect ( new uri ( token request . get server url ( ) ) , http2 client . worker , http2 client . ssl , http2 client . buffer pool , token request . enable http2 ? option map . create ( undertow options . enable htt , bool ) : option map . empty ) . get ( ) ; } else if ( token request . get service id ( ) != null ) { cluster cluster = singleton service factory . get bean ( cluster . class ) ; string url = cluster . service to url ( str , token request . get service id ( ) , env tag , null ) ; connection = client . connect ( new uri ( url ) , http2 client . worker , http2 client . ssl , http2 client . buffer pool , token request . enable http2 ? option map . create ( undertow options . enable htt , bool ) : option map . empty ) . get ( ) ; } else {	Get an access token from the token service.
public static string get key ( key request key request , string env tag ) throws client exception { final http2 client client = http2 client . get instance ( ) ; final count down latch latch = new count down latch ( num ) ; final client connection connection ; try { if ( key request . get server url ( ) != null ) { connection = client . connect ( new uri ( key request . get server url ( ) ) , http2 client . worker , http2 client . ssl , http2 client . buffer pool , key request . enable http2 ? option map . create ( undertow options . enable htt , bool ) : option map . empty ) . get ( ) ; } else if ( key request . get service id ( ) != null ) { cluster cluster = singleton service factory . get bean ( cluster . class ) ; string url = cluster . service to url ( str , key request . get service id ( ) , env tag , null ) ; connection = client . connect ( new uri ( url ) , http2 client . worker , http2 client . ssl , http2 client . buffer pool , key request . enable http2 ? option map . create ( undertow options . enable htt , bool ) : option map . empty ) . get ( ) ; } else {	Get the certificate from key distribution service of OAuth 2.0 provider with the kid.
private static result < jwt > renew cc ( final jwt jwt ) {	renew Client Credential token synchronously.When success will renew the Jwt jwt passed in.When fail will return Status code so that can be handled by caller.
private static void renew cc ( final jwt jwt ) {	renew the given Jwt jwt asynchronously.When fail, it will swallow the exception, so no need return type to be handled by caller.
private static result < jwt > get cc ( final jwt jwt ) { token request token request = new client credentials request ( ) ;	get Client Credential token from auth server.
private static string escape xml ( string non escaped xml str ) { string builder escaped xml = new string builder ( ) ; for ( int i = num ; i < non escaped xml str . length ( ) ; i ++ ) { char c = non escaped xml str . char at ( i ) ; switch ( c ) { case str : escaped xml . append ( str ) ; break ; case str : escaped xml . append ( str ) ; break ; case str : escaped xml . append ( str ) ; break ; case str : escaped xml . append ( str ) ; break ; case str : escaped xml . append ( str ) ; break ; default : if ( c > num ) { escaped xml . append ( str + ( ( int ) c ) + str ) ; } else { escaped xml . append ( c ) ; } } } return escaped xml . to string ( ) ; }	Instead of including a large library just for escaping xml, using this util.it should be used in very rare cases because the server should not return xml format message.
public static void adjust no chunked encoding ( client request request , string request body ) { string fixed length string = request . get request headers ( ) . get first ( headers . content length ) ; string transfer encoding string = request . get request headers ( ) . get last ( headers . transfer encoding ) ; if ( transfer encoding string != null ) { request . get request headers ( ) . remove ( headers . transfer encoding ) ; }	this method is to support sending a server which doesn't support chunked transfer encoding.
private void attach form data body ( final http server exchange exchange ) throws io { object data ; form parser factory form parser factory = form parser factory . builder ( ) . build ( ) ; form data parser parser = form parser factory . create parser ( exchange ) ; if ( parser != null ) { form data form data = parser . parse blocking ( ) ; data = body converter . convert ( form data ) ; exchange . put attachment ( request body , data ) ; } }	Method used to parse the body into FormData and attach it into exchange.
private void attach json body ( final http server exchange exchange , string string ) throws io { object body ; if ( string != null ) { string = string . trim ( ) ; if ( string . starts with ( str ) ) { body = config . get instance ( ) . get mapper ( ) . read value ( string , new type reference < map < string , object > > ( ) { } ) ; } else if ( string . starts with ( str ) ) { body = config . get instance ( ) . get mapper ( ) . read value ( string , new type reference < list < object > > ( ) { } ) ; } else {	Method used to parse the body into a Map or a List and attach it into exchange.
public static string mask string ( string input , string key ) { string output = input ; map < string , object > string config = ( map < string , object > ) config . get ( mask type string ) ; if ( string config != null ) { map < string , object > key config = ( map < string , object > ) string config . get ( key ) ; if ( key config != null ) { set < string > patterns = key config . key set ( ) ; for ( string pattern : patterns ) { output = output . replace all ( pattern , ( string ) key config . get ( pattern ) ) ; } } } return output ; }	Mask the input string with a list of patterns indexed by key in string section in mask.jsonThis is usually used to mask header values, query parameters and uri parameters.
public static string mask json ( string input , string key ) { document context ctx = json path . parse ( input ) ; return mask json ( ctx , key ) ; }	Replace values in JSON using json path.
public void skip white space ( final char sequence buf , final parser cursor cursor ) { args . not null ( buf , str ) ; args . not null ( cursor , str ) ; int pos = cursor . get pos ( ) ; final int index from = cursor . get pos ( ) ; final int index to = cursor . get upper bound ( ) ; for ( int i = index from ; i < index to ; i ++ ) { final char current = buf . char at ( i ) ; if ( ! is whitespace ( current ) ) { break ; } pos ++ ; } cursor . update pos ( pos ) ; }	Skips semantically insignificant whitespace characters and moves the cursor to the closestnon-whitespace character.
public void copy content ( final char sequence buf , final parser cursor cursor , final bit set delimiters , final string builder dst ) { args . not null ( buf , str ) ; args . not null ( cursor , str ) ; args . not null ( dst , str ) ; int pos = cursor . get pos ( ) ; final int index from = cursor . get pos ( ) ; final int index to = cursor . get upper bound ( ) ; for ( int i = index from ; i < index to ; i ++ ) { final char current = buf . char at ( i ) ; if ( ( delimiters != null && delimiters . get ( current ) ) || is whitespace ( current ) ) { break ; } pos ++ ; dst . append ( current ) ; } cursor . update pos ( pos ) ; }	Transfers content into the destination buffer until a whitespace character or any ofthe given delimiters is encountered.
public void copy quoted content ( final char sequence buf , final parser cursor cursor , final string builder dst ) { args . not null ( buf , str ) ; args . not null ( cursor , str ) ; args . not null ( dst , str ) ; if ( cursor . at end ( ) ) { return ; } int pos = cursor . get pos ( ) ; int index from = cursor . get pos ( ) ; final int index to = cursor . get upper bound ( ) ; char current = buf . char at ( pos ) ; if ( current != dquote ) { return ; } pos ++ ; index from ++ ; boolean escaped = bool ; for ( int i = index from ; i < index to ; i ++ , pos ++ ) { current = buf . char at ( i ) ; if ( escaped ) { if ( current != dquote && current != escape ) { dst . append ( escape ) ; } dst . append ( current ) ; escaped = bool ; } else { if ( current == dquote ) { pos ++ ; break ; } if ( current == escape ) { escaped = bool ; } else if ( current != cr && current != lf ) { dst . append ( current ) ; } } } cursor . update pos ( pos ) ; }	Transfers content enclosed with quote marks into the destination buffer.
static void log result ( map < string , object > result , dump config config ) { consumer < string > logger func = get logger func based on level ( config . get log level ( ) ) ; if ( config . is use json ( ) ) { log result using json ( result , logger func ) ; } else { int start level = - num ; string builder sb = new string builder ( str ) ; log result ( result , start level , config . get indent size ( ) , sb ) ; logger func . accept ( sb . to string ( ) ) ; } }	A help method to log result pojo.
private static < t > void log result ( t result , int level , int indent size , string builder info ) { if ( result instanceof map ) { level += num ; int final level = level ; ( ( map ) result ) . for each ( ( k , v ) -> { info . append ( str ) ; info . append ( get tab based on level ( final level , indent size ) ) . append ( k . to string ( ) ) . append ( str ) ; log result ( v , final level , indent size , info ) ; } ) ; } else if ( result instanceof list ) { int final level = level ; ( ( list ) result ) . for each ( element -> log result ( element , final level , indent size , info ) ) ; } else if ( result instanceof string ) { info . append ( str ) . append ( result ) ; } else if ( result != null ) { try { logger . warn ( get tab based on level ( level , indent size ) + str , result ) ; } catch ( exception e ) { logger . error ( str , result . get class ( ) . get type name ( ) ) ; } } }	this method actually append result to result string.
private static string get tab based on level ( int level , int indent size ) { string builder sb = new string builder ( ) ; for ( int i = num ; i < level ; i ++ ) { for ( int j = num ; j < indent size ; j ++ ) { sb . append ( str ) ; } } return sb . to string ( ) ; }	calculate indent for formatting.
private url remove unnecessary parmas ( url url ) { url . get parameters ( ) . remove ( url . codec . get name ( ) ) ; return url ; }	client doesn't need to know codec.
@ override public void dump response ( map < string , object > result ) { this . status code result = string . value of ( exchange . get status code ( ) ) ; this . put dump info to ( result ) ; }	impl of dumping response status code to result.
@ override protected void put dump info to ( map < string , object > result ) { if ( string utils . is not blank ( this . status code result ) ) { result . put ( dump constants . status code , this . status code result ) ; } }	put this.statusCodeResult to result.
public static string get uuid ( ) { uuid id = uuid . random uuid ( ) ; byte buffer bb = byte buffer . wrap ( new byte [ num ] ) ; bb . put long ( id . get most significant bits ( ) ) ; bb . put long ( id . get least significant bits ( ) ) ; return base64 . encode base64 url ( bb . array ( ) ) ; }	Generate UUID across the entire app and it is used for correlationId.
public static string quote ( final string value ) { if ( value == null ) { return null ; } string result = value ; if ( ! result . starts with ( str ) ) { result = str + result ; } if ( ! result . ends with ( str ) ) { result = result + str ; } return result ; }	Quote the given string if needed.
@ override public string service to url ( string protocol , string service id , string tag , string request key ) { url url = load balance . select ( discovery ( protocol , service id , tag ) , request key ) ; if ( logger . is debug enabled ( ) ) logger . debug ( str + url ) ;	Implement serviceToUrl with client side service discovery.
private string get server tls finger print ( ) { string finger print = null ; map < string , object > server config = config . get instance ( ) . get json map config no cache ( str ) ; map < string , object > secret config = config . get instance ( ) . get json map config no cache ( str ) ;	We can get it from server module but we don't want mutual dependency. Soget it from config and keystore directly.
private void do custom server identity check ( x509 certificate cert ) throws certificate exception { if ( endpoint identification algorithm . apis == identity alg ) { api . verify and throw ( trusted name set , cert ) ; } }	check server identify as per tls.trustedNames in client.yml.Notes: this method should only be applied to verify server certificates on the client side.
private void check identity ( ssl session , x509 certificate cert ) throws certificate exception { if ( session == null ) { throw new certificate exception ( str ) ; } if ( endpoint identification algorithm . https == identity alg ) { string hostname = session . get peer host ( ) ; api . verify and throw ( hostname , cert ) ; } }	check server identify against hostnames.
public static trust manager [ ] decorate ( trust manager [ ] trust managers , tls tls config ) { if ( null != trust managers && trust managers . length > num ) { trust manager [ ] decorated trust managers = new trust manager [ trust managers . length ] ; for ( int i = num ; i < trust managers . length ; ++ i ) { trust manager trust manager = trust managers [ i ] ; if ( trust manager instanceof x509 trust manager ) { decorated trust managers [ i ] = new client x509 extended trust manager ( ( x509 trust manager ) trust manager , tls config ) ; } else { decorated trust managers [ i ] = trust manager ; } } return decorated trust managers ; } return trust managers ; }	This method converts existing X509TrustManagers to ClientX509ExtendedTrustManagers.
@ override protected void put dump info to ( map < string , object > result ) { if ( string utils . is not blank ( this . url ) ) { result . put ( dump constants . url , this . url ) ; } }	put this.url to result.
@ override public void handle request ( final http server exchange exchange ) throws exception {	Check iterate the configuration on both request and response section and updateheaders accordingly.
public static string get host name ( socket address socket address ) { if ( socket address == null ) { return null ; } if ( socket address instanceof inet socket address ) { inet address addr = ( ( inet socket address ) socket address ) . get address ( ) ; if ( addr != null ) { return addr . get host address ( ) ; } } return null ; }	return ip to avoid lookup dns.
public static object merge object ( object config , class clazz ) { merge ( config ) ; return convert map to obj ( ( map < string , object > ) config , clazz ) ; }	Merge map config with values generated by ConfigInjection.class and return mapping object.
private static void merge ( object m1 ) { if ( m1 instanceof map ) { iterator < object > field names = ( ( map < object , object > ) m1 ) . key set ( ) . iterator ( ) ; string field name = null ; while ( field names . has next ( ) ) { field name = string . value of ( field names . next ( ) ) ; object field1 = ( ( map < string , object > ) m1 ) . get ( field name ) ; if ( field1 != null ) { if ( field1 instanceof map || field1 instanceof list ) { merge ( field1 ) ;	Search the config map recursively, expand List and Map level by level util no further expand.
private static object convert map to obj ( map < string , object > map , class clazz ) { object mapper mapper = new object mapper ( ) ; object obj = mapper . convert value ( map , clazz ) ; return obj ; }	Method used to convert map to object based on the reference class provided.
@ override public double get mean ( ) { if ( values . length == num ) { return num ; } double sum = num ; for ( long value : values ) { sum += value ; } return sum / values . length ; }	Returns the arithmetic mean of the values in the snapshot.
@ override public double get std dev ( ) {	Returns the standard deviation of the values in the snapshot.
@ override public void dump ( output stream output ) { try ( print writer out = new print writer ( new output stream writer ( output , utf 8 ) ) ) { for ( long value : values ) { out . printf ( str , value ) ; } } }	Writes the values of the snapshot to the given stream.
public static object construct by named params ( class clazz , map params ) throws exception { object obj = clazz . get constructor ( ) . new instance ( ) ; method [ ] all methods = clazz . get methods ( ) ; for ( method method : all methods ) { if ( method . get name ( ) . starts with ( str ) ) { object [ ] o = new object [ num ] ; string property name = introspector . decapitalize ( method . get name ( ) . substring ( num ) ) ; if ( params . contains key ( property name ) ) { o [ num ] = params . get ( property name ) ; method . invoke ( obj , o ) ; } } } return obj ; }	Build an object out of a given class and a map for field names to values.
public static object construct by parameterized constructor ( class clazz , list parameters ) throws exception {	Build an object out of a given class and a list of single element maps of object type to value.A constructor is searched for that matches the given set.
public static metric name name ( string name , string ... names ) { final int length ; if ( names == null ) { length = num ; } else { length = names . length ; } final string [ ] parts = new string [ length + num ] ; parts [ num ] = name ; system . arraycopy ( names , num , parts , num , length ) ; return metric name . build ( parts ) ; }	Shorthand method for backwards compatibility in creating metric names.Uses {.
public void remove all ( ) { for ( iterator < map . entry < metric name , metric > > it = metrics . entry set ( ) . iterator ( ) ; it . has next ( ) ; ) { map . entry < metric name , metric > entry = it . next ( ) ; metric metric = entry . get value ( ) ; if ( metric != null ) { on metric removed ( entry . get key ( ) , metric ) ; } it . remove ( ) ; } }	Removes all the metrics in registry.
public void stop ( ) { executor . shutdown ( ) ;	Stops the reporter and shuts down its thread of execution.Uses the shutdown pattern from http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ExecutorService.html.
public void report ( ) { synchronized ( this ) { report ( registry . get gauges ( filter ) , registry . get counters ( filter ) , registry . get histograms ( filter ) , registry . get meters ( filter ) , registry . get timers ( filter ) ) ; } }	Report the current values of all metrics in the registry.
private static void server option init ( ) { map < string , object > map config = config . get instance ( ) . get json map config no cache ( server config name ) ; server option . server option init ( map config , get server config ( ) ) ; }	Method used to initialize server options. If the user has configured a valid server option,load it into the server configuration, otherwise use the default value.
static public void shutdown ( ) {	implement shutdown hook here.
protected static void merge status config ( ) { map < string , object > app status config = config . get instance ( ) . get json map config no cache ( status config name [ num ] ) ; if ( app status config == null ) { return ; } map < string , object > status config = config . get instance ( ) . get json map config ( status config name [ num ] ) ;	method used to merge status.yml and app-status.yml.
public static url register ( string service id , int port ) { try { registry = singleton service factory . get bean ( registry . class ) ; if ( registry == null ) throw new runtime exception ( str ) ;	Register the service to the Consul or other service registry.
public void append ( final string str ) { final string s = str != null ? str : str ; final int strlen = s . length ( ) ; final int newlen = this . len + strlen ; if ( newlen > this . array . length ) { expand ( newlen ) ; } s . get chars ( num , strlen , this . array , this . len ) ; this . len = newlen ; }	Appends chars of the given string to this buffer.
public char [ ] to char array ( ) { final char [ ] b = new char [ this . len ] ; if ( this . len > num ) { system . arraycopy ( this . array , num , b , num , this . len ) ; } return b ; }	Converts the content of this buffer to an array of chars.
@ override protected void put dump info to ( map < string , object > result ) { if ( string utils . is not blank ( this . body content ) ) { result . put ( dump constants . body , this . body content ) ; } }	put bodyContent to result.
@ override public void dump request ( map < string , object > result ) { string content type = exchange . get request headers ( ) . get first ( headers . content type ) ;	impl of dumping request body to result.
@ override public void dump response ( map < string , object > result ) { byte [ ] response body attachment = exchange . get attachment ( store response stream sink conduit . response ) ; if ( response body attachment != null ) { this . body content = config . is mask enabled ( ) ? mask . mask json ( new byte array input stream ( response body attachment ) , str ) : new string ( response body attachment , utf 8 ) ; } this . put dump info to ( result ) ; }	impl of dumping response body to result.
private void dump input stream ( ) {	read from input stream, convert it to string, put into this.bodyContent.
private void dump body attachment ( object request body attachment ) { this . body content = config . is mask enabled ( ) ? mask . mask json ( request body attachment , str ) : request body attachment . to string ( ) ; }	read from body attachment from Body Handler, convert it to string, put into this.bodyContent.
static public x509 certificate read certificate ( string filename ) throws exception { input stream in stream = null ; x509 certificate cert = null ; try { in stream = config . get instance ( ) . get input stream from file ( filename ) ; if ( in stream != null ) { certificate factory cf = certificate factory . get instance ( str ) ; cert = ( x509 certificate ) cf . generate certificate ( in stream ) ; } else { logger . info ( str + encode . for java ( filename ) + str ) ; } } catch ( exception e ) { logger . error ( str , e ) ; } finally { if ( in stream != null ) { try { in stream . close ( ) ; } catch ( io ioe ) { logger . error ( str , ioe ) ; } } } return cert ; }	Read certificate from a file and convert it into X509Certificate object.
public static string get jwt from authorization ( string authorization ) { string jwt = null ; if ( authorization != null ) { string [ ] parts = authorization . split ( str ) ; if ( parts . length == num ) { string scheme = parts [ num ] ; string credentials = parts [ num ] ; pattern pattern = pattern . compile ( str , pattern . case insensitive ) ; if ( pattern . matcher ( scheme ) . matches ( ) ) { jwt = credentials ; } } } return jwt ; }	Parse the jwt token from Authorization header.
public static string input stream to string ( input stream input stream , charset charset ) throws io { if ( input stream != null && input stream . available ( ) != - num ) { byte array output stream result = new byte array output stream ( ) ; byte [ ] buffer = new byte [ num ] ; int length ; while ( ( length = input stream . read ( buffer ) ) != - num ) { result . write ( buffer , num , length ) ; } if ( charset != null ) { return result . to string ( charset . name ( ) ) ; } return result . to string ( standard charsets . utf 8 . name ( ) ) ; } return null ; }	Convert an InputStream into a String.Highest performing conversion per: https://stackoverflow.com/a/35446009.
@ override public void dump request ( map < string , object > result ) { map < string , cookie > cookies map = exchange . get request cookies ( ) ; dump cookies ( cookies map , str ) ; this . put dump info to ( result ) ; }	impl of dumping request cookies to result.
@ override public void dump response ( map < string , object > result ) { map < string , cookie > cookies map = exchange . get response cookies ( ) ; dump cookies ( cookies map , str ) ; this . put dump info to ( result ) ; }	impl of dumping response cookies to result.
private void dump cookies ( map < string , cookie > cookies map , string mask key ) { cookies map . for each ( ( key , cookie ) -> { if ( ! config . get request filtered cookies ( ) . contains ( cookie . get name ( ) ) ) { list < map < string , string > > cookie info list = new array list < > ( ) ;	put cookies info to cookieMap.
@ override protected void put dump info to ( map < string , object > result ) { if ( this . cookie map . size ( ) > num ) { result . put ( dump constants . cookies , cookie map ) ; } }	put cookieMap to result.
public static boolean is same ( list < url > urls1 , list < url > urls2 ) { if ( urls1 == null || urls2 == null ) { return bool ; } if ( urls1 . size ( ) != urls2 . size ( ) ) { return bool ; } return urls1 . contains all ( urls2 ) ; }	Check if two lists have the same urls. If any list is empty, return false.
public static consul service build service ( url url ) { consul service service = new consul service ( ) ; service . set address ( url . get host ( ) ) ; service . set id ( consul utils . convert consul serivce id ( url ) ) ; service . set name ( url . get path ( ) ) ; service . set port ( url . get port ( ) ) ; list < string > tags = new array list < string > ( ) ; string env = url . get parameter ( constants . tag environment ) ; if ( env != null ) tags . add ( env ) ; service . set tags ( tags ) ; return service ; }	build consul service from url.
public static url build url ( consul service service ) { url url = null ; if ( url == null ) { map < string , string > params = new hash map < string , string > ( ) ;	build url from service.
public static string get path from service id ( string service id ) { return service id . substring ( service id . index of ( str ) + num , service id . last index of ( str ) ) ; }	get path of url from service id in consul.
public static string get jwt ( jwt claims claims ) throws jose exception { string jwt ; rsa private key = ( rsa ) get private key ( jwt config . get key ( ) . get filename ( ) , ( string ) secret config . get ( jwt private key password ) , jwt config . get key ( ) . get key name ( ) ) ;	A static method that generate JWT token from JWT claims object.
private static private key get private key ( string filename , string password , string key ) { if ( logger . is debug enabled ( ) ) logger . debug ( str + filename + str + key ) ; private key private key = null ; try { key store keystore = key store . get instance ( str ) ; keystore . load ( config . get instance ( ) . get input stream from file ( filename ) , password . to char array ( ) ) ; private key = ( private key ) keystore . get key ( key , password . to char array ( ) ) ; } catch ( exception e ) { logger . error ( str , e ) ; } if ( private key == null ) { logger . error ( str ) ; } return private key ; }	Get private key from java key store.
public static string to time precision ( final time unit t ) { switch ( t ) { case hours : return str ; case minutes : return str ; case seconds : return str ; case milliseconds : return str ; case microseconds : return str ; case nanoseconds : return str ; default : enum set < time unit > allowed timeunits = enum set . of ( time unit . hours , time unit . minutes , time unit . seconds , time unit . milliseconds , time unit . microseconds , time unit . nanoseconds ) ; throw new illegal argument exception ( str + allowed timeunits ) ; } }	Convert from a TimeUnit to a influxDB timeunit String.
private synchronized jwt get jwt ( i cache strategy , jwt . key key ) { jwt result = cache strategy . get cached jwt ( key ) ; if ( result == null ) {	cache jwt if not exist.
private boolean is switcher change ( boolean switcher status ) { boolean ret = bool ; if ( switcher status != last heart beat switcher status ) { ret = bool ; last heart beat switcher status = switcher status ; logger . info ( str + switcher status ) ; } return ret ; }	check heart beat switcher status, if switcher is changed, then change lastHeartBeatSwitcherStatusto the latest status.
@ override public string get identity ( ) { return protocol + constants . protocol separator + host + str + port + str + get parameter ( url . group . get name ( ) , url . group . get value ( ) ) + str + get path ( ) + str + get parameter ( url . version . get name ( ) , url . version . get value ( ) ) + str + get parameter ( url . node type . get name ( ) , url . node type . get value ( ) ) ; }	Return service identity, if two urls have the same identity, then same service.
@ override public boolean can serve ( url ref url ) { if ( ref url == null || ! this . get path ( ) . equals ( ref url . get path ( ) ) ) { return bool ; } if ( ! protocol . equals ( ref url . get protocol ( ) ) ) { return bool ; } if ( ! constants . node type service . equals ( this . get parameter ( url . node type . get name ( ) ) ) ) { return bool ; } string version = get parameter ( url . version . get name ( ) , url . version . get value ( ) ) ; string ref version = ref url . get parameter ( url . version . get name ( ) , url . version . get value ( ) ) ; if ( ! version . equals ( ref version ) ) { return bool ; }	check if this url can serve the refUrl.
public metric name resolve ( string p ) { final string next ; if ( p != null && ! p . is empty ( ) ) { if ( key != null && ! key . is empty ( ) ) { next = key + separator + p ; } else { next = p ; } } else { next = this . key ; } return new metric name ( next , tags ) ; }	Build the MetricName that is this with another path appended to it.The new MetricName inherits the tags of this one.
public metric name tagged ( map < string , string > add ) { final map < string , string > tags = new hash map < > ( add ) ; tags . put all ( this . tags ) ; return new metric name ( key , tags ) ; }	Add tags to a metric name and return the newly created MetricName.
public static metric name join ( metric name ... parts ) { final string builder name builder = new string builder ( ) ; final map < string , string > tags = new hash map < > ( ) ; boolean first = bool ; for ( metric name part : parts ) { final string name = part . get key ( ) ; if ( name != null && ! name . is empty ( ) ) { if ( first ) { first = bool ; } else { name builder . append ( separator ) ; } name builder . append ( name ) ; } if ( ! part . get tags ( ) . is empty ( ) ) tags . put all ( part . get tags ( ) ) ; } return new metric name ( name builder . to string ( ) , tags ) ; }	Join the specified set of metric names.
public static metric name build ( string ... parts ) { if ( parts == null || parts . length == num ) return metric name . empty ; if ( parts . length == num ) return new metric name ( parts [ num ] , empty tags ) ; return new metric name ( build name ( parts ) , empty tags ) ; }	Build a new metric name using the specific path components.
static void init paths ( ) { if ( config != null && config . get paths ( ) != null ) { for ( path chain path chain : config . get paths ( ) ) { path chain . validate ( config name + str ) ;	Build "handlerListById" and "reqTypeMatcherMap" from the paths in the config.
static void init default handlers ( ) { if ( config != null && config . get default handlers ( ) != null ) { default handlers = get handlers from exec list ( config . get default handlers ( ) ) ; handler list by id . put ( str , default handlers ) ; } }	Build "defaultHandlers" from the defaultHandlers in the config.
private static void add source chain ( path chain source chain ) { try { class source class = class . for name ( source chain . get source ( ) ) ; endpoint source source = ( endpoint source ) source class . new instance ( ) ; for ( endpoint source . endpoint endpoint : source . list endpoints ( ) ) { path chain sourced path = new path chain ( ) ; sourced path . set path ( endpoint . get path ( ) ) ; sourced path . set method ( endpoint . get method ( ) ) ; sourced path . set exec ( source chain . get exec ( ) ) ; sourced path . validate ( source chain . get source ( ) ) ; add path chain ( sourced path ) ; } } catch ( exception e ) { logger . error ( str + source chain ) ; if ( e instanceof runtime exception ) { throw ( runtime exception ) e ; } else { throw new runtime exception ( e ) ; } } }	Add PathChains crated from the EndpointSource given in sourceChain.
public static void next ( http server exchange http server exchange ) throws exception { http handler http handler = get next ( http server exchange ) ; if ( http handler != null ) { http handler . handle request ( http server exchange ) ; } else if ( last handler != null ) { last handler . handle request ( http server exchange ) ; } }	Handle the next request in the chain.
public static void next ( http server exchange http server exchange , http handler next ) throws exception { if ( next != null ) { next . handle request ( http server exchange ) ; } else { next ( http server exchange ) ; } }	Go to the next handler if the given next is none null.
public static void next ( http server exchange http server exchange , string exec name , boolean return to orig flow ) throws exception { string current chain id = http server exchange . get attachment ( chain id ) ; integer current next index = http server exchange . get attachment ( chain seq ) ; http server exchange . put attachment ( chain id , exec name ) ; http server exchange . put attachment ( chain seq , num ) ; next ( http server exchange ) ;	Allow nexting directly to a flow.
public static http handler get next ( http server exchange http server exchange ) { string chain id = http server exchange . get attachment ( chain id ) ; list < http handler > handlers for id = handler list by id . get ( chain id ) ; integer next index = http server exchange . get attachment ( chain seq ) ;	Returns the instance of the next handler, rather then calling handleRequeston it.
public static http handler get next ( http server exchange http server exchange , http handler next ) throws exception { if ( next != null ) { return next ; } return get next ( http server exchange ) ; }	Returns the instance of the next handler, or the given next param if it's notnull.
public static boolean start ( http server exchange http server exchange ) {	On the first step of the request, match the request against the configuredpaths.
public static boolean start default handlers ( http server exchange http server exchange ) {	If there is no matching path, the OrchestrationHandler is going to try to start the defaultHandlers.If there are default handlers defined, store the chain id within the exchange.Otherwise return false.
private static list < http handler > get handlers from exec list ( list < string > execs ) { list < http handler > handlers from exec list = new array list < > ( ) ; if ( execs != null ) { for ( string exec : execs ) { list < http handler > handler list = handler list by id . get ( exec ) ; if ( handler list == null ) throw new runtime exception ( str + exec ) ; for ( http handler handler : handler list ) { if ( handler instanceof middleware handler ) {	Converts the list of chains and handlers to a flat list of handlers.
private static void register middleware handler ( object handler ) { if ( handler instanceof middleware handler ) {	Detect if the handler is a MiddlewareHandler instance.
private static void init map defined handler ( map < string , object > handler ) {	Helper method for generating the instance of a handler from its mapdefinition in config.
static tuple < string , class > split class and name ( string class label ) { string [ ] string name split = class label . split ( str ) ;	To support multiple instances of the same class, support a naming.
static void set config ( string config name ) throws exception { handler . config name = config name ; config = ( handler config ) config . get instance ( ) . get json object config ( config name , handler config . class ) ; init handlers ( ) ; init paths ( ) ; }	Exposed for testing only.
public i get composer ( client request composers composer name ) { i composer = composers map . get ( composer name ) ; if ( composer == null ) { init default composer ( composer name ) ; } return composers map . get ( composer name ) ; }	get IClientRequestComposable based on ClientRequestComposers composer name.
@ override public double get mean ( ) { if ( values . length == num ) { return num ; } double sum = num ; for ( int i = num ; i < values . length ; i ++ ) { sum += values [ i ] * norm weights [ i ] ; } return sum ; }	Returns the weighted arithmetic mean of the values in the snapshot.
@ override public double get std dev ( ) {	Returns the weighted standard deviation of the values in the snapshot.
public void validate ( string origin ) { list < string > problems = new array list < > ( ) ; if ( source == null ) { if ( path == null ) { problems . add ( str ) ; } else if ( method == null ) { problems . add ( str + path ) ; } } else { if ( path != null ) { problems . add ( str + source + str + path ) ; } if ( method != null ) { problems . add ( str + source + str + method ) ; } } if ( method != null && ! util . methods . contains ( method . to upper case ( ) ) ) { problems . add ( str + method ) ; } if ( ! problems . is empty ( ) ) { throw new runtime exception ( str + origin + str + string . join ( str , problems ) + str ) ; } }	Validate the settings and raise Exception on error.The origin is used to help locate problems.
private void start listener thread if new service ( url url ) { string service name = url . get path ( ) ; if ( ! lookup services . contains key ( service name ) ) { long value = lookup services . put if absent ( service name , num ) ; if ( value == null ) { service lookup thread lookup thread = new service lookup thread ( service name ) ; lookup thread . set daemon ( bool ) ; lookup thread . start ( ) ; } } }	if new service registered, start a new lookup threadeach serviceName start a lookup thread to discover service.
private consul response < list < consul service > > lookup consul service ( string service name , long last consul index id ) { consul response < list < consul service > > response = client . lookup health service ( service name , null , last consul index id , get consul token ( ) ) ; return response ; }	directly fetch consul service data.
private void update service cache ( string service name , concurrent hash map < string , list < url > > service urls , boolean need notify ) { if ( service urls != null && ! service urls . is empty ( ) ) { list < url > urls = service cache . get ( service name ) ; if ( urls == null ) { if ( logger . is debug enabled ( ) ) { try { logger . debug ( str + config . get instance ( ) . get mapper ( ) . write value as string ( service urls ) ) ; } catch ( exception e ) { } } service cache . put ( service name , service urls . get ( service name ) ) ; } for ( map . entry < string , list < url > > entry : service urls . entry set ( ) ) { boolean change = bool ; if ( urls != null ) { list < url > new urls = entry . get value ( ) ; if ( new urls == null || new urls . is empty ( ) || consul utils . is same ( new urls , urls ) ) { change = bool ; } else { service cache . put ( service name , new urls ) ; } } if ( change && need notify ) { notify executor . execute ( new notify service ( entry . get key ( ) , entry . get value ( ) ) ) ; logger . info ( str + entry . get key ( ) ) ; string builder sb = new string builder ( ) ; for ( url url : entry . get value ( ) ) { sb . append ( url . get uri ( ) ) . append ( str ) ; } logger . info ( str + sb . to string ( ) ) ; } } } }	update service cache of the serviceName.update local cache when service list changed,if need notify, notify service.
public static void verify and throw ( final set < string > name set , final x509 certificate cert ) throws certificate exception { if ( ! verify ( name set , cert ) ) { throw new certificate exception ( str + name set + str ) ; } }	Perform server identify check using given names and throw CertificateException if the check fails.
public static void verify and throw ( final string name , final x509 certificate cert ) throws certificate exception { if ( ! verify ( name , cert ) ) { throw new certificate exception ( str + name + str ) ; } }	Perform server identify check using given name and throw CertificateException if the check fails.
public void add auth token ( client request request , string token ) { if ( token != null && ! token . starts with ( str ) ) { if ( token . to upper case ( ) . starts with ( str ) ) {	Add Authorization Code grant token the caller app gets from OAuth2 server.This is the method called from client like web server.
public void add auth token trace ( client request request , string token , string traceability id ) { if ( token != null && ! token . starts with ( str ) ) { if ( token . to upper case ( ) . starts with ( str ) ) {	Add Authorization Code grant token the caller app gets from OAuth2 server and add traceabilityIdThis is the method called from client like web server that want to have traceabilityId pass through.
public result propagate headers ( client request request , final http server exchange exchange ) { string tid = exchange . get request headers ( ) . get first ( http string constants . traceability id ) ; string token = exchange . get request headers ( ) . get first ( headers . authorization ) ; string cid = exchange . get request headers ( ) . get first ( http string constants . correlation id ) ; return populate header ( request , token , cid , tid ) ; }	Support API to API calls with scope token.
public result populate header ( client request request , string auth token , string correlation id , string traceability id ) { if ( traceability id != null ) { add auth token trace ( request , auth token , traceability id ) ; } else { add auth token ( request , auth token ) ; } result < jwt > result = token manager . get jwt ( request ) ; if ( result . is failure ( ) ) { return failure . of ( result . get error ( ) ) ; } request . get request headers ( ) . put ( http string constants . correlation id , correlation id ) ; request . get request headers ( ) . put ( http string constants . scope token , str + result . get result ( ) . get jwt ( ) ) ; return result ; }	Support API to API calls with scope token.
public static ssl create ssl ( ) throws io { map < string , object > tls map = ( map < string , object > ) client config . get ( ) . get mapped config ( ) . get ( tls ) ; return null == tls map ? null : create ssl ( ( string ) tls map . get ( tls . default group key ) ) ; }	default method for creating ssl context.
public completable future < client connection > connect async ( uri uri ) { return this . connect async ( null , uri , com . networknt . client . http2 client . worker , com . networknt . client . http2 client . ssl , com . networknt . client . http2 client . buffer pool , option map . create ( undertow options . enable htt , bool ) ) ; }	Create async connection with default config value.
static char [ ] to char array ( final char sequence cs ) { if ( cs instanceof string ) { return ( ( string ) cs ) . to char array ( ) ; } final int sz = cs . length ( ) ; final char [ ] array = new char [ cs . length ( ) ] ; for ( int i = num ; i < sz ; i ++ ) { array [ i ] = cs . char at ( i ) ; } return array ; }	Green implementation of toCharArray.
static boolean region matches ( final char sequence cs , final boolean ignore case , final int this start , final char sequence substring , final int start , final int length ) { if ( cs instanceof string && substring instanceof string ) { return ( ( string ) cs ) . region matches ( ignore case , this start , ( string ) substring , start , length ) ; } int index1 = this start ; int index2 = start ; int tmp len = length ;	Green implementation of regionMatches.
private static file system create zip file system ( string zip filename , boolean create ) throws io {	Returns a zip file system.
public static void list ( string zip filename ) throws io { if ( logger . is debug enabled ( ) ) logger . debug ( str , zip filename ) ;	List the contents of the specified zip file.
public static void delete old files ( string dir path , int older than minute ) { file folder = new file ( dir path ) ; if ( folder . exists ( ) ) { file [ ] list files = folder . list files ( ) ; long eligible for deletion = system . current time millis ( ) - ( older than minute * num * num ) ; for ( file list file : list files ) { if ( list file . last modified ( ) < eligible for deletion ) { if ( ! list file . delete ( ) ) { logger . error ( str , list file ) ; } } } } }	Delele old files.
public static byte buffer to byte buffer ( string s ) { byte buffer buffer = byte buffer . allocate direct ( s . length ( ) ) ; buffer . put ( s . get bytes ( utf 8 ) ) ; buffer . flip ( ) ; return buffer ; }	convert String to ByteBuffer.
public static byte buffer to byte buffer ( file file ) { byte buffer buffer = byte buffer . allocate direct ( ( int ) file . length ( ) ) ; try { buffer . put ( to byte array ( new file input stream ( file ) ) ) ; } catch ( io e ) { logger . error ( str + e . get message ( ) ) ; } buffer . flip ( ) ; return buffer ; }	Convert a File into a ByteBuffer.
public static string get temp dir ( ) {	get temp dir from OS.
public static byte [ ] to byte array ( input stream is ) throws io { byte array output stream output = new byte array output stream ( ) ; try { byte [ ] b = new byte [ buffer size ] ; int n = num ; while ( ( n = is . read ( b ) ) != - num ) { output . write ( b , num , n ) ; } return output . to byte array ( ) ; } finally { output . close ( ) ; } }	Reads and returns the rest of the given input stream as a byte array.Caller is responsible for closing the given input stream.
public static object get inject value ( string string ) { matcher m = pattern . matcher ( string ) ; string buffer sb = new string buffer ( ) ;	Method used to generate the values from environment variables or "values.yaml".
public static boolean is exclusion config file ( string config name ) { list < object > exclusion config file list = ( exclusion map == null ) ? new array list < > ( ) : ( list < object > ) exclusion map . get ( exclusion config file list ) ; return centralized management . equals ( config name ) || scalable config . equals ( config name ) || exclusion config file list . contains ( config name ) ; }	Double check values and exclusions to ensure no dead loop.
private static object type cast ( string str ) { if ( str == null || str . equals ( str ) ) { return null ; }	Method used to cast string into int, double or boolean.
public list < i > create request dumpers ( dump config config , http server exchange exchange ) { request dumper factory factory = new request dumper factory ( ) ; list < i > dumpers = new array list < > ( ) ; for ( string dumper names : request dumpers ) { i dumper = factory . create ( dumper names , config , exchange ) ; dumpers . add ( dumper ) ; } return dumpers ; }	use RequestDumperFactory to create dumpers listed in this.requestDumpers.
public list < i > create response dumpers ( dump config config , http server exchange exchange ) { response dumper factory factory = new response dumper factory ( ) ; list < i > dumpers = new array list < > ( ) ; for ( string dumper names : response dumpers ) { i dumper = factory . create ( dumper names , config , exchange ) ; dumpers . add ( dumper ) ; } return dumpers ; }	use ResponseDumperFactory to create dumpers listed in this.responseDumpers.
public static string default origin ( http server exchange exchange ) { string host = network utils . format possible ipv6 address ( exchange . get host name ( ) ) ; string protocol = exchange . get request scheme ( ) ; int port = exchange . get host port ( ) ;	Determine the default origin, to allow for local access.
public static string sanitize default port ( string url ) { int after scheme index = url . index of ( str ) ; if ( after scheme index < num ) { return url ; } string scheme = url . substring ( num , after scheme index ) ; int from index = scheme . length ( ) + num ;	Removes the port from a URL if this port is the default one for the URL's scheme.
public void add first ( property source < ? > property source ) { if ( logger . is debug enabled ( ) ) { logger . debug ( str + property source . get name ( ) + str ) ; } remove if present ( property source ) ; this . property source list . add ( num , property source ) ; }	Add the given property source object with highest precedence.
public void add before ( string relative property source name , property source < ? > property source ) { if ( logger . is debug enabled ( ) ) { logger . debug ( str + property source . get name ( ) + str + relative property source name + str ) ; } assert legal relative addition ( relative property source name , property source ) ; remove if present ( property source ) ; int index = assert present and get index ( relative property source name ) ; add at index ( index , property source ) ; }	Add the given property source object with precedence immediately higher thanthe named relative property source.
public void replace ( string name , property source < ? > property source ) { if ( logger . is debug enabled ( ) ) { logger . debug ( str + name + str + property source . get name ( ) + str ) ; } int index = assert present and get index ( name ) ; this . property source list . set ( index , property source ) ; }	Replace the property source with the given name with the given propertysource object.
protected void assert legal relative addition ( string relative property source name , property source < ? > property source ) { string new property source name = property source . get name ( ) ; if ( relative property source name . equals ( new property source name ) ) { throw new illegal argument exception ( str + new property source name + str ) ; } }	Ensure that the given property source is not being added relative to itself.
private void add at index ( int index , property source < ? > property source ) { remove if present ( property source ) ; this . property source list . add ( index , property source ) ; }	Add the given property source at a particular index in the list.
private int assert present and get index ( string name ) { int index = this . property source list . index of ( property source . named ( name ) ) ; if ( index == - num ) { throw new illegal argument exception ( str + name + str ) ; } return index ; }	Assert that the named property source is present and return its index.
public void fail ( ) { has failure = bool ;	Countdown and fail-fast if the sub message is failed.
protected final void put int16 ( int i16 ) { ensure capacity ( position + num ) ; byte [ ] buf = buffer ; buf [ position ++ ] = ( byte ) ( i16 & num ) ; buf [ position ++ ] = ( byte ) ( i16 > > > num ) ; }	Put 16-bit integer in the buffer.
protected final void put int32 ( long i32 ) { ensure capacity ( position + num ) ; byte [ ] buf = buffer ; buf [ position ++ ] = ( byte ) ( i32 & num ) ; buf [ position ++ ] = ( byte ) ( i32 > > > num ) ; buf [ position ++ ] = ( byte ) ( i32 > > > num ) ; buf [ position ++ ] = ( byte ) ( i32 > > > num ) ; }	Put 32-bit integer in the buffer.
protected final void put string ( string s ) { ensure capacity ( position + ( s . length ( ) * num ) + num ) ; system . arraycopy ( s . get bytes ( ) , num , buffer , position , s . length ( ) ) ; position += s . length ( ) ; buffer [ position ++ ] = num ; }	Put a string in the buffer.
public static final string collate charset ( string charset ) { string [ ] output = string utils . split ( charset , str ) ; return output [ num ] . replace ( str , str ) . trim ( ) ; }	'utf8' COLLATE 'utf8_general_ci'.
public void update by query ( es config , map < string , object > params tmp , map < string , object > es field data ) { if ( params tmp . is empty ( ) ) { return ; } es mapping = config . get es mapping ( ) ; bool query builder query builder = query builders . bool query ( ) ; params tmp . for each ( ( field name , value ) -> query builder . must ( query builders . terms query ( field name , value ) ) ) ;	update by query.
public static string get charset ( final int id ) { entry entry = get entry ( id ) ; if ( entry != null ) { return entry . mysql charset ; } else { logger . warn ( str + id ) ; return null ; } }	Return defined charset name for mysql.
public static string get collation ( final int id ) { entry entry = get entry ( id ) ; if ( entry != null ) { return entry . mysql collation ; } else { logger . warn ( str + id ) ; return null ; } }	Return defined collaction name for mysql.
public static string get java charset ( final int id ) { entry entry = get entry ( id ) ; if ( entry != null ) { if ( entry . java charset != null ) { return entry . java charset ; } else { logger . warn ( str + id + str + entry . mysql charset + str + entry . mysql collation ) ; return null ; } } else { logger . warn ( str + id ) ; return null ; } }	Return converted charset name for java.
public static hash mode get partition hash columns ( string name , string pk hash configs ) { if ( string utils . is empty ( pk hash configs ) ) { return null ; } list < partition data > datas = partition datas . get ( pk hash configs ) ; for ( partition data data : datas ) { if ( data . simple name != null ) { if ( data . simple name . equals ignore case ( name ) ) { return data . hash mode ; } } else { if ( data . regex filter . filter ( name ) ) { return data . hash mode ; } } } return null ; }	match return List , not match return null.
private final void decode fields ( log buffer buffer , final int len ) { final int limit = buffer . limit ( ) ; buffer . limit ( len + buffer . position ( ) ) ; for ( int i = num ; i < column cnt ; i ++ ) { column info info = column info [ i ] ; switch ( info . type ) { case mysql type tiny blob : case mysql type blob : case mysql type medium blob : case mysql type long blob : case mysql type double : case mysql type float : case mysql type geometry : case mysql type json : info . meta = buffer . get uint8 ( ) ; break ; case mysql type set : case mysql type enum : logger . warn ( str + str + info . type ) ; break ; case mysql type string : { int x = ( buffer . get uint8 ( ) << num ) ;	Decode field metadata by column types.
public log event decode ( log buffer buffer , log context context ) throws io { final int limit = buffer . limit ( ) ; if ( limit >= format description log event . log event header len ) { log header header = new log header ( buffer , context . get format description ( ) ) ; final int len = header . get event len ( ) ; if ( limit >= len ) { log event event ; if ( handle set . get ( header . get type ( ) ) ) { buffer . limit ( len ) ; try { event = decode ( buffer , header , context ) ; } catch ( io e ) { if ( logger . is warn enabled ( ) ) { logger . warn ( str + log event . get type name ( header . get type ( ) ) + str + context . get log position ( ) , e ) ; } throw e ; } finally { buffer . limit ( limit ) ; } } else { event = new unknown log event ( header ) ; } if ( event != null ) {	Decoding an event from binary-log buffer.
public final int compare to ( string file name , final long position ) { final int val = this . file name . compare to ( file name ) ; if ( val == num ) { return ( int ) ( this . position - position ) ; } return val ; }	Compares with the specified fileName and position.
public final log buffer duplicate ( final int pos , final int len ) { if ( pos + len > limit ) throw new illegal argument exception ( str + ( pos + len ) ) ;	Return n bytes in this buffer.
public final log buffer forward ( final int len ) { if ( position + len > origin + limit ) throw new illegal argument exception ( str + ( position + len - origin ) ) ; this . position += len ; return this ; }	Forwards this buffer's position.
public final log buffer consume ( final int len ) { if ( limit > len ) { limit -= len ; origin += len ; position = origin ; return this ; } else if ( limit == len ) { limit = num ; origin = num ; position = num ; return this ; } else { throw new illegal argument exception ( str + len ) ; } }	Consume this buffer, moving origin and position.
public final int get int8 ( final int pos ) { if ( pos >= limit || pos < num ) throw new illegal argument exception ( str + pos ) ; return buffer [ origin + pos ] ; }	Return 8-bit signed int from buffer.
public final int get uint8 ( final int pos ) { if ( pos >= limit || pos < num ) throw new illegal argument exception ( str + pos ) ; return num & buffer [ origin + pos ] ; }	Return 8-bit unsigned int from buffer.
public final string get fix string ( final int pos , final int len , string charset name ) { if ( pos + len > limit || pos < num ) throw new illegal argument exception ( str + ( pos < num ? pos : ( pos + len ) ) ) ; final int from = origin + pos ; final int end = from + len ; byte [ ] buf = buffer ; int found = from ; for ( ; ( found < end ) && buf [ found ] != str ; found ++ ) ; try { return new string ( buf , from , found - from , charset name ) ; } catch ( unsupported encoding exception e ) { throw new illegal argument exception ( str + charset name , e ) ; } }	Return fix length string from buffer.
public final string get fix string ( final int len , string charset name ) { if ( position + len > origin + limit ) throw new illegal argument exception ( str + ( position + len - origin ) ) ; final int from = position ; final int end = from + len ; byte [ ] buf = buffer ; int found = from ; for ( ; ( found < end ) && buf [ found ] != str ; found ++ ) ; try { string string = new string ( buf , from , found - from , charset name ) ; position += len ; return string ; } catch ( unsupported encoding exception e ) { throw new illegal argument exception ( str + charset name , e ) ; } }	Return next fix length string from buffer.
public final string get string ( final int pos , string charset name ) { if ( pos >= limit || pos < num ) throw new illegal argument exception ( str + pos ) ; byte [ ] buf = buffer ; final int len = ( num & buf [ origin + pos ] ) ; if ( pos + len + num > limit ) throw new illegal argument exception ( str + ( pos + len + num ) ) ; try { return new string ( buf , origin + pos + num , len , charset name ) ; } catch ( unsupported encoding exception e ) { throw new illegal argument exception ( str + charset name , e ) ; } }	Return dynamic length string from buffer.
public final string get string ( string charset name ) { if ( position >= origin + limit ) throw new illegal argument exception ( str + position ) ; byte [ ] buf = buffer ; final int len = ( num & buf [ position ] ) ; if ( position + len + num > origin + limit ) throw new illegal argument exception ( str + ( position + len + num - origin ) ) ; try { string string = new string ( buf , position + num , len , charset name ) ; position += len + num ; return string ; } catch ( unsupported encoding exception e ) { throw new illegal argument exception ( str + charset name , e ) ; } }	Return next dynamic length string from buffer.
public final boolean next one row ( bit set columns , boolean after ) { final boolean has one row = buffer . has remaining ( ) ; if ( has one row ) { int column = num ; for ( int i = num ; i < column len ; i ++ ) if ( columns . get ( i ) ) { column ++ ; } if ( after && partial ) { partial bits . clear ( ) ; long value options = buffer . get packed long ( ) ; int partial json updates = num ; if ( ( value options & partial json updates ) != num ) { partial bits . set ( num ) ; buffer . forward ( ( json column count + num ) / num ) ; } } null bit index = num ; null bits . clear ( ) ; buffer . fill bitmap ( null bits , column ) ; } return has one row ; }	Extracting next row from packed buffer.
static int mysql to java type ( int type , final int meta , boolean is binary ) { int java type ; if ( type == log event . mysql type string ) { if ( meta >= num ) { int byte0 = meta > > num ; if ( ( byte0 & num ) != num ) { type = byte0 | num ; } else { switch ( byte0 ) { case log event . mysql type set : case log event . mysql type enum : case log event . mysql type string : type = byte0 ; } } } } switch ( type ) { case log event . mysql type long : java type = types . integer ; break ; case log event . mysql type tiny : java type = types . tinyint ; break ; case log event . mysql type short : java type = types . smallint ; break ; case log event . mysql type in : java type = types . integer ; break ; case log event . mysql type longlong : java type = types . bigint ; break ; case log event . mysql type decimal : java type = types . decimal ; break ; case log event . mysql type newdecimal : java type = types . decimal ; break ; case log event . mysql type float : java type = types . real ;	Maps the given MySQL type to the correct JDBC type.
protected void after start event parser ( canal event parser event parser ) {	around event parser, default impl.
@ override public long last modified ( ) throws io { long last modified = get file for last modified check ( ) . last modified ( ) ; if ( last modified == num ) { throw new file not found exception ( get description ( ) + str ) ; } return last modified ; }	This implementation checks the timestamp of the underlying File, ifavailable.
@ override public org . springframework . core . io . resource create relative ( string relative path ) throws io { throw new file not found exception ( str + get description ( ) ) ; }	This implementation throws a FileNotFoundException, assuming that relativeresources cannot be created for this resource.
public void set name aliases ( map < string , list < string > > aliases ) { this . name aliases = new linked multi value map < string , string > ( aliases ) ; }	Set name aliases.
public static long read unsigned int little endian ( byte [ ] data , int index ) { long result = ( long ) ( data [ index ] & num ) | ( long ) ( ( data [ index + num ] & num ) << num ) | ( long ) ( ( data [ index + num ] & num ) << num ) | ( long ) ( ( data [ index + num ] & num ) << num ) ; return result ; }	Read 4 bytes in Little-endian byte order.
@ override public synchronized string format ( final log record record ) { buffer . set length ( prefix . length ( ) ) ; buffer . append ( timestamp formatter . format ( new date ( record . get millis ( ) ) ) ) ; buffer . append ( str ) ; buffer . append ( level number to commons level name ( record . get level ( ) ) ) ; string [ ] parts = record . get source class name ( ) . split ( str ) ; buffer . append ( str + parts [ parts . length - num ] + str + record . get source method name ( ) + str ) ; buffer . append ( suffix ) ; buffer . append ( format message ( record ) ) . append ( line separator ) ; if ( record . get thrown ( ) != null ) { final string writer trace = new string writer ( ) ; record . get thrown ( ) . print stack trace ( new print writer ( trace ) ) ; buffer . append ( trace ) ; } return buffer . to string ( ) ; }	Format the given log record and return the formatted string.
public string get selenium script ( string name ) { string raw function = read script ( prefix + name ) ; return string . format ( str , raw function ) ; }	Loads the named Selenium script and returns it wrapped in an anonymous function.
public static registration request from json ( map < string , object > raw ) throws json exception {	Create an object from a registration request formatted as a json string.
public void validate ( ) throws grid configuration exception {	Validate the current setting and throw a config exception is an invalid setup is detected.
public void forward new session request and update registry ( test session session ) throws new session exception { try ( new session payload payload = new session payload . create ( immutable map . of ( str , session . get requested capabilities ( ) ) ) ) { string builder json = new string builder ( ) ; payload . write to ( json ) ; request . set body ( json . to string ( ) ) ; session . forward ( get request ( ) , get response ( ) , bool ) ; } catch ( io e ) {	Forward the new session request to the TestSession that has been assigned, and parse theresponse to extract and return the external key assigned by the remote.
private void before session event ( ) throws new session exception { remote proxy p = session . get slot ( ) . get proxy ( ) ; if ( p instanceof test session listener ) { try { ( ( test session listener ) p ) . before session ( session ) ; } catch ( exception e ) { log . severe ( str + e . get message ( ) ) ; e . print stack trace ( ) ; throw new new session exception ( str , e ) ; } } }	calls the TestSessionListener is the proxy for that node has one specified.
public void wait for session bound ( ) throws interrupted exception , timeout exception {	wait for the registry to match the request with a TestSlot.
public static expected condition < boolean > title is ( final string title ) { return new expected condition < boolean > ( ) { private string current title = str ; @ override public boolean apply ( web driver driver ) { current title = driver . get title ( ) ; return title . equals ( current title ) ; } @ override public string to string ( ) { return string . format ( str , title , current title ) ; } } ; }	An expectation for checking the title of a page.
public static expected condition < boolean > title contains ( final string title ) { return new expected condition < boolean > ( ) { private string current title = str ; @ override public boolean apply ( web driver driver ) { current title = driver . get title ( ) ; return current title != null && current title . contains ( title ) ; } @ override public string to string ( ) { return string . format ( str , title , current title ) ; } } ; }	An expectation for checking that the title contains a case-sensitive substring.
public static expected condition < boolean > url to be ( final string url ) { return new expected condition < boolean > ( ) { private string current url = str ; @ override public boolean apply ( web driver driver ) { current url = driver . get current url ( ) ; return current url != null && current url . equals ( url ) ; } @ override public string to string ( ) { return string . format ( str , url , current url ) ; } } ; }	An expectation for the URL of the current page to be a specific url.
public static expected condition < boolean > url contains ( final string fraction ) { return new expected condition < boolean > ( ) { private string current url = str ; @ override public boolean apply ( web driver driver ) { current url = driver . get current url ( ) ; return current url != null && current url . contains ( fraction ) ; } @ override public string to string ( ) { return string . format ( str , fraction , current url ) ; } } ; }	An expectation for the URL of the current page to contain specific text.
public static expected condition < boolean > url matches ( final string regex ) { return new expected condition < boolean > ( ) { private string current url ; private pattern pattern ; private matcher matcher ; @ override public boolean apply ( web driver driver ) { current url = driver . get current url ( ) ; pattern = pattern . compile ( regex ) ; matcher = pattern . matcher ( current url ) ; return matcher . find ( ) ; } @ override public string to string ( ) { return string . format ( str , regex , current url ) ; } } ; }	Expectation for the URL to match a specific regular expression.
public static expected condition < web element > presence of element located ( final by locator ) { return new expected condition < web element > ( ) { @ override public web element apply ( web driver driver ) { return driver . find element ( locator ) ; } @ override public string to string ( ) { return str + locator ; } } ; }	An expectation for checking that an element is present on the DOM of a page.
public static expected condition < web element > visibility of element located ( final by locator ) { return new expected condition < web element > ( ) { @ override public web element apply ( web driver driver ) { try { return element if visible ( driver . find element ( locator ) ) ; } catch ( stale element reference exception e ) { return null ; } } @ override public string to string ( ) { return str + locator ; } } ; }	An expectation for checking that an element is present on the DOM of a page and visible.Visibility means that the element is not only displayed but also has a height and width that isgreater than 0.
public static expected condition < web element > visibility of ( final web element element ) { return new expected condition < web element > ( ) { @ override public web element apply ( web driver driver ) { return element if visible ( element ) ; } @ override public string to string ( ) { return str + element ; } } ; }	An expectation for checking that an element, known to be present on the DOM of a page, isvisible.
public static expected condition < list < web element > > presence of all elements located by ( final by locator ) { return new expected condition < list < web element > > ( ) { @ override public list < web element > apply ( web driver driver ) { list < web element > elements = driver . find elements ( locator ) ; return elements . size ( ) > num ? elements : null ; } @ override public string to string ( ) { return str + locator ; } } ; }	An expectation for checking that there is at least one element present on a web page.
public static expected condition < boolean > text to be present in element ( final web element element , final string text ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { try { string element text = element . get text ( ) ; return element text . contains ( text ) ; } catch ( stale element reference exception e ) { return null ; } } @ override public string to string ( ) { return string . format ( str , text , element ) ; } } ; }	An expectation for checking if the given text is present in the specified element.
public static expected condition < boolean > text to be present in element located ( final by locator , final string text ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { try { string element text = driver . find element ( locator ) . get text ( ) ; return element text . contains ( text ) ; } catch ( stale element reference exception e ) { return null ; } } @ override public string to string ( ) { return string . format ( str , text , locator ) ; } } ; }	An expectation for checking if the given text is present in the element that matches the givenlocator.
public static expected condition < boolean > invisibility of element located ( final by locator ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { try { return ! ( driver . find element ( locator ) . is displayed ( ) ) ; } catch ( no such element exception e ) {	An expectation for checking that an element is either invisible or not present on the DOM.
public static expected condition < boolean > invisibility of element with text ( final by locator , final string text ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { try { return ! driver . find element ( locator ) . get text ( ) . equals ( text ) ; } catch ( no such element exception e ) {	An expectation for checking that an element with text is either invisible or not present on theDOM.
public static expected condition < boolean > staleness of ( final web element element ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver ignored ) { try {	Wait until an element is no longer attached to the DOM.
public static < t > expected condition < t > refreshed ( final expected condition < t > condition ) { return new expected condition < t > ( ) { @ override public t apply ( web driver driver ) { try { return condition . apply ( driver ) ; } catch ( stale element reference exception e ) { return null ; } } @ override public string to string ( ) { return string . format ( str , condition ) ; } } ; }	Wrapper for a condition, which allows for elements to update by redrawing.This works around the problem of conditions which have two parts: find an element and thencheck for some condition on it.
public static expected condition < boolean > element selection state to be ( final web element element , final boolean selected ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { return element . is selected ( ) == selected ; } @ override public string to string ( ) { return string . format ( str , element , ( selected ? str : str ) ) ; } } ; }	An expectation for checking if the given element is selected.
public static expected condition < boolean > not ( final expected condition < ? > condition ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { object result = condition . apply ( driver ) ; return result == null || result . equals ( boolean . false ) ; } @ override public string to string ( ) { return str + condition ; } } ; }	An expectation with the logical opposite condition of the given condition.Note that if the Condition you are inverting throws an exception that is caught by the IgnoredExceptions, the inversion will not take place and lead to confusing results.
public static expected condition < boolean > attribute to be ( final by locator , final string attribute , final string value ) { return new expected condition < boolean > ( ) { private string current value = null ; @ override public boolean apply ( web driver driver ) { web element element = driver . find element ( locator ) ; current value = element . get attribute ( attribute ) ; if ( current value == null || current value . is empty ( ) ) { current value = element . get css value ( attribute ) ; } return value . equals ( current value ) ; } @ override public string to string ( ) { return string . format ( str , locator , value , current value ) ; } } ; }	An expectation for checking WebElement with given locator has attribute with a specific value.
public static expected condition < boolean > text to be ( final by locator , final string value ) { return new expected condition < boolean > ( ) { private string current value = null ; @ override public boolean apply ( web driver driver ) { try { current value = driver . find element ( locator ) . get text ( ) ; return current value . equals ( value ) ; } catch ( exception e ) { return bool ; } } @ override public string to string ( ) { return string . format ( str , locator , value , current value ) ; } } ; }	An expectation for checking WebElement with given locator has specific text.
public static expected condition < boolean > text matches ( final by locator , final pattern pattern ) { return new expected condition < boolean > ( ) { private string current value = null ; @ override public boolean apply ( web driver driver ) { try { current value = driver . find element ( locator ) . get text ( ) ; return pattern . matcher ( current value ) . find ( ) ; } catch ( exception e ) { return bool ; } } @ override public string to string ( ) { return string . format ( str , locator , pattern . pattern ( ) , current value ) ; } } ; }	An expectation for checking WebElement with given locator has text with a value as a part ofit.
public static expected condition < list < web element > > number of elements to be more than ( final by locator , final integer number ) { return new expected condition < list < web element > > ( ) { private integer current number = num ; @ override public list < web element > apply ( web driver web driver ) { list < web element > elements = web driver . find elements ( locator ) ; current number = elements . size ( ) ; return current number > number ? elements : null ; } @ override public string to string ( ) { return string . format ( str , locator , number , current number ) ; } } ; }	An expectation for checking number of WebElements with given locator being more than defined number.
public static expected condition < boolean > attribute contains ( final web element element , final string attribute , final string value ) { return new expected condition < boolean > ( ) { private string current value = null ; @ override public boolean apply ( web driver driver ) { return get attribute or css value ( element , attribute ) . map ( seen -> seen . contains ( value ) ) . or else ( bool ) ; } @ override public string to string ( ) { return string . format ( str , value , current value ) ; } } ; }	An expectation for checking WebElement with given locator has attribute which contains specificvalue.
public static expected condition < boolean > attribute to be not empty ( final web element element , final string attribute ) { return driver -> get attribute or css value ( element , attribute ) . is present ( ) ; }	An expectation for checking WebElement any non empty value for given attribute.
public static expected condition < web element > presence of nested element located by ( final web element element , final by child locator ) { return new expected condition < web element > ( ) { @ override public web element apply ( web driver web driver ) { return element . find element ( child locator ) ; } @ override public string to string ( ) { return string . format ( str , child locator ) ; } } ; }	An expectation for checking child WebElement as a part of parent element to be present.
public static expected condition < boolean > invisibility of all elements ( final list < web element > elements ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver web driver ) { return elements . stream ( ) . all match ( expected conditions :: is invisible ) ; } @ override public string to string ( ) { return str + elements ; } } ; }	An expectation for checking all elements from given list to be invisible.
public static expected condition < boolean > invisibility of ( final web element element ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver web driver ) { return is invisible ( element ) ; } @ override public string to string ( ) { return str + element ; } } ; }	An expectation for checking the element to be invisible.
public static expected condition < boolean > or ( final expected condition < ? > ... conditions ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { runtime exception last exception = null ; for ( expected condition < ? > condition : conditions ) { try { object result = condition . apply ( driver ) ; if ( result != null ) { if ( result instanceof boolean ) { if ( boolean . true . equals ( result ) ) { return bool ; } } else { return bool ; } } } catch ( runtime exception e ) { last exception = e ; } } if ( last exception != null ) { throw last exception ; } return bool ; } @ override public string to string ( ) { string builder message = new string builder ( str ) ; joiner . on ( str ) . append to ( message , conditions ) ; return message . to string ( ) ; } } ; }	An expectation with the logical or condition of the given list of conditions.Each condition is checked until at least one of them returns true or not null.
public static expected condition < boolean > and ( final expected condition < ? > ... conditions ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { for ( expected condition < ? > condition : conditions ) { object result = condition . apply ( driver ) ; if ( result instanceof boolean ) { if ( boolean . false . equals ( result ) ) { return bool ; } } if ( result == null ) { return bool ; } } return bool ; } @ override public string to string ( ) { string builder message = new string builder ( str ) ; joiner . on ( str ) . append to ( message , conditions ) ; return message . to string ( ) ; } } ; }	An expectation with the logical and condition of the given list of conditions.Each condition is checked until all of them return true or not null.
public static expected condition < boolean > java script throws no exceptions ( final string java script ) { return new expected condition < boolean > ( ) { @ override public boolean apply ( web driver driver ) { try { ( ( javascript executor ) driver ) . execute script ( java script ) ; return bool ; } catch ( web driver exception e ) { return bool ; } } @ override public string to string ( ) { return string . format ( str , java script ) ; } } ; }	An expectation to check if js executable.Useful when you know that there should be a Javascript value or something at the stage.
public static expected condition < object > js returns value ( final string java script ) { return new expected condition < object > ( ) { @ override public object apply ( web driver driver ) { try { object value = ( ( javascript executor ) driver ) . execute script ( java script ) ; if ( value instanceof list ) { return ( ( list < ? > ) value ) . is empty ( ) ? null : value ; } if ( value instanceof string ) { return ( ( string ) value ) . is empty ( ) ? null : value ; } return value ; } catch ( web driver exception e ) { return null ; } } @ override public string to string ( ) { return string . format ( str , java script ) ; } } ; }	An expectation for String value from javascript.
@ beta public map < string , string > get alert ( ) { hash map < string , string > to return = new hash map < > ( ) ; to return . put ( str , get alert text ( ) ) ; return collections . unmodifiable map ( to return ) ; }	Used for serialising. Some of the drivers return the alert text like this.
@ suppress warnings ( str ) public static string escape ( string to escape ) { if ( to escape . contains ( str ) && to escape . contains ( str ) ) { boolean quote is last = bool ; if ( to escape . last index of ( str ) == to escape . length ( ) - num ) { quote is last = bool ; } string [ ] substrings without quotes = to escape . split ( str ) ; string builder quoted = new string builder ( str ) ; for ( int i = num ; i < substrings without quotes . length ; i ++ ) { quoted . append ( str ) . append ( substrings without quotes [ i ] ) . append ( str ) ; quoted . append ( ( ( i == substrings without quotes . length - num ) ? ( quote is last ? str : str ) : str ) ) ; } return quoted . to string ( ) ; }	Convert strings with both quotes and ticks into a valid xpath componentFor example, {.
@ suppress warnings ( str ) public static < t extends remote proxy > t get new instance ( registration request request , grid registry registry ) { try { string proxy class = request . get configuration ( ) . proxy ; if ( proxy class == null ) { log . fine ( str ) ; proxy class = base remote proxy . class . get canonical name ( ) ; } class < ? > clazz = class . for name ( proxy class ) ; log . fine ( str + clazz . get name ( ) ) ; object [ ] args = new object [ ] { request , registry } ; class < ? > [ ] args class = new class [ ] { registration request . class , grid registry . class } ; constructor < ? > c = clazz . get constructor ( args class ) ; object proxy = c . new instance ( args ) ; if ( proxy instanceof remote proxy ) { ( ( remote proxy ) proxy ) . setup timeout listener ( ) ; return ( t ) proxy ; } throw new invalid parameter exception ( str + proxy . get class ( ) + str ) ; } catch ( invocation target exception e ) { throw new invalid parameter exception ( str + e . get target exception ( ) . get message ( ) ) ; } catch ( exception e ) { throw new invalid parameter exception ( str + e . get message ( ) ) ; } }	Takes a registration request and return the RemoteProxy associated to it.
public remote proxy remove ( remote proxy proxy ) {	Removes the specified instance from the proxySet.
public remote web driver builder one of ( capabilities maybe this , capabilities ... or one of these ) { options . clear ( ) ; add alternative ( maybe this ) ; for ( capabilities an or one of these : or one of these ) { add alternative ( an or one of these ) ; } return this ; }	Clears the current set of alternative browsers and instead sets the list of possible choices tothe arguments given to this method.
public remote web driver builder add alternative ( capabilities options ) { map < string , object > serialized = validate ( objects . require non null ( options ) ) ; this . options . add ( serialized ) ; return this ; }	Add to the list of possible configurations that might be asked for.
@ beta public string new window ( window type type ) { response response = execute ( str , immutable map . of ( str , type == window type . tab ) ) ; return ( string ) response . get value ( ) ; }	Open either a new tab or window, depending on what is requested, and return the window handlewithout switching to it.
private string get console icon path ( desired capabilities cap ) { string name = console icon name ( cap ) ; string path = str ; input stream in = thread . current thread ( ) . get context class loader ( ) . get resource as stream ( path + name + str ) ; if ( in == null ) { return null ; } return str + path + name + str ; }	get the icon representing the browser for the grid.
private string get config info ( ) { string builder builder = new string builder ( ) ; builder . append ( str ) ; grid hub configuration config = get registry ( ) . get hub ( ) . get configuration ( ) ; builder . append ( str ) ; builder . append ( str ) ; builder . append ( pretty html print ( config ) ) ; builder . append ( get verbose config ( ) ) ;	retracing how the hub config was built to help debugging.
private string get verbose config ( ) { string builder builder = new string builder ( ) ; grid hub configuration config = get registry ( ) . get hub ( ) . get configuration ( ) ; builder . append ( str ) ; builder . append ( str ) ; builder . append ( str ) ; grid hub configuration tmp = new grid hub configuration ( ) ; builder . append ( str ) ; builder . append ( str ) ; builder . append ( pretty html print ( tmp ) ) ; if ( config . get raw args ( ) != null ) { builder . append ( str ) ; builder . append ( string . join ( str , config . get raw args ( ) ) ) ; if ( config . get config file ( ) != null ) { builder . append ( str ) . append ( config . get config file ( ) ) . append ( str ) ; try { builder . append ( string . join ( str , files . read all lines ( new file ( config . get config file ( ) ) . to path ( ) ) ) ) ; } catch ( io e ) { builder . append ( str ) . append ( e . get message ( ) ) . append ( str ) ; } } } builder . append ( str ) ;	Displays more detailed configuration.
@ override public test session get new session ( map < string , object > requested capability ) { if ( down ) { return null ; } return super . get new session ( requested capability ) ; }	overwrites the session allocation to discard the proxy that are down.
@ override public http response encode ( supplier < http response > factory , response response ) { int status = response . get status ( ) == error codes . success ? http ok : http internal error ; byte [ ] data = json . to json ( get value to encode ( response ) ) . get bytes ( utf 8 ) ; http response http response = factory . get ( ) ; http response . set status ( status ) ; http response . set header ( cache control , str ) ; http response . set header ( expires , str ) ; http response . set header ( content length , string . value of ( data . length ) ) ; http response . set header ( content type , json utf 8 . to string ( ) ) ; http response . set content ( data ) ; return http response ; }	Encodes the given response as a HTTP response message.
public property setting property setting ( property setting setter ) { property setting previous = this . setter ; this . setter = objects . require non null ( setter ) ; return previous ; }	Change how property setting is done.
public actions key up ( char sequence key ) { if ( is building actions ( ) ) { action . add action ( new key up action ( json keyboard , json mouse , as keys ( key ) ) ) ; } return add key action ( key , code point -> tick ( default keyboard . create key up ( code point ) ) ) ; }	Performs a modifier key release.
public actions release ( ) { if ( is building actions ( ) ) { action . add action ( new button release action ( json mouse , null ) ) ; } return tick ( default mouse . create pointer up ( button . left . as arg ( ) ) ) ; }	Releases the depressed left mouse button at the current mouse location.
public actions double click ( ) { if ( is building actions ( ) ) { action . add action ( new double click action ( json mouse , null ) ) ; } return click in ticks ( left ) . click in ticks ( left ) ; }	Performs a double-click at the current mouse location.
public actions move to element ( web element target ) { if ( is building actions ( ) ) { action . add action ( new move mouse action ( json mouse , ( locatable ) target ) ) ; } return move in ticks ( target , num , num ) ; }	Moves the mouse to the middle of the element.
public actions move to element ( web element target , int x offset , int y offset ) { if ( is building actions ( ) ) { action . add action ( new move to offset action ( json mouse , ( locatable ) target , x offset , y offset ) ) ; }	Moves the mouse to an offset from the top-left corner of the element.The element is scrolled into view and its location is calculated using getBoundingClientRect.
public actions context click ( web element target ) { if ( is building actions ( ) ) { action . add action ( new context click action ( json mouse , ( locatable ) target ) ) ; } return move in ticks ( target , num , num ) . click in ticks ( right ) ; }	Performs a context-click at middle of the given element.
public actions context click ( ) { if ( is building actions ( ) ) { action . add action ( new context click action ( json mouse , null ) ) ; } return click in ticks ( right ) ; }	Performs a context-click at the current mouse location.
public actions drag and drop ( web element source , web element target ) { if ( is building actions ( ) ) { action . add action ( new click and hold action ( json mouse , ( locatable ) source ) ) ; action . add action ( new move mouse action ( json mouse , ( locatable ) target ) ) ; action . add action ( new button release action ( json mouse , ( locatable ) target ) ) ; } return move in ticks ( source , num , num ) . tick ( default mouse . create pointer down ( left . as arg ( ) ) ) . move in ticks ( target , num , num ) . tick ( default mouse . create pointer up ( left . as arg ( ) ) ) ; }	A convenience method that performs click-and-hold at the location of the source element,moves to the location of the target element, then releases the mouse.
public actions drag and drop by ( web element source , int x offset , int y offset ) { if ( is building actions ( ) ) { action . add action ( new click and hold action ( json mouse , ( locatable ) source ) ) ; action . add action ( new move to offset action ( json mouse , null , x offset , y offset ) ) ; action . add action ( new button release action ( json mouse , null ) ) ; } return move in ticks ( source , num , num ) . tick ( default mouse . create pointer down ( left . as arg ( ) ) ) . tick ( default mouse . create pointer move ( duration . of millis ( num ) , origin . pointer ( ) , x offset , y offset ) ) . tick ( default mouse . create pointer up ( left . as arg ( ) ) ) ; }	A convenience method that performs click-and-hold at the location of the source element,moves by a given offset, then releases the mouse.
public actions pause ( long pause ) { if ( is building actions ( ) ) { action . add action ( new pause action ( pause ) ) ; } return tick ( new pause ( default mouse , duration . of millis ( pause ) ) ) ; }	Performs a pause.
public void wait ( string message , long timeout in milliseconds , long interval in milliseconds ) { long start = system . current time millis ( ) ; long end = start + timeout in milliseconds ; while ( system . current time millis ( ) < end ) { if ( until ( ) ) return ; try { thread . sleep ( interval in milliseconds ) ; } catch ( interrupted exception e ) { throw new runtime exception ( e ) ; } } throw new wait timed out exception ( message ) ; }	Wait until the "until" condition returns true or time runs out.
public string get query parameter ( string name ) { iterable < string > all params = get query parameters ( name ) ; if ( all params == null ) { return null ; } iterator < string > iterator = all params . iterator ( ) ; return iterator . has next ( ) ? iterator . next ( ) : null ; }	Get a query parameter.
public http request add query parameter ( string name , string value ) { query parameters . put ( objects . require non null ( name , str ) , objects . require non null ( value , str ) ) ; return this ; }	Set a query parameter, adding to existing values if present.
public cross domain rpc load rpc ( http servlet request request ) throws io { charset encoding ; try { string enc = request . get character encoding ( ) ; encoding = charset . for name ( enc ) ; } catch ( illegal argument exception | null pointer exception e ) { encoding = utf 8 ; }	Parses the request for a CrossDomainRpc.
private static capabilities drop capabilities ( capabilities capabilities ) { if ( capabilities == null ) { return new immutable capabilities ( ) ; } mutable capabilities caps ; if ( is legacy ( capabilities ) ) { final set < string > to remove = sets . new hash set ( binary , profile ) ; caps = new mutable capabilities ( maps . filter keys ( capabilities . as map ( ) , key -> ! to remove . contains ( key ) ) ) ; } else { caps = new mutable capabilities ( capabilities ) ; }	Drops capabilities that we shouldn't send over the wire.Used for capabilities which aren't BeanToJson-convertable, and are only used by the locallauncher.
public file create temp dir ( string prefix , string suffix ) { try {	Create a temporary directory, and track it for deletion.
public void delete temp dir ( file file ) { if ( ! should reap ( ) ) { return ; }	Delete a temporary directory that we were responsible for creating.
public void delete temporary files ( ) { if ( ! should reap ( ) ) { return ; } for ( file file : temporary files ) { try { file handler . delete ( file ) ; } catch ( web driver exception e ) {	Perform the operation that a shutdown hook would have.
public proxy set autodetect ( boolean autodetect ) { if ( this . autodetect == autodetect ) { return this ; } if ( autodetect ) { verify proxy type compatibility ( proxy type . autodetect ) ; this . proxy type = proxy type . autodetect ; } else { this . proxy type = proxy type . unspecified ; } this . autodetect = autodetect ; return this ; }	Specifies whether to autodetect proxy settings.
public proxy set ssl proxy ( string ssl proxy ) { verify proxy type compatibility ( proxy type . manual ) ; this . proxy type = proxy type . manual ; this . ssl proxy = ssl proxy ; return this ; }	Specify which proxy to use for SSL connections.
public proxy set socks proxy ( string socks proxy ) { verify proxy type compatibility ( proxy type . manual ) ; this . proxy type = proxy type . manual ; this . socks proxy = socks proxy ; return this ; }	Specifies which proxy to use for SOCKS.
public proxy set socks username ( string username ) { verify proxy type compatibility ( proxy type . manual ) ; this . proxy type = proxy type . manual ; this . socks username = username ; return this ; }	Specifies a username for the SOCKS proxy.
public proxy set socks password ( string password ) { verify proxy type compatibility ( proxy type . manual ) ; this . proxy type = proxy type . manual ; this . socks password = password ; return this ; }	Specifies a password for the SOCKS proxy.
public static level normalize ( level level ) { if ( level map . contains key ( level . int value ( ) ) ) { return level map . get ( level . int value ( ) ) ; } else if ( level . int value ( ) >= level . severe . int value ( ) ) { return level . severe ; } else if ( level . int value ( ) >= level . warning . int value ( ) ) { return level . warning ; } else if ( level . int value ( ) >= level . info . int value ( ) ) { return level . info ; } else { return level . fine ; } }	Normalizes the given level to one of those supported by Selenium.
public static string get name ( level level ) { level normalized = normalize ( level ) ; return normalized == level . fine ? debug : normalized . get name ( ) ; }	Converts the JDK level to a name supported by Selenium.
@ override public void define command ( string name , http method method , string path pattern ) { define command ( name , new command spec ( method , path pattern ) ) ; }	Defines a new command mapping.
public web element find element ( search context context ) { list < web element > all elements = find elements ( context ) ; if ( all elements == null || all elements . is empty ( ) ) { throw new no such element exception ( str + to string ( ) ) ; } return all elements . get ( num ) ; }	Find a single element.
public static map < string , session logs > get session logs ( map < string , object > raw session map ) { map < string , session logs > session logs map = new hash map < > ( ) ; for ( map . entry < string , object > entry : raw session map . entry set ( ) ) { string session id = entry . get key ( ) ; if ( ! ( entry . get value ( ) instanceof map ) ) { throw new invalid argument exception ( str + entry . get value ( ) ) ; } @ suppress warnings ( str ) map < string , object > value = ( map < string , object > ) entry . get value ( ) ; session logs session logs = session logs . from json ( value ) ; session logs map . put ( session id , session logs ) ; } return session logs map ; }	Creates a session logs map, with session logs mapped to session IDs, givena raw session log map as a JSON object.
public void set environment variables ( map < string , string > environment ) { for ( map . entry < string , string > entry : environment . entry set ( ) ) { set environment variable ( entry . get key ( ) , entry . get value ( ) ) ; } }	Adds the specified environment variables.
@ override public mutable capabilities merge ( capabilities extra capabilities ) { if ( extra capabilities == null ) { return this ; } extra capabilities . as map ( ) . for each ( this :: set capability ) ; return this ; }	Merge the extra capabilities provided into this DesiredCapabilities instance.
public inet address get ip4 non loopback address of this machine ( ) { for ( network interface iface : network interface provider . get network interfaces ( ) ) { final inet address ip4 non loopback = iface . get ip4 non loop back only ( ) ; if ( ip4 non loopback != null ) { return ip4 non loopback ; } } throw new web driver exception ( str ) ; }	Returns a non-loopback IP4 hostname of the local host.
public string obtain loopback ip4 address ( ) { final network interface network interface = get loop back and ip4 only ( ) ; if ( network interface != null ) { return network interface . get ip4 loopback only ( ) . get host name ( ) ; } final string ip of ip4 loop back = get ip of loop back ip4 ( ) ; if ( ip of ip4 loop back != null ) { return ip of ip4 loop back ; } if ( platform . get current ( ) . is ( platform . unix ) ) { network interface linux loopback = network interface provider . get lo interface ( ) ; if ( linux loopback != null ) { final inet address net address = linux loopback . get ip4 loopback only ( ) ; if ( net address != null ) { return net address . get host address ( ) ; } } } throw new web driver exception ( str + get net work diags ( ) + str ) ; }	Returns a single address that is guaranteed to resolve to an ipv4 representation of localhostThis may either be a hostname or an ip address, depending if we can guarantee what that thehostname will resolve to ip4.
public string serialize ( ) { string builder sb = new string builder ( ) ; boolean first = bool ; for ( string key : options . key set ( ) ) { if ( first ) { first = bool ; } else { sb . append ( str ) ; } sb . append ( key ) . append ( str ) . append ( options . get ( key ) ) ; } return sb . to string ( ) ; }	Serializes to the format "name=value;name=value".
public browser configuration options set ( string key , string value ) { if ( value != null ) { options . put ( key , value ) ; } return this ; }	Sets the given key to the given value unless the value is null.
public string execute command on servlet ( string command ) { try { return get command response as string ( command ) ; } catch ( io e ) { if ( e instanceof connect exception ) { throw new selenium exception ( e . get message ( ) , e ) ; } e . print stack trace ( ) ; throw new unsupported operation exception ( str + command + str + e , e ) ; } }	Sends the specified command string to the bridge servlet.
public static string [ ] parse csv ( string input ) { list < string > output = new array list < > ( ) ; string buffer sb = new string buffer ( ) ; for ( int i = num ; i < input . length ( ) ; i ++ ) { char c = input . char at ( i ) ; switch ( c ) { case str : output . add ( sb . to string ( ) ) ; sb = new string buffer ( ) ; continue ; case str : i ++ ; c = input . char at ( i ) ;	Convert backslash-escaped comma-delimited string into String array.
public void merge ( standalone configuration other ) { if ( other == null ) { return ; } if ( is merge able ( integer . class , other . browser timeout , browser timeout ) ) { browser timeout = other . browser timeout ; } if ( is merge able ( integer . class , other . jetty max threads , jetty max threads ) ) { jetty max threads = other . jetty max threads ; } if ( is merge able ( integer . class , other . timeout , timeout ) ) { timeout = other . timeout ; }	copy another configuration's values into this one if they are set.
public map < string , object > to json ( ) { map < string , object > json = new hash map < > ( ) ; json . put ( str , browser timeout ) ; json . put ( str , debug ) ; json . put ( str , jetty max threads ) ; json . put ( str , log ) ; json . put ( str , host ) ; json . put ( str , port ) ; json . put ( str , role ) ; json . put ( str , timeout ) ; serialize fields ( json ) ; return json . entry set ( ) . stream ( ) . filter ( entry -> entry . get value ( ) != null ) . collect ( to immutable sorted map ( natural ( ) , map . entry :: get key , map . entry :: get value ) ) ; }	Return a JsonElement representation of the configuration.
public static external session key from response body ( string response body ) throws new session exception { if ( response body != null && response body . starts with ( str ) ) { return new external session key ( response body . replace ( str , str ) ) ; } throw new new session exception ( str + response body ) ; }	extract the external key from the server response for a selenium1 new session request.
public < k extends throwable > fluent wait < t > ignore all ( collection < class < ? extends k > > types ) { ignored exceptions . add all ( types ) ; return this ; }	Configures this instance to ignore specific types of exceptions while waiting for a condition.Any exceptions not whitelisted will be allowed to propagate, terminating the wait.
private string tab config ( ) { string builder builder = new string builder ( ) ; builder . append ( str ) ; builder . append ( proxy . get config ( ) . to string ( str ) ) ; builder . append ( str ) ; return builder . to string ( ) ; }	content of the config tab.
private string tab browsers ( ) { string builder builder = new string builder ( ) ; builder . append ( str ) ; slots lines rc lines = new slots lines ( ) ; slots lines wd lines = new slots lines ( ) ; for ( test slot slot : proxy . get test slots ( ) ) { if ( slot . get protocol ( ) == selenium protocol . selenium ) { rc lines . add ( slot ) ; } else { wd lines . add ( slot ) ; } } if ( rc lines . get lines type ( ) . size ( ) != num ) { builder . append ( str ) ; builder . append ( get lines ( rc lines ) ) ; } if ( wd lines . get lines type ( ) . size ( ) != num ) { builder . append ( str ) ; builder . append ( get lines ( wd lines ) ) ; } builder . append ( str ) ; return builder . to string ( ) ; }	content of the browsers tab.
private string get lines ( slots lines lines ) { string builder builder = new string builder ( ) ; for ( mini capability cap : lines . get lines type ( ) ) { string icon = cap . get icon ( ) ; string version = cap . get version ( ) ; builder . append ( str ) ; if ( version != null ) { builder . append ( str ) . append ( version ) ; } for ( test slot s : lines . get line ( cap ) ) { builder . append ( get single slot html ( s , icon ) ) ; } builder . append ( str ) ; } return builder . to string ( ) ; }	the lines of icon representing the possible slots.
private string node tabs ( ) { string builder builder = new string builder ( ) ; builder . append ( str ) ; builder . append ( str ) ; builder . append ( str ) ; builder . append ( str ) ; builder . append ( str ) ; builder . append ( str ) ; return builder . to string ( ) ; }	the tabs header.
public static string get platform ( remote proxy proxy ) { if ( proxy . get test slots ( ) . size ( ) == num ) { return str ; } platform res = get platform ( proxy . get test slots ( ) . get ( num ) ) ; for ( test slot slot : proxy . get test slots ( ) ) { platform tmp = get platform ( slot ) ; if ( tmp != res ) { return str ; } res = tmp ; } if ( res == null ) { return str ; } return res . to string ( ) ; }	return the platform for the proxy.
public void merge ( grid configuration other ) { if ( other == null ) { return ; } super . merge ( other ) ; if ( is merge able ( integer . class , other . clean up cycle , clean up cycle ) ) { clean up cycle = other . clean up cycle ; } if ( is merge able ( map . class , other . custom , custom ) ) { if ( custom == null ) { custom = new hash map < > ( ) ; } custom . put all ( other . custom ) ; } if ( is merge able ( integer . class , other . max session , max session ) && other . max session > num ) { max session = other . max session ; } if ( is merge able ( list . class , other . servlets , servlets ) ) { servlets = other . servlets ; } if ( is merge able ( list . class , other . without servlets , without servlets ) ) { without servlets = other . without servlets ; } }	replaces this instance of configuration value with the 'other' value if it's set.
public static class < ? extends servlet > create servlet ( string class name ) { try { return class . for name ( class name ) . as subclass ( servlet . class ) ; } catch ( class not found exception e ) { log . warning ( str + class name + str + e . get message ( ) ) ; } return null ; }	Reflexion to create the servlet based on the class name.
protected void log ( session id session id , string command name , object to log , when when ) { if ( ! logger . is loggable ( level ) ) { return ; } string text = string . value of ( to log ) ; if ( command name . equals ( driver command . execute script ) || command name . equals ( driver command . execute async script ) ) { if ( text . length ( ) > num && boolean . get boolean ( str ) ) { text = text . substring ( num , num ) + str ; } } switch ( when ) { case before : logger . log ( level , str + command name + str + text ) ; break ; case after : logger . log ( level , str + text ) ; break ; case exception : logger . log ( level , str + text ) ; break ; default : logger . log ( level , text ) ; break ; } }	Override this to be notified at key points in the execution of a command.
public boolean is running ( ) { lock . lock ( ) ; try { return process != null && process . is running ( ) ; } catch ( illegal thread state exception e ) { return bool ; } finally { lock . unlock ( ) ; } }	Checks whether the driver child process is currently running.
public void start ( ) throws io { lock . lock ( ) ; try { if ( process != null ) { return ; } process = new command line ( this . executable , args . to array ( new string [ ] { } ) ) ; process . set environment variables ( environment ) ; process . copy output to ( get output stream ( ) ) ; process . execute async ( ) ; wait until available ( ) ; } finally { lock . unlock ( ) ; } }	Starts this service if it is not already running.
public void stop ( ) { lock . lock ( ) ; web driver exception to throw = null ; try { if ( process == null ) { return ; } if ( has shutdown endpoint ( ) ) { try { url kill url = new url ( url . to string ( ) + str ) ; new url checker ( ) . wait until unavailable ( num , seconds , kill url ) ; } catch ( url e ) { to throw = new web driver exception ( e ) ; } catch ( url checker . timeout exception e ) { to throw = new web driver exception ( str , e ) ; } } process . destroy ( ) ; if ( get output stream ( ) instanceof file output stream ) { try { get output stream ( ) . close ( ) ; } catch ( io e ) { } } } finally { process = null ; lock . unlock ( ) ; } if ( to throw != null ) { throw to throw ; } }	Stops this service if it is currently running.
public touch actions single tap ( web element on element ) { if ( touch screen != null ) { action . add action ( new single tap action ( touch screen , ( locatable ) on element ) ) ; } tick ( touch pointer . create pointer down ( num ) ) ; tick ( touch pointer . create pointer up ( num ) ) ; return this ; }	Allows the execution of single tap on the screen, analogous to click using a Mouse.
public touch actions down ( int x , int y ) { if ( touch screen != null ) { action . add action ( new down action ( touch screen , x , y ) ) ; } return this ; }	Allows the execution of the gesture 'down' on the screen.
public touch actions up ( int x , int y ) { if ( touch screen != null ) { action . add action ( new up action ( touch screen , x , y ) ) ; } return this ; }	Allows the execution of the gesture 'up' on the screen.
public touch actions move ( int x , int y ) { if ( touch screen != null ) { action . add action ( new move action ( touch screen , x , y ) ) ; } return this ; }	Allows the execution of the gesture 'move' on the screen.
public touch actions scroll ( web element on element , int x offset , int y offset ) { if ( touch screen != null ) { action . add action ( new scroll action ( touch screen , ( locatable ) on element , x offset , y offset ) ) ; } return this ; }	Creates a scroll gesture that starts on a particular screen location.
public touch actions double tap ( web element on element ) { if ( touch screen != null ) { action . add action ( new double tap action ( touch screen , ( locatable ) on element ) ) ; } return this ; }	Allows the execution of double tap on the screen, analogous to double click using a Mouse.
public touch actions long press ( web element on element ) { if ( touch screen != null ) { action . add action ( new long press action ( touch screen , ( locatable ) on element ) ) ; } return this ; }	Allows the execution of long press gestures.
public touch actions scroll ( int x offset , int y offset ) { if ( touch screen != null ) { action . add action ( new scroll action ( touch screen , x offset , y offset ) ) ; } return this ; }	Allows the view to be scrolled by an x and y offset.
public touch actions flick ( int x speed , int y speed ) { if ( touch screen != null ) { action . add action ( new flick action ( touch screen , x speed , y speed ) ) ; } return this ; }	Sends a flick gesture to the current view.
public touch actions flick ( web element on element , int x offset , int y offset , int speed ) { if ( touch screen != null ) { action . add action ( new flick action ( touch screen , ( locatable ) on element , x offset , y offset , speed ) ) ; } return this ; }	Allows the execution of flick gestures starting in a location's element.
public logging preferences add preferences ( logging preferences prefs ) { if ( prefs == null ) { return this ; } for ( string log type : prefs . get enabled log types ( ) ) { enable ( log type , prefs . get level ( log type ) ) ; } return this ; }	Adds the given logging preferences giving them precedence over existingpreferences.
@ suppress warnings ( str ) public t get ( ) { try { is loaded ( ) ; return ( t ) this ; } catch ( error e ) { load ( ) ; } is loaded ( ) ; return ( t ) this ; }	Ensure that the component is currently loaded.
public void add ( request handler request ) { lock . write lock ( ) . lock ( ) ; try { new session requests . add ( request ) ; } finally { lock . write lock ( ) . unlock ( ) ; } }	Adds a request handler to this queue.
public void process queue ( predicate < request handler > handler consumer , prioritizer prioritizer ) { comparator < request handler > comparator = prioritizer == null ? ordering . all equal ( ) :: compare : ( a , b ) -> prioritizer . compare to ( a . get request ( ) . get desired capabilities ( ) , b . get request ( ) . get desired capabilities ( ) ) ; lock . write lock ( ) . lock ( ) ; try { new session requests . stream ( ) . sorted ( comparator ) . filter ( handler consumer ) . for each ( request handler -> { if ( ! remove new session request ( request handler ) ) { log . severe ( str + request handler ) ; } } ) ; } finally { lock . write lock ( ) . unlock ( ) ; } }	Processes all the entries in this queue.
public boolean remove new session request ( request handler request ) { lock . write lock ( ) . lock ( ) ; try { return new session requests . remove ( request ) ; } finally { lock . write lock ( ) . unlock ( ) ; } }	Remove a specific request.
public iterable < desired capabilities > get desired capabilities ( ) { lock . read lock ( ) . lock ( ) ; try { return new session requests . stream ( ) . map ( req -> new desired capabilities ( req . get request ( ) . get desired capabilities ( ) ) ) . collect ( collectors . to list ( ) ) ; } finally { lock . read lock ( ) . unlock ( ) ; } }	Provides the desired capabilities of all the items in this queue.
public static remote command parse ( string input line ) { if ( null == input line ) throw new null pointer exception ( str ) ; string [ ] values = input line . split ( str ) ; if ( values . length != numargsincludingboundaries ) { throw new illegal state exception ( str + input line + values . length ) ; } return new default remote command ( values [ firstindex ] , new string [ ] { values [ secondindex ] , values [ thirdindex ] } ) ; }	Factory method to create a RemoteCommand from a wiki-style input string.
public void add browser ( desired capabilities cap , int instances ) { string s = cap . get browser name ( ) ; if ( s == null || str . equals ( s ) ) { throw new invalid parameter exception ( cap + str ) ; } if ( cap . get platform ( ) == null ) { cap . set platform ( platform . get current ( ) ) ; } cap . set capability ( registration request . max instances , instances ) ; registration request . get configuration ( ) . capabilities . add ( cap ) ; registration request . get configuration ( ) . fix up capabilities ( ) ; }	Adding the browser described by the capability, automatically finding out what platform thenode is launched from.
private grid hub configuration get hub configuration ( ) throws exception { string hub api = str + registration request . get configuration ( ) . get hub host ( ) + str + registration request . get configuration ( ) . get hub port ( ) + str ; url api = new url ( hub api ) ; http client client = http client factory . create client ( api ) ; string url = api . to external form ( ) ; http request request = new http request ( get , url ) ; http response response = client . execute ( request ) ; try ( reader reader = new string reader ( response . get content string ( ) ) ; json input json input = new json ( ) . new input ( reader ) ) { return grid hub configuration . load from json ( json input ) ; } }	uses the hub API to get some of its configuration.
public static response success ( session id session id , object value ) { response response = new response ( ) ; response . set session id ( session id != null ? session id . to string ( ) : null ) ; response . set value ( value ) ; response . set status ( error codes . success ) ; response . set state ( error codes . success string ) ; return response ; }	Creates a response object for a successful command execution.
public void create log file and add to map ( session id session id ) throws io { file rc log file ;	This creates log file object which represents logs in file form.
private void assign request to proxy ( ) { while ( ! stop ) { try { test session available . await ( num , time unit . seconds ) ; new session queue . process queue ( this :: take request handler , get hub ( ) . get configuration ( ) . prioritizer ) ;	iterates the list of incoming session request to find a potential match in the list of proxies.If something changes in the registry, the matcher iteration is stopped to account for thatchange.
private void release ( test session session , session termination reason reason ) { try { lock . lock ( ) ; boolean removed = active test sessions . remove ( session , reason ) ; if ( removed ) { fire matcher state changed ( ) ; } } finally { lock . unlock ( ) ; } }	mark the session as finished for the registry. The resources that were associated to it are nowfree to be reserved by other tests.
private static executable locate firefox binary from system property ( ) { string binary name = system . get property ( firefox driver . system property . browser binary ) ; if ( binary name == null ) return null ; file binary = new file ( binary name ) ; if ( binary . exists ( ) && ! binary . is directory ( ) ) return new executable ( binary ) ; platform current = platform . get current ( ) ; if ( current . is ( windows ) ) { if ( ! binary name . ends with ( str ) ) { binary name += str ; } } else if ( current . is ( mac ) ) { if ( ! binary name . ends with ( str ) ) { binary name += str ; } binary name += str ; } binary = new file ( binary name ) ; if ( binary . exists ( ) ) return new executable ( binary ) ; throw new web driver exception ( string . format ( str , firefox driver . system property . browser binary , binary name ) ) ; }	Locates the firefox binary from a system property.
private static stream < executable > locate firefox binaries from platform ( ) { immutable list . builder < executable > executables = new immutable list . builder < > ( ) ; platform current = platform . get current ( ) ; if ( current . is ( windows ) ) { executables . add all ( stream . of ( str , str , str ) . map ( firefox binary :: get paths in program files ) . flat map ( list :: stream ) . map ( file :: new ) . filter ( file :: exists ) . map ( executable :: new ) . collect ( to list ( ) ) ) ; } else if ( current . is ( mac ) ) {	Locates the firefox binary by platform.
public synchronized void remove session logs ( session id session id ) { if ( store logs on session quit ) { return ; } thread key thread id = session to thread map . get ( session id ) ; session id session id for thread = thread to session map . get ( thread id ) ; if ( thread id != null && session id for thread != null && session id for thread . equals ( session id ) ) { thread to session map . remove ( thread id ) ; session to thread map . remove ( session id ) ; } per session records . remove ( session id ) ; log file repository . remove log file ( session id ) ; }	Removes session logs for the given session id.NB! If the handler has been configured to capture logs on quit no logs will be removed.
public synchronized string get log ( session id session id ) throws io {	This returns Selenium Remote Control logs associated with the sessionId.
public synchronized list < session id > get logged sessions ( ) {	Returns a list of session IDs for which there are logs.The type of logs that are available depends on the log types providedby the driver.
public synchronized session logs get all logs for session ( session id session id ) { session logs session logs = new session logs ( ) ; if ( per session driver entries . contains key ( session id ) ) { map < string , log entries > type to entries map = per session driver entries . get ( session id ) ; for ( string log type : type to entries map . key set ( ) ) { session logs . add log ( log type , type to entries map . get ( log type ) ) ; } per session driver entries . remove ( session id ) ; } return session logs ; }	Gets all logs for a session.
public synchronized log entries get session log ( session id session id ) throws io { list < log entry > entries = new array list < > ( ) ; for ( log record record : records ( session id ) ) { if ( record . get level ( ) . int value ( ) >= server log level . int value ( ) ) entries . add ( new log entry ( record . get level ( ) , record . get millis ( ) , record . get message ( ) ) ) ; } return new log entries ( entries ) ; }	Returns the server log for the given session id.
public synchronized void fetch and store logs from driver ( session id session id , web driver driver ) throws io { if ( ! per session driver entries . contains key ( session id ) ) { per session driver entries . put ( session id , new hash map < > ( ) ) ; } map < string , log entries > type to entries map = per session driver entries . get ( session id ) ; if ( store logs on session quit ) { type to entries map . put ( log type . server , get session log ( session id ) ) ; set < string > log type set = driver . manage ( ) . logs ( ) . get available log types ( ) ; for ( string log type : log type set ) { type to entries map . put ( log type , driver . manage ( ) . logs ( ) . get ( log type ) ) ; } } }	Fetches and stores available logs from the given session and driver.
public void update ( long a0 , long a1 , long a2 , long a3 ) { if ( done ) { throw new illegal state exception ( str ) ; } v1 [ num ] += mul0 [ num ] + a0 ; v1 [ num ] += mul0 [ num ] + a1 ; v1 [ num ] += mul0 [ num ] + a2 ; v1 [ num ] += mul0 [ num ] + a3 ; for ( int i = num ; i < num ; ++ i ) { mul0 [ i ] ^= ( v1 [ i ] & num ) * ( v0 [ i ] > > > num ) ; v0 [ i ] += mul1 [ i ] ; mul1 [ i ] ^= ( v0 [ i ] & num ) * ( v1 [ i ] > > > num ) ; } v0 [ num ] += zipper merge0 ( v1 [ num ] , v1 [ num ] ) ; v0 [ num ] += zipper merge1 ( v1 [ num ] , v1 [ num ] ) ; v0 [ num ] += zipper merge0 ( v1 [ num ] , v1 [ num ] ) ; v0 [ num ] += zipper merge1 ( v1 [ num ] , v1 [ num ] ) ; v1 [ num ] += zipper merge0 ( v0 [ num ] , v0 [ num ] ) ; v1 [ num ] += zipper merge1 ( v0 [ num ] , v0 [ num ] ) ; v1 [ num ] += zipper merge0 ( v0 [ num ] , v0 [ num ] ) ; v1 [ num ] += zipper merge1 ( v0 [ num ] , v0 [ num ] ) ; }	Updates the hash with 32 bytes of data given as 4 longs.
protected void decrement lock ( session implementor session , object key , lock lock ) { lock . unlock ( region . next timestamp ( ) ) ; region . put ( session , key , lock ) ; }	Unlock and re-put the given key, lock combination.
public void set config ( map < string , ? extends cache config > config ) { this . config map = ( map < string , cache config > ) config ; }	Set cache config mapped by cache name.
public static redisson cache monitor ( meter registry registry , redisson cache cache , iterable < tag > tags ) { new redisson cache metrics ( cache , tags ) . bind to ( registry ) ; return cache ; }	Record metrics on a Redisson cache.
private remote executor service async async scheduled service at fixed ( string executor id , string request id ) { scheduled tasks service scheduled remote service = new scheduled tasks service ( codec , name , command executor , executor id , responses ) ; scheduled remote service . set termination topic name ( termination topic name ) ; scheduled remote service . set tasks counter name ( tasks counter name ) ; scheduled remote service . set status name ( status name ) ; scheduled remote service . set scheduler queue name ( scheduler queue name ) ; scheduled remote service . set scheduler channel name ( scheduler channel name ) ; scheduled remote service . set tasks name ( tasks name ) ; scheduled remote service . set request id ( new request id ( request id ) ) ; scheduled remote service . set tasks retry interval name ( tasks retry interval name ) ; remote executor service async async scheduled service at fixed = scheduled remote service . get ( remote executor service async . class , remote invocation options . defaults ( ) . no ack ( ) . no result ( ) ) ; return async scheduled service at fixed ; }	Creates RemoteExecutorServiceAsync with special executor which overrides requestId generationand uses current requestId.
public static string to json ( map < string , ? extends cache config > config ) throws io { return new cache config support ( ) . to json ( config ) ; }	Convert current configuration to JSON format.
public static string to yaml ( map < string , ? extends cache config > config ) throws io { return new cache config support ( ) . to yaml ( config ) ; }	Convert current configuration to YAML format.
public static cron schedule daily at hour and minute ( int hour , int minute ) { string expression = string . format ( str , minute , hour ) ; return of ( expression ) ; }	Creates cron expression which schedule task executionevery day at the given time.
public static cron schedule weekly on day and hour and minute ( int hour , int minute , integer ... days of week ) { if ( days of week == null || days of week . length == num ) { throw new illegal argument exception ( str ) ; } string expression = string . format ( str , minute , hour , days of week [ num ] ) ; for ( int i = num ; i < days of week . length ; i ++ ) { expression = expression + str + days of week [ i ] ; } return of ( expression ) ; }	Creates cron expression which schedule task executionevery given days of the week at the given time.Use Calendar object constants to define day.
public static cron schedule monthly on day and hour and minute ( int day of month , int hour , int minute ) { string expression = string . format ( str , minute , hour , day of month ) ; return of ( expression ) ; }	Creates cron expression which schedule task executionevery given day of the month at the given time.
public local cached map options < k , v > eviction policy ( eviction policy eviction policy ) { if ( eviction policy == null ) { throw new null pointer exception ( str ) ; } this . eviction policy = eviction policy ; return this ; }	Sets eviction policy.
public final boolean await uninterruptibly ( ) { try { return await ( num , time unit . seconds ) ; } catch ( interrupted exception e ) { thread . current thread ( ) . interrupt ( ) ; return bool ; } }	waiting for an open state.
protected void handle lock expiry ( shared session contract implementor session , object key , lockable lock ) { long ts = region . next timestamp ( ) + region . get timeout ( ) ;	Handle the timeout of a previous lock mapped to this key.
public void start ( ) { if ( has redisson instance ) { redisson = redisson . create ( config ) ; } retrieve addresses ( ) ; if ( config . get redisson node initializer ( ) != null ) { config . get redisson node initializer ( ) . on startup ( this ) ; } int map reduce workers = config . get map reduce workers ( ) ; if ( map reduce workers != - num ) { if ( map reduce workers == num ) { map reduce workers = runtime . get runtime ( ) . available processors ( ) ; } redisson . get executor service ( r . mapreduce name ) . register workers ( map reduce workers ) ; log . info ( str , map reduce workers ) ; } for ( entry < string , integer > entry : config . get executor service workers ( ) . entry set ( ) ) { string name = entry . get key ( ) ; int workers = entry . get value ( ) ; redisson . get executor service ( name ) . register workers ( workers ) ; log . info ( str , workers , name ) ; } log . info ( str ) ; }	Start Redisson node instance.
public static object convert value ( final object value , final class < ? > convert type ) { if ( null == value ) { return convert null value ( convert type ) ; } if ( value . get class ( ) == convert type ) { return value ; } if ( value instanceof number ) { return convert number value ( value , convert type ) ; } if ( value instanceof date ) { return convert date value ( value , convert type ) ; } if ( string . class . equals ( convert type ) ) { return value . to string ( ) ; } else { return value ; } }	Convert value via expected class type.
public synchronized void remove ( final int statement id ) { sql binary statement = get binary statement ( statement id ) ; if ( null != binary statement ) { statement id assigner . remove ( binary statement . get sql ( ) ) ; binary statements . remove ( statement id ) ; } }	Remove expired cache statement.
public registry center load ( final registry center configuration reg center config ) { preconditions . check not null ( reg center config , str ) ; registry center result = new service ( reg center config . get type ( ) , reg center config . get properties ( ) ) ; result . init ( reg center config ) ; return result ; }	Load registry center from SPI.
public static string get exactly value ( final string value ) { return null == value ? null : char matcher . any of ( str ) . remove from ( value ) ; }	Get exactly value for SQL expression. remove special char for SQL expression .
public static string get exactly expression ( final string value ) { return null == value ? null : char matcher . any of ( str ) . remove from ( value ) ; }	Get exactly SQL expression. remove space for SQL expression .
public static string get original value ( final string value , final database type database type ) { if ( database type . sql != database type ) { return value ; } try { default keyword . value of ( value . to upper case ( ) ) ; return string . format ( str , value ) ; } catch ( final illegal argument exception ex ) { return get original value for my sql ( value ) ; } }	Get original value for SQL expression.
public int skip whitespace ( ) { int length = num ; while ( char type . is whitespace ( char at ( offset + length ) ) ) { length ++ ; } return offset + length ; }	skip whitespace.
public int skip comment ( ) { char current = char at ( offset ) ; char next = char at ( offset + num ) ; if ( is single line comment begin ( current , next ) ) { return skip single line comment ( comment begin symbol length ) ; } else if ( str == current ) { return skip single line comment ( mysql special comment begin symbol length ) ; } else if ( is multiple line comment begin ( current , next ) ) { return skip multi line comment ( ) ; } return offset ; }	skip comment.
public token scan variable ( ) { int length = num ; if ( str == char at ( offset + num ) ) { length ++ ; } while ( is variable char ( char at ( offset + length ) ) ) { length ++ ; } return new token ( literals . variable , input . substring ( offset , offset + length ) , offset + length ) ; }	scan variable.
public token scan identifier ( ) { if ( str == char at ( offset ) ) { int length = get length until terminated char ( str ) ; return new token ( literals . identifier , input . substring ( offset , offset + length ) , offset + length ) ; } if ( str == char at ( offset ) ) { int length = get length until terminated char ( str ) ; return new token ( literals . identifier , input . substring ( offset , offset + length ) , offset + length ) ; } if ( str == char at ( offset ) ) { int length = get length until terminated char ( str ) ; return new token ( literals . identifier , input . substring ( offset , offset + length ) , offset + length ) ; } int length = num ; while ( is identifier char ( char at ( offset + length ) ) ) { length ++ ; } string literals = input . substring ( offset , offset + length ) ; if ( is ambiguous identifier ( literals ) ) { return new token ( process ambiguous identifier ( offset + length , literals ) , literals , offset + length ) ; } return new token ( dictionary . find token type ( literals , literals . identifier ) , literals , offset + length ) ; }	scan identifier.
public token scan hex decimal ( ) { int length = hex begin symbol length ; if ( str == char at ( offset + length ) ) { length ++ ; } while ( is hex ( char at ( offset + length ) ) ) { length ++ ; } return new token ( literals . hex , input . substring ( offset , offset + length ) , offset + length ) ; }	scan hex decimal.
public token scan number ( ) { int length = num ; if ( str == char at ( offset + length ) ) { length ++ ; } length += get digital length ( offset + length ) ; boolean is float = bool ; if ( str == char at ( offset + length ) ) { is float = bool ; length ++ ; length += get digital length ( offset + length ) ; } if ( is scientific notation ( offset + length ) ) { is float = bool ; length ++ ; if ( str == char at ( offset + length ) || str == char at ( offset + length ) ) { length ++ ; } length += get digital length ( offset + length ) ; } if ( is binary number ( offset + length ) ) { is float = bool ; length ++ ; } return new token ( is float ? literals . float : literals . int , input . substring ( offset , offset + length ) , offset + length ) ; }	scan number.
public token scan symbol ( ) { int length = num ; while ( char type . is symbol ( char at ( offset + length ) ) ) { length ++ ; } string literals = input . substring ( offset , offset + length ) ; symbol symbol ; while ( null == ( symbol = symbol . literals of ( literals ) ) ) { literals = input . substring ( offset , offset + -- length ) ; } return new token ( symbol , literals , offset + length ) ; }	scan symbol.
public static data source property provider get provider ( final data source data source ) { string data source class name = data source . get class ( ) . get name ( ) ; return data source property providers . contains key ( data source class name ) ? data source property providers . get ( data source class name ) : new default data source property provider ( ) ; }	Get data source property provider.
public static data source get data source ( final string data source class name , final map < string , object > data source properties ) throws reflective operation exception { data source result = ( data source ) class . for name ( data source class name ) . new instance ( ) ; for ( entry < string , object > entry : data source properties . entry set ( ) ) { call setter method ( result , get setter method name ( entry . get key ( ) ) , null == entry . get value ( ) ? null : entry . get value ( ) . to string ( ) ) ; } return result ; }	Get data source.
public static sql new instance ( final database type db type , final encrypt rule encrypt rule , final sharding table meta data sharding table meta data , final string sql ) { if ( database type . sql == db type || database type . h2 == db type ) { return new antlr parsing engine ( db type , sql , encrypt rule , sharding table meta data ) ; } throw new sql ( string . format ( str , db type ) ) ; }	Create Encrypt SQL parser.
public static int round half up ( final object obj ) { if ( obj instanceof short ) { return ( short ) obj ; } if ( obj instanceof integer ) { return ( int ) obj ; } if ( obj instanceof long ) { return ( ( long ) obj ) . int value ( ) ; } if ( obj instanceof double ) { return new big decimal ( ( double ) obj ) . set scale ( num , big decimal . round half up ) . int value ( ) ; } if ( obj instanceof float ) { return new big decimal ( ( float ) obj ) . set scale ( num , big decimal . round half up ) . int value ( ) ; } if ( obj instanceof string ) { return new big decimal ( ( string ) obj ) . set scale ( num , big decimal . round half up ) . int value ( ) ; } throw new sharding exception ( str , obj ) ; }	Round half up.
public static number get exactly number ( final string value , final int radix ) { try { return get big integer ( value , radix ) ; } catch ( final number format exception ex ) { return new big decimal ( value ) ; } }	Get exactly number value and type.
public void parse ( final insert statement insert statement ) { lexer engine . unsupported if equal ( get unsupported keywords before into ( ) ) ; lexer engine . skip until ( default keyword . into ) ; lexer engine . next token ( ) ; table references clause parser . parse ( insert statement , bool ) ; skip between table and values ( insert statement ) ; }	Parse insert into.
public final void parse ( final sql sql statement , final boolean is single table only ) { do { parse table reference ( sql statement , is single table only ) ; } while ( lexer engine . skip if equal ( symbol . comma ) ) ; }	Parse table references.
public final void parse single table without alias ( final sql sql statement ) { int begin position = lexer engine . get current token ( ) . get end position ( ) - lexer engine . get current token ( ) . get literals ( ) . length ( ) ; string literals = lexer engine . get current token ( ) . get literals ( ) ; int skipped schema name length = num ; lexer engine . next token ( ) ; if ( lexer engine . skip if equal ( symbol . dot ) ) { skipped schema name length = literals . length ( ) + symbol . dot . get literals ( ) . length ( ) ; literals = lexer engine . get current token ( ) . get literals ( ) ; lexer engine . next token ( ) ; } sql statement . add sql ( new table token ( begin position , literals , quote character . get quote character ( literals ) , skipped schema name length ) ) ; sql statement . get tables ( ) . add ( new table ( sql . get exactly value ( literals ) , null ) ) ; }	Parse single table without alias.
public static executor service get executor ( final boolean is occupy thread for per connection , final transaction type transaction type , final channel id channel id ) { return ( is occupy thread for per connection || transaction type . xa == transaction type || transaction type . base == transaction type ) ? channel thread executor group . get instance ( ) . get ( channel id ) : user executor group . get instance ( ) . get executor service ( ) ; }	Get executor service.
public void append placeholder ( final sharding placeholder sharding placeholder ) { segments . add ( sharding placeholder ) ; current segment = new string builder ( ) ; segments . add ( current segment ) ; }	Append sharding placeholder.
public sql rewrite ( final boolean is single routing ) { sql result = new sql ( parameters ) ; if ( sql tokens . is empty ( ) ) { return append original literals ( result ) ; } append initial literals ( ! is single routing , result ) ; append tokens and placeholders ( ! is single routing , result ) ; revise parameters ( ) ; return result ; }	rewrite SQL.
public sql generate sql ( final table unit table unit , final sql sql builder , final sharding data source meta data sharding data source meta data ) { return sql builder . to sql ( table unit , get table tokens ( table unit ) , sharding rule , sharding data source meta data ) ; }	Generate SQL string.
public static abstract insert parser new instance ( final database type db type , final sharding rule sharding rule , final lexer engine lexer engine , final sharding table meta data sharding table meta data ) { switch ( db type ) { case h2 : case sql : return new sql ( sharding rule , lexer engine , sharding table meta data ) ; case oracle : return new oracle insert parser ( sharding rule , lexer engine , sharding table meta data ) ; case sql : return new sql ( sharding rule , lexer engine , sharding table meta data ) ; case sql : return new sql ( sharding rule , lexer engine , sharding table meta data ) ; default : throw new unsupported operation exception ( string . format ( str , db type ) ) ; } }	Create insert parser instance.
public static properties unmarshal properties ( final string yaml content ) { return strings . is null or empty ( yaml content ) ? new properties ( ) : new yaml ( ) . load as ( yaml content , properties . class ) ; }	Unmarshal properties YAML.
public collection < string > get all instance data source names ( ) { collection < string > result = new linked list < > ( ) ; for ( entry < string , data source meta data > entry : data source meta data map . entry set ( ) ) { if ( ! is existed ( entry . get key ( ) , result ) ) { result . add ( entry . get key ( ) ) ; } } return result ; }	Get all instance data source names.
public map < string , list < data node > > get data node groups ( ) { map < string , list < data node > > result = new linked hash map < > ( actual data nodes . size ( ) , num ) ; for ( data node each : actual data nodes ) { string data source name = each . get data source name ( ) ; if ( ! result . contains key ( data source name ) ) { result . put ( data source name , new linked list < data node > ( ) ) ; } result . get ( data source name ) . add ( each ) ; } return result ; }	Get data node groups.
public collection < string > get actual datasource names ( ) { collection < string > result = new linked hash set < > ( actual data nodes . size ( ) ) ; for ( data node each : actual data nodes ) { result . add ( each . get data source name ( ) ) ; } return result ; }	Get actual data source names.
public collection < string > get actual table names ( final string target data source ) { collection < string > result = new linked hash set < > ( actual data nodes . size ( ) ) ; for ( data node each : actual data nodes ) { if ( target data source . equals ( each . get data source name ( ) ) ) { result . add ( each . get table name ( ) ) ; } } return result ; }	Get actual table names via target data source name.
public optional < string > parse select item alias ( ) { if ( lexer engine . skip if equal ( default keyword . as ) ) { return parse with as ( null , bool , null ) ; } if ( lexer engine . equal any ( get default available keywords for select item alias ( ) ) || lexer engine . equal any ( get customized available keywords for select item alias ( ) ) ) { return parse alias ( null , bool , null ) ; } return optional . absent ( ) ; }	Parse alias for select item.
public optional < string > parse table alias ( final sql sql statement , final boolean set table token , final string table name ) { if ( lexer engine . skip if equal ( default keyword . as ) ) { return parse with as ( sql statement , set table token , table name ) ; } if ( lexer engine . equal any ( get default available keywords for table alias ( ) ) || lexer engine . equal any ( get customized available keywords for table alias ( ) ) ) { return parse alias ( sql statement , set table token , table name ) ; } return optional . absent ( ) ; }	Parse alias for table.
@ subscribe @ sneaky throws public final synchronized void renew ( final data source changed event data source changed event ) { data source . close ( ) ; data source = new sharding data source ( data source converter . get data source map ( data source changed event . get data source configurations ( ) ) , data source . get sharding context ( ) . get sharding rule ( ) , data source . get sharding context ( ) . get sharding properties ( ) . get props ( ) ) ; }	Renew sharding data source.
public void process parameters ( final list < object > parameters , final boolean is fetch all , final database type database type ) { fill ( parameters ) ; rewrite ( parameters , is fetch all , database type ) ; }	Fill parameters for rewrite limit.
public boolean is need rewrite row count ( final database type database type ) { return database type . sql == database type || database type . sql == database type || database type . h2 == database type ; }	Judge is need rewrite row count or not.
public static xa create xa ( final database type database type , final xa xa data source , final connection connection ) { switch ( database type ) { case sql : return new sqlxa ( ) . wrap ( xa data source , connection ) ; case sql : return new sqlxa ( ) . wrap ( xa data source , connection ) ; case h2 : return new xa ( ) . wrap ( xa data source , connection ) ; default : throw new unsupported operation exception ( string . format ( str , database type ) ) ; } }	Create XA connection from normal connection.
public void add ( final orchestration sharding schema orchestration sharding schema ) { string schema name = orchestration sharding schema . get schema name ( ) ; if ( ! schema group . contains key ( schema name ) ) { schema group . put ( schema name , new linked list < string > ( ) ) ; } schema group . get ( schema name ) . add ( orchestration sharding schema . get data source name ( ) ) ; }	Add orchestration sharding schema.
public void put ( final string sharding schema name , final collection < string > data source names ) { schema group . put ( sharding schema name , data source names ) ; }	Put orchestration sharding schema.
public collection < string > get data source names ( final string sharding schema name ) { return schema group . contains key ( sharding schema name ) ? schema group . get ( sharding schema name ) : collections . < string > empty list ( ) ; }	Get data source names.
public or condition build condition ( final or predicate segment sql segment , final sql sql statement ) { or condition result = create or condition ( sql segment , sql statement ) ; create encrypt or predicate filler ( ) . fill ( sql segment , sql statement ) ; return result ; }	Build condition.
public static boolean is dcl ( final token type primary token type , final token type secondary token type ) { return statement prefix . contains ( primary token type ) || ( primary statement prefix . contains ( primary token type ) && secondary statement prefix . contains ( secondary token type ) ) ; }	Is DCL statement.
public void close ( ) { shutdown executor . execute ( new runnable ( ) { @ override public void run ( ) { try { executor service . shutdown ( ) ; while ( ! executor service . await termination ( num , time unit . seconds ) ) { executor service . shutdown now ( ) ; } } catch ( final interrupted exception ex ) { thread . current thread ( ) . interrupt ( ) ; } } } ) ; }	Close executor service.
@ sneaky throws public sql fill ( final collection < sql > sql segments , final sql rule ) { sql result = rule . get sql statement class ( ) . new instance ( ) ; result . set logic sql ( sql ) ; for ( sql each : sql segments ) { optional < sql > filler = parsing rule registry . find sql ( database type , each . get class ( ) ) ; if ( filler . is present ( ) ) { do fill ( each , result , filler . get ( ) ) ; } } return result ; }	Fill SQL statement.
public final list < connection > get connections ( final connection mode connection mode , final string data source name , final int connection size ) throws sql { data source data source = get data source map ( ) . get ( data source name ) ; preconditions . check state ( null != data source , str , data source name ) ; collection < connection > connections ; synchronized ( cached connections ) { connections = cached connections . get ( data source name ) ; } list < connection > result ; if ( connections . size ( ) >= connection size ) { result = new array list < > ( connections ) . sub list ( num , connection size ) ; } else if ( ! connections . is empty ( ) ) { result = new array list < > ( connection size ) ; result . add all ( connections ) ; list < connection > new connections = create connections ( data source name , connection mode , data source , connection size - connections . size ( ) ) ; result . add all ( new connections ) ; synchronized ( cached connections ) { cached connections . put all ( data source name , new connections ) ; } } else { result = new array list < > ( create connections ( data source name , connection mode , data source , connection size ) ) ; synchronized ( cached connections ) { cached connections . put all ( data source name , result ) ; } } return result ; }	Get database connections.
public final void parse ( final select statement select statement ) { if ( ! lexer engine . skip if equal ( default keyword . order ) ) { return ; } list < order item > result = new linked list < > ( ) ; lexer engine . skip if equal ( oracle keyword . siblings ) ; lexer engine . accept ( default keyword . by ) ; do { optional < order item > order item = parse select order by item ( select statement ) ; if ( order item . is present ( ) ) { result . add ( order item . get ( ) ) ; } } while ( lexer engine . skip if equal ( symbol . comma ) ) ; select statement . get order by items ( ) . add all ( result ) ; }	Parse order by.
public boolean is always false ( ) { if ( sharding conditions . is empty ( ) ) { return bool ; } for ( sharding condition each : sharding conditions ) { if ( ! ( each instanceof always false sharding condition ) ) { return bool ; } } return bool ; }	Judge sharding conditions is always false or not.
public static boolean is symbol ( final char ch ) { return str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch || str == ch ; }	Judge is symbol or not.
@ suppress warnings ( str ) public < t > t get value ( final sharding properties constant sharding properties constant ) { if ( cached properties . contains key ( sharding properties constant ) ) { return ( t ) cached properties . get ( sharding properties constant ) ; } string value = props . get property ( sharding properties constant . get key ( ) ) ; if ( strings . is null or empty ( value ) ) { object obj = props . get ( sharding properties constant . get key ( ) ) ; if ( null == obj ) { value = sharding properties constant . get default value ( ) ; } else { value = obj . to string ( ) ; } } object result ; if ( boolean . class == sharding properties constant . get type ( ) ) { result = boolean . value of ( value ) ; } else if ( int . class == sharding properties constant . get type ( ) ) { result = integer . value of ( value ) ; } else if ( long . class == sharding properties constant . get type ( ) ) { result = long . value of ( value ) ; } else { result = value ; } cached properties . put ( sharding properties constant , result ) ; return ( t ) result ; }	Get property value.
@ sneaky throws public void init ( final extractor rule definition entity rule definition entity ) { for ( extractor rule entity each : rule definition entity . get rules ( ) ) { rules . put ( each . get id ( ) , ( sql ) class . for name ( each . get extractor class ( ) ) . new instance ( ) ) ; } }	Initialize SQL extractor rule definition.
public void clear ( ) throws sql { clear statements ( ) ; statements . clear ( ) ; parameter sets . clear ( ) ; connections . clear ( ) ; result sets . clear ( ) ; execute groups . clear ( ) ; }	Clear data.
public list < string > split and evaluate ( ) { if ( null == inline expression ) { return collections . empty list ( ) ; } return flatten ( evaluate ( split ( ) ) ) ; }	Split and evaluate inline expression.
public static boolean is boolean value ( final string value ) { return boolean . true . to string ( ) . equals ignore case ( value ) || boolean . false . to string ( ) . equals ignore case ( value ) ; }	Judge is boolean value or not.
public static boolean is int value ( final string value ) { try { integer . parse int ( value ) ; return bool ; } catch ( final number format exception ex ) { return bool ; } }	Judge is int value or not.
public static boolean is long value ( final string value ) { try { long . parse long ( value ) ; return bool ; } catch ( final number format exception ex ) { return bool ; } }	Judge is long value or not.
public static void init ( ) { string tracer class name = system . get property ( opentracing tracer class name ) ; preconditions . check not null ( tracer class name , str , opentracing tracer class name ) ; try { init ( ( tracer ) class . for name ( tracer class name ) . new instance ( ) ) ; } catch ( final reflective operation exception ex ) { throw new sharding exception ( str , ex ) ; } }	Initialize sharding tracer.
public sqlast parse ( ) { parse tree parse tree = sql . new instance ( database type , sql ) . execute ( ) . get child ( num ) ; if ( parse tree instanceof error node ) { throw new sql ( string . format ( str , sql ) ) ; } optional < sql > sql statement rule = parsing rule registry . find sql ( database type , parse tree . get class ( ) . get simple name ( ) ) ; if ( sql statement rule . is present ( ) ) { return new sqlast ( ( parser rule context ) parse tree , sql statement rule . get ( ) ) ; } if ( parsing rule registry instanceof encrypt parsing rule registry ) { return new sqlast ( ( parser rule context ) parse tree ) ; } throw new sql ( string . format ( str , sql ) ) ; }	Parse SQL to abstract syntax tree.
public string get instances node full path ( final string instance id ) { return joiner . on ( str ) . join ( str , name , root , instances node path , instance id ) ; }	Get instance node full path.
public string get data sources node full path ( final string schema data source name ) { return joiner . on ( str ) . join ( str , name , root , data sources node path , schema data source name ) ; }	Get data source node full path.
public static lexer engine new instance ( final database type db type , final string sql ) { switch ( db type ) { case h2 : return new lexer engine ( new h2 lexer ( sql ) ) ; case sql : return new lexer engine ( new sql ( sql ) ) ; case oracle : return new lexer engine ( new oracle lexer ( sql ) ) ; case sql : return new lexer engine ( new sql ( sql ) ) ; case sql : return new lexer engine ( new sql ( sql ) ) ; default : throw new unsupported operation exception ( string . format ( str , db type ) ) ; } }	Create lexical analysis engine instance.
public boolean execute ( final int [ ] column indexes ) throws sql { return execute ( new executor ( ) { @ override public boolean execute ( final statement statement , final string sql ) throws sql { return statement . execute ( sql , column indexes ) ; } } ) ; }	Execute SQL with column indexes.
public static boolean is tcl ( final database type database type , final token type token type , final lexer engine lexer engine ) { if ( default keyword . set . equals ( token type ) || database type . sql . equals ( database type ) && default keyword . if . equals ( token type ) ) { lexer engine . skip until ( default keyword . transaction , default keyword . autocommit , default keyword . implicit transactions ) ; if ( ! lexer engine . is end ( ) ) { return bool ; } } return bool ; }	Is TCL statement.
public void add batch for route units ( final sql route result ) { handle old route units ( create batch route units ( route result . get route units ( ) ) ) ; handle new route units ( create batch route units ( route result . get route units ( ) ) ) ; batch count ++ ; }	Add batch for route units.
public logic schema get logic schema ( final string schema name ) { return strings . is null or empty ( schema name ) ? null : logic schemas . get ( schema name ) ; }	Get logic schema.
@ subscribe public synchronized void renew ( final schema added event schema added event ) { logic schemas . put ( schema added event . get sharding schema name ( ) , create logic schema ( schema added event . get sharding schema name ( ) , collections . singleton map ( schema added event . get sharding schema name ( ) , data source converter . get data source parameter map ( schema added event . get data source configurations ( ) ) ) , schema added event . get rule configuration ( ) , bool ) ) ; }	Renew to add new schema.
public static command executor new instance ( final sql command packet type , final sql command packet , final backend connection backend connection ) { log . debug ( str , command packet type , command packet ) ; switch ( command packet type ) { case query : return new sql ( ( sql ) command packet , backend connection ) ; case parse : return new sql ( ( sql ) command packet , backend connection ) ; case bind : return new sql ( ( sql ) command packet , backend connection ) ; case describe : return new sql ( ) ; case execute : return new sql ( ) ; case sync : return new sql ( ) ; case terminate : return new sql ( ) ; default : return new sql ( ) ; } }	Create new instance of command executor.
public final void next token ( ) { skip ignored token ( ) ; if ( is variable begin ( ) ) { current token = new tokenizer ( input , dictionary , offset ) . scan variable ( ) ; } else if ( is n ( ) ) { current token = new tokenizer ( input , dictionary , ++ offset ) . scan chars ( ) ; } else if ( is identifier begin ( ) ) { current token = new tokenizer ( input , dictionary , offset ) . scan identifier ( ) ; } else if ( is hex decimal begin ( ) ) { current token = new tokenizer ( input , dictionary , offset ) . scan hex decimal ( ) ; } else if ( is number begin ( ) ) { current token = new tokenizer ( input , dictionary , offset ) . scan number ( ) ; } else if ( is symbol begin ( ) ) { current token = new tokenizer ( input , dictionary , offset ) . scan symbol ( ) ; } else if ( is chars begin ( ) ) { current token = new tokenizer ( input , dictionary , offset ) . scan chars ( ) ; } else if ( is end ( ) ) { current token = new token ( assist . end , str , offset ) ; } else { throw new sql ( this , assist . error ) ; } offset = current token . get end position ( ) ; }	Analyse next token.
public final void parse ( ) { collection < keyword > unsupported rest keywords = new linked list < > ( ) ; unsupported rest keywords . add all ( arrays . as list ( default keyword . union , default keyword . intersect , default keyword . except , default keyword . minus ) ) ; unsupported rest keywords . add all ( arrays . as list ( get unsupported keywords rest ( ) ) ) ; lexer engine . unsupported if equal ( unsupported rest keywords . to array ( new keyword [ unsupported rest keywords . size ( ) ] ) ) ; }	Parse select rest.
public boolean is single table ( ) { collection < string > table names = new tree set < > ( string . case insensitive order ) ; for ( table each : tables ) { table names . add ( each . get name ( ) ) ; } return num == table names . size ( ) ; }	Judge is single table or not.
public collection < string > get table names ( ) { collection < string > result = new linked hash set < > ( tables . size ( ) , num ) ; for ( table each : tables ) { result . add ( each . get name ( ) ) ; } return result ; }	Get table names.
public optional < table > find ( final string table name or alias ) { optional < table > table from name = find table from name ( table name or alias ) ; return table from name . is present ( ) ? table from name : find table from alias ( table name or alias ) ; }	Find table via table name or alias.
public string skip parentheses ( final sql sql statement ) { string builder result = new string builder ( str ) ; int count = num ; if ( symbol . left paren == lexer . get current token ( ) . get type ( ) ) { final int begin position = lexer . get current token ( ) . get end position ( ) ; result . append ( symbol . left paren . get literals ( ) ) ; lexer . next token ( ) ; while ( bool ) { if ( equal any ( symbol . question ) ) { sql statement . set parameters index ( sql statement . get parameters index ( ) + num ) ; } if ( assist . end == lexer . get current token ( ) . get type ( ) || ( symbol . right paren == lexer . get current token ( ) . get type ( ) && num == count ) ) { break ; } if ( symbol . left paren == lexer . get current token ( ) . get type ( ) ) { count ++ ; } else if ( symbol . right paren == lexer . get current token ( ) . get type ( ) ) { count -- ; } lexer . next token ( ) ; } result . append ( lexer . get input ( ) . substring ( begin position , lexer . get current token ( ) . get end position ( ) ) ) ; lexer . next token ( ) ; } return result . to string ( ) ; }	skip all tokens that inside parentheses.
public void accept ( final token type token type ) { if ( lexer . get current token ( ) . get type ( ) != token type ) { throw new sql ( lexer , token type ) ; } lexer . next token ( ) ; }	Assert current token type should equals input token and go to next token type.
public boolean equal any ( final token type ... token types ) { for ( token type each : token types ) { if ( each == lexer . get current token ( ) . get type ( ) ) { return bool ; } } return bool ; }	Judge current token equals one of input tokens or not.
public void skip all ( final token type ... token types ) { set < token type > token type set = sets . new hash set ( token types ) ; while ( token type set . contains ( lexer . get current token ( ) . get type ( ) ) ) { lexer . next token ( ) ; } }	Skip all input tokens.
public void skip until ( final token type ... token types ) { set < token type > token type set = sets . new hash set ( token types ) ; token type set . add ( assist . end ) ; while ( ! token type set . contains ( lexer . get current token ( ) . get type ( ) ) ) { lexer . next token ( ) ; } }	Skip until one of input tokens.
public void parse ( final select statement select statement ) { if ( ! lexer engine . skip if equal ( sql . top ) ) { return ; } int begin position = lexer engine . get current token ( ) . get end position ( ) ; if ( ! lexer engine . skip if equal ( symbol . left paren ) ) { begin position = lexer engine . get current token ( ) . get end position ( ) - lexer engine . get current token ( ) . get literals ( ) . length ( ) ; } sql sql expression = basic expression parser . parse ( select statement ) ; lexer engine . skip if equal ( symbol . right paren ) ; limit value row count value ; if ( sql expression instanceof sql ) { int row count = ( ( sql ) sql expression ) . get number ( ) . int value ( ) ; row count value = new limit value ( row count , - num , bool ) ; select statement . add sql ( new row count token ( begin position , row count ) ) ; } else if ( sql expression instanceof sql ) { row count value = new limit value ( - num , ( ( sql ) sql expression ) . get index ( ) , bool ) ; } else { throw new sql ( lexer engine ) ; } lexer engine . unsupported if equal ( sql . percent ) ; lexer engine . skip if equal ( default keyword . with , sql . ties ) ; if ( null == select statement . get limit ( ) ) { limit limit = new limit ( ) ; limit . set row count ( row count value ) ; select statement . set limit ( limit ) ; } else { select statement . get limit ( ) . set row count ( row count value ) ; } }	Parse top.
public static sql new instance ( final int sequence id , final exception cause ) { if ( cause instanceof sql ) { sql sql exception = ( sql ) cause ; return new sql ( sequence id , sql exception . get error code ( ) , sql exception . get sql ( ) , sql exception . get message ( ) ) ; } if ( cause instanceof ctl ) { ctl sharding ctl = ( ctl ) cause ; return new sql ( sequence id , ctl . value of ( sharding ctl ) , sharding ctl . get sharding ctl ( ) ) ; } if ( cause instanceof table modify in transaction exception ) { return new sql ( sequence id , sql . er error on modifying gtid executed table , ( ( table modify in transaction exception ) cause ) . get table name ( ) ) ; } if ( cause instanceof unknown database exception ) { return new sql ( sequence id , sql . er bad db error , ( ( unknown database exception ) cause ) . get database name ( ) ) ; } if ( cause instanceof no database selected exception ) { return new sql ( sequence id , sql . er no db error ) ; } return new sql ( sequence id , common error code . unknown exception , cause . get message ( ) ) ; }	New instance of MytSQL ERR packet.
public static < t > void register ( final class < t > service ) { for ( t each : service loader . load ( service ) ) { register service class ( service , each ) ; } }	Register SPI service into map for new instance.
@ sneaky throws @ suppress warnings ( str ) public static < t > collection < t > new service instances ( final class < t > service ) { collection < t > result = new linked list < > ( ) ; if ( null == service map . get ( service ) ) { return result ; } for ( class < ? > each : service map . get ( service ) ) { result . add ( ( t ) each . new instance ( ) ) ; } return result ; }	New service instances.
public static void main ( final string [ ] args ) throws io { sharding configuration sharding config = new sharding configuration loader ( ) . load ( ) ; int port = get port ( args ) ; if ( null == sharding config . get server configuration ( ) . get orchestration ( ) ) { start without registry center ( sharding config . get rule configuration map ( ) , sharding config . get server configuration ( ) . get authentication ( ) , sharding config . get server configuration ( ) . get props ( ) , port ) ; } else { start with registry center ( sharding config . get server configuration ( ) , sharding config . get rule configuration map ( ) . key set ( ) , sharding config . get rule configuration map ( ) , port ) ; } }	Main entrance.
public collection < sharding execute group < statement execute unit > > get execute unit groups ( final collection < route unit > route units , final sql callback ) throws sql { return get synchronized execute unit groups ( route units , callback ) ; }	Get execute unit groups.
public list < comparable < ? > > get condition values ( final list < ? > parameters ) { list < comparable < ? > > result = new linked list < > ( position value map . values ( ) ) ; for ( entry < integer , integer > entry : position index map . entry set ( ) ) { object parameter = parameters . get ( entry . get value ( ) ) ; if ( ! ( parameter instanceof comparable < ? > ) ) { throw new sharding exception ( str , parameter ) ; } if ( entry . get key ( ) < result . size ( ) ) { result . add ( entry . get key ( ) , ( comparable < ? > ) parameter ) ; } else { result . add ( ( comparable < ? > ) parameter ) ; } } return result ; }	Get condition values.
public static data source configuration get data source configuration ( final data source data source ) { data source configuration result = new data source configuration ( data source . get class ( ) . get name ( ) ) ; result . get properties ( ) . put all ( find all getter properties ( data source ) ) ; return result ; }	Get data source configuration.
@ sneaky throws public data source create data source ( ) { data source result = ( data source ) class . for name ( data source class name ) . new instance ( ) ; method [ ] methods = result . get class ( ) . get methods ( ) ; for ( entry < string , object > entry : properties . entry set ( ) ) { if ( skipped property names . contains ( entry . get key ( ) ) ) { continue ; } optional < method > setter method = find setter method ( methods , entry . get key ( ) ) ; if ( setter method . is present ( ) ) { setter method . get ( ) . invoke ( result , entry . get value ( ) ) ; } } return result ; }	Create data source.
public void set status ( final connection status update ) { status . get and set ( update ) ; if ( connection status . terminated == status . get ( ) ) { resource synchronizer . do notify ( ) ; } }	Change connection status using get and set.
public void set running status if necessary ( ) { if ( connection status . transaction != status . get ( ) && connection status . running != status . get ( ) ) { status . get and set ( connection status . running ) ; } }	Change connection status to running if necessary.
void do notify if necessary ( ) { if ( status . compare and set ( connection status . running , connection status . release ) || status . compare and set ( connection status . terminated , connection status . release ) ) { resource synchronizer . do notify ( ) ; } }	Notify connection to finish wait if necessary.
public void wait until connection released if necessary ( ) throws interrupted exception { if ( connection status . running == status . get ( ) || connection status . terminated == status . get ( ) ) { while ( ! status . compare and set ( connection status . release , connection status . running ) ) { resource synchronizer . do await until ( ) ; } } }	Wait until connection is released if necessary.
public order item create order item ( ) { if ( order by item segment instanceof index order by item segment ) { return create order item ( ( index order by item segment ) order by item segment ) ; } if ( order by item segment instanceof column order by item segment ) { return create order item ( select statement , ( column order by item segment ) order by item segment ) ; } if ( order by item segment instanceof expression order by item segment ) { return create order item ( select statement , ( expression order by item segment ) order by item segment ) ; } throw new unsupported operation exception ( ) ; }	Create order item.
public void init listeners ( ) { instance state changed listener . watch ( changed type . updated ) ; data source state changed listener . watch ( changed type . updated , changed type . deleted ) ; }	Initialize all state changed listeners.
public void set transaction type ( final transaction type transaction type ) { if ( null == schema name ) { throw new sharding exception ( str ) ; } if ( is switch failed ( ) ) { throw new sharding exception ( str ) ; } this . transaction type = transaction type ; }	Change transaction type of current channel.
public void set current schema ( final string schema name ) { if ( is switch failed ( ) ) { throw new sharding exception ( str ) ; } this . schema name = schema name ; this . logic schema = logic schemas . get instance ( ) . get logic schema ( schema name ) ; }	Change logic schema of current channel.
public list < connection > get connections ( final connection mode connection mode , final string data source name , final int connection size ) throws sql { if ( state handler . is in transaction ( ) ) { return get connections with transaction ( connection mode , data source name , connection size ) ; } else { return get connections without transaction ( connection mode , data source name , connection size ) ; } }	Get connections of current thread datasource.
public synchronized void close ( final boolean force close ) throws sql { collection < sql > exceptions = new linked list < > ( ) ; master visited manager . clear ( ) ; exceptions . add all ( close result sets ( ) ) ; exceptions . add all ( close statements ( ) ) ; if ( ! state handler . is in transaction ( ) || force close ) { exceptions . add all ( release connections ( force close ) ) ; } state handler . do notify if necessary ( ) ; throw sql ( exceptions ) ; }	Close cached connection.
public void parse ( final insert statement insert statement ) { if ( ! lexer engine . skip if equal ( get customized insert keywords ( ) ) ) { return ; } lexer engine . accept ( default keyword . duplicate ) ; lexer engine . accept ( default keyword . key ) ; lexer engine . accept ( default keyword . update ) ; do { column column = new column ( sql . get exactly value ( lexer engine . get current token ( ) . get literals ( ) ) , insert statement . get tables ( ) . get single table name ( ) ) ; if ( sharding rule . is sharding column ( column . get name ( ) , column . get table name ( ) ) ) { throw new sql ( str , lexer engine . get current token ( ) . get type ( ) , lexer engine . get current token ( ) . get literals ( ) ) ; } basic expression parser . parse ( insert statement ) ; lexer engine . accept ( symbol . eq ) ; if ( lexer engine . skip if equal ( default keyword . values ) ) { lexer engine . accept ( symbol . left paren ) ; basic expression parser . parse ( insert statement ) ; lexer engine . accept ( symbol . right paren ) ; } else { lexer engine . next token ( ) ; } } while ( lexer engine . skip if equal ( symbol . comma ) ) ; }	Parse insert duplicate key update.
public static optional < condition > create compare condition ( final predicate compare right value compare right value , final column column ) { return compare right value . get expression ( ) instanceof simple expression segment ? optional . of ( new condition ( column , ( ( simple expression segment ) compare right value . get expression ( ) ) . get sql ( ) ) ) : optional . < condition > absent ( ) ; }	Create condition of compare operator.
public static optional < condition > create in condition ( final predicate in right value in right value , final column column ) { list < sql > sql expressions = new linked list < > ( ) ; for ( expression segment each : in right value . get sql expressions ( ) ) { if ( ! ( each instanceof simple expression segment ) ) { sql expressions . clear ( ) ; break ; } else { sql expressions . add ( ( ( simple expression segment ) each ) . get sql ( ) ) ; } } return sql expressions . is empty ( ) ? optional . < condition > absent ( ) : optional . of ( new condition ( column , sql expressions ) ) ; }	Create condition of IN operator.
public static optional < condition > create between condition ( final predicate between right value between right value , final column column ) { return between right value . get between expression ( ) instanceof simple expression segment && between right value . get and expression ( ) instanceof simple expression segment ? optional . of ( new condition ( column , ( ( simple expression segment ) between right value . get between expression ( ) ) . get sql ( ) , ( ( simple expression segment ) between right value . get and expression ( ) ) . get sql ( ) ) ) : optional . < condition > absent ( ) ; }	Create condition of BETWEEN ..
public void init ( final database type database type , final map < string , data source > data source map ) { for ( entry < transaction type , sharding transaction manager > entry : transaction manager map . entry set ( ) ) { entry . get value ( ) . init ( database type , get resource data sources ( data source map ) ) ; } }	Initialize sharding transaction managers.
public sharding transaction manager get transaction manager ( final transaction type transaction type ) { sharding transaction manager result = transaction manager map . get ( transaction type ) ; if ( transaction type . local != transaction type ) { preconditions . check not null ( result , str , transaction type ) ; } return result ; }	Get sharding transaction manager.
public void close ( ) throws exception { for ( entry < transaction type , sharding transaction manager > entry : transaction manager map . entry set ( ) ) { entry . get value ( ) . close ( ) ; } }	Close sharding transaction managers.
public static abstract update parser new instance ( final database type db type , final sharding rule sharding rule , final lexer engine lexer engine ) { switch ( db type ) { case h2 : case sql : return new sql ( sharding rule , lexer engine ) ; case oracle : return new oracle update parser ( sharding rule , lexer engine ) ; case sql : return new sql ( sharding rule , lexer engine ) ; case sql : return new sql ( sharding rule , lexer engine ) ; default : throw new unsupported operation exception ( string . format ( str , db type ) ) ; } }	Create update parser instance.
public static string get sql ( final string root dir , final database type database type ) { return joiner . on ( str ) . join ( root dir , database type . name ( ) . to lower case ( ) , sql statement rule definition file name ) ; }	Get SQL statement rule definition file name.
public static optional < generated key > get generate key ( final sharding rule sharding rule , final list < object > parameters , final insert statement insert statement ) { optional < string > generate key column name = sharding rule . find generate key column name ( insert statement . get tables ( ) . get single table name ( ) ) ; if ( ! generate key column name . is present ( ) ) { return optional . absent ( ) ; } return is contains generate key column ( insert statement , generate key column name . get ( ) ) ? find generated key ( parameters , insert statement , generate key column name . get ( ) ) : optional . of ( create generated key ( sharding rule , insert statement , generate key column name . get ( ) ) ) ; }	Get generate key.
public static string get driver class name ( final string url ) { for ( entry < string , string > entry : url prefix and driver class name mapper . entry set ( ) ) { if ( url . starts with ( entry . get key ( ) ) ) { return entry . get value ( ) ; } } throw new sharding exception ( str , url , jdbc url . class . get name ( ) ) ; }	Get JDBC driver class name.
public static data source meta data new instance ( final database type database type , final string url ) { switch ( database type ) { case h2 : return new h2 data source meta data ( url ) ; case sql : return new sql ( url ) ; case oracle : return new oracle data source meta data ( url ) ; case sql : return new sql ( url ) ; case sql : return new sql ( url ) ; default : throw new unsupported operation exception ( string . format ( str , database type ) ) ; } }	Create new instance of data source meta data.
@ sneaky throws public static xa build ( final database type database type , final data source data source ) { xa result = create xa ( database type ) ; properties xa properties = xa . create xa ( database type ) . build ( swapper . swap ( data source ) ) ; property utils . set properties ( result , xa properties ) ; return result ; }	Create XA data source through general data source.
public sql judge ( ) { lexer engine lexer engine = lexer engine factory . new instance ( database type . sql , sql ) ; lexer engine . next token ( ) ; while ( bool ) { token type token type = lexer engine . get current token ( ) . get type ( ) ; if ( token type instanceof keyword ) { if ( dql . is dql ( token type ) ) { return get dql ( ) ; } if ( dml . is dml ( token type ) ) { return get dml ( token type ) ; } if ( tcl . is tcl ( token type ) ) { return get tcl ( ) ; } if ( dal . is dal ( token type ) ) { return get dal ( token type , lexer engine ) ; } lexer engine . next token ( ) ; token type secondary token type = lexer engine . get current token ( ) . get type ( ) ; if ( ddl . is ddl ( token type , secondary token type ) ) { return get ddl ( ) ; } if ( dcl . is dcl ( token type , secondary token type ) ) { return get dcl ( ) ; } if ( tcl . is tcl ( database type . sql , token type , lexer engine ) ) { return get tcl ( ) ; } if ( default keyword . set . equals ( token type ) ) { return new set statement ( ) ; } } else { lexer engine . next token ( ) ; } if ( sql . to upper case ( ) . starts with ( str ) ) { return get dql ( ) ; } if ( token type instanceof assist && assist . end == token type ) { throw new sql ( str , sql ) ; } } }	Judge SQL type only.
public object get cell ( final int column index ) { preconditions . check argument ( column index > num && column index < data . length + num ) ; return data [ column index - num ] ; }	Get data from cell.
public void set cell ( final int column index , final object value ) { preconditions . check argument ( column index > num && column index < data . length + num ) ; data [ column index - num ] = value ; }	Set data for cell.
public void parse ( final insert statement insert statement , final sharding table meta data sharding table meta data ) { string table name = insert statement . get tables ( ) . get single table name ( ) ; insert statement . get column names ( ) . add all ( lexer engine . equal any ( symbol . left paren ) ? parse with column ( insert statement ) : parse without column ( sharding table meta data , table name ) ) ; }	Parse insert columns.
public static abstract delete parser new instance ( final database type db type , final sharding rule sharding rule , final lexer engine lexer engine ) { switch ( db type ) { case h2 : case sql : return new sql ( sharding rule , lexer engine ) ; case oracle : return new oracle delete parser ( sharding rule , lexer engine ) ; case sql : return new sql ( sharding rule , lexer engine ) ; case sql : return new sql ( sharding rule , lexer engine ) ; default : throw new unsupported operation exception ( string . format ( str , db type ) ) ; } }	Create delete parser instance.
public static abstract show parser new instance ( final database type db type , final sharding rule sharding rule , final lexer engine lexer engine ) { switch ( db type ) { case h2 : case sql : return new sql ( sharding rule , lexer engine ) ; default : throw new unsupported operation exception ( string . format ( str , db type ) ) ; } }	Create show parser instance.
public static boolean is ddl ( final token type primary token type , final token type secondary token type ) { return primary statement prefix . contains ( primary token type ) && ! not secondary statement prefix . contains ( secondary token type ) ; }	Is DDL statement.
public void init ( final map < string , map < string , data source configuration > > data source configuration map , final map < string , rule configuration > schema rule map , final authentication authentication , final properties props ) { for ( entry < string , map < string , data source configuration > > entry : data source configuration map . entry set ( ) ) { config service . persist configuration ( entry . get key ( ) , data source configuration map . get ( entry . get key ( ) ) , schema rule map . get ( entry . get key ( ) ) , authentication , props , is overwrite ) ; } state service . persist instance online ( ) ; state service . persist data sources node ( ) ; listener manager . init listeners ( ) ; }	Initialize for orchestration.
public void parse ( final sharding rule sharding rule , final sql sql statement , final list < select item > items ) { alias expression parser . parse table alias ( ) ; if ( lexer engine . skip if equal ( default keyword . where ) ) { parse where ( sharding rule , sql statement , items ) ; } }	Parse where.
public optional < sql > find sql ( final database type database type , final string context class name ) { return optional . from nullable ( parser rule definitions . get ( database type . h2 == database type ? database type . sql : database type ) . get sql statement rule definition ( ) . get rules ( ) . get ( context class name ) ) ; }	Find SQL statement rule.
public optional < sql > find sql ( final database type database type , final class < ? extends sql > sql segment class ) { return optional . from nullable ( parser rule definitions . get ( database type . h2 == database type ? database type . sql : database type ) . get filler rule definition ( ) . get rules ( ) . get ( sql segment class ) ) ; }	Find SQL segment rule.
public string read string nul ( ) { byte [ ] result = new byte [ byte buf . bytes before ( ( byte ) num ) ] ; byte buf . read bytes ( result ) ; byte buf . skip bytes ( num ) ; return new string ( result ) ; }	Read null terminated string from byte buffers.
public static text protocol backend handler new instance ( final string sql , final backend connection backend connection ) { if ( sql . to upper case ( ) . starts with ( sctl set ) ) { return new ctl ( sql , backend connection ) ; } if ( sql . to upper case ( ) . starts with ( sctl show ) ) { return new ctl ( sql , backend connection ) ; } if ( sql . to upper case ( ) . starts with ( sctl explain ) ) { return new ctl ( sql , backend connection ) ; } throw new illegal argument exception ( sql ) ; }	Create new instance of sharding CTL backend handler.
public void optimize ( final sql rule , final sql sql statement ) { optional < sql > optimizer = rule . get optimizer ( ) ; if ( optimizer . is present ( ) ) { optimizer . get ( ) . optimize ( sql statement , sharding table meta data ) ; } }	Optimize SQL statement.
public static parser rule context get first child node ( final parser rule context node , final rule name rule name ) { optional < parser rule context > result = find first child node ( node , rule name ) ; preconditions . check state ( result . is present ( ) ) ; return result . get ( ) ; }	Get first child node.
public static optional < parser rule context > find first child node ( final parser rule context node , final rule name rule name ) { queue < parser rule context > parser rule contexts = new linked list < > ( ) ; parser rule contexts . add ( node ) ; parser rule context parser rule context ; while ( null != ( parser rule context = parser rule contexts . poll ( ) ) ) { if ( is matched node ( parser rule context , rule name ) ) { return optional . of ( parser rule context ) ; } for ( int i = num ; i < parser rule context . get child count ( ) ; i ++ ) { if ( parser rule context . get child ( i ) instanceof parser rule context ) { parser rule contexts . add ( ( parser rule context ) parser rule context . get child ( i ) ) ; } } } return optional . absent ( ) ; }	Find first child node.
public static optional < parser rule context > find first child node none recursive ( final parser rule context node , final rule name rule name ) { if ( is matched node ( node , rule name ) ) { return optional . of ( node ) ; } for ( int i = num ; i < node . get child count ( ) ; i ++ ) { if ( node . get child ( i ) instanceof parser rule context ) { parser rule context child = ( parser rule context ) node . get child ( i ) ; if ( is matched node ( child , rule name ) ) { return optional . of ( child ) ; } } } return optional . absent ( ) ; }	Find first child node none recursive.
public static optional < parser rule context > find single node from first descendant ( final parser rule context node , final rule name rule name ) { parser rule context next node = node ; do { if ( is matched node ( next node , rule name ) ) { return optional . of ( next node ) ; } if ( num != next node . get child count ( ) || ! ( next node . get child ( num ) instanceof parser rule context ) ) { return optional . absent ( ) ; } next node = ( parser rule context ) next node . get child ( num ) ; } while ( null != next node ) ; return optional . absent ( ) ; }	Find single node from first descendant which only has one child.
public static collection < parser rule context > get all descendant nodes ( final parser rule context node , final rule name rule name ) { collection < parser rule context > result = new linked list < > ( ) ; if ( is matched node ( node , rule name ) ) { result . add ( node ) ; } for ( parser rule context each : get children nodes ( node ) ) { result . add all ( get all descendant nodes ( each , rule name ) ) ; } return result ; }	Get all descendant nodes.
public boolean contains column ( final string table name , final string column ) { return contains table ( table name ) && tables . get ( table name ) . get columns ( ) . key set ( ) . contains ( column . to lower case ( ) ) ; }	Judge contains column from table meta data or not.
public collection < string > get all column names ( final string table name ) { return tables . contains key ( table name ) ? tables . get ( table name ) . get columns ( ) . key set ( ) : collections . < string > empty list ( ) ; }	Get all column names via table.
public static aggregation unit create ( final aggregation type type ) { switch ( type ) { case max : return new comparable aggregation unit ( bool ) ; case min : return new comparable aggregation unit ( bool ) ; case sum : case count : return new accumulation aggregation unit ( ) ; case avg : return new average aggregation unit ( ) ; default : throw new unsupported operation exception ( type . name ( ) ) ; } }	Create aggregation unit instance.
public void execute ( final collection < t > targets , final force execute callback < t > callback ) throws sql { collection < sql > exceptions = new linked list < > ( ) ; for ( t each : targets ) { try { callback . execute ( each ) ; } catch ( final sql ex ) { exceptions . add ( ex ) ; } } throw sql ( exceptions ) ; }	Force execute.
public final t new service ( final string type , final properties props ) { collection < t > type based services = load type based services ( type ) ; if ( type based services . is empty ( ) ) { throw new sharding configuration exception ( str , class type . get name ( ) , type ) ; } t result = type based services . iterator ( ) . next ( ) ; result . set properties ( props ) ; return result ; }	Create new instance for type based SPI.
public static void set error ( final span span , final exception cause ) { span . set tag ( tags . error . get key ( ) , bool ) . log ( system . current time millis ( ) , get reason ( cause ) ) ; }	Set error.
public static sharding router new instance ( final sharding rule sharding rule , final sharding meta data sharding meta data , final database type database type , final parsing result cache parsing result cache ) { return hint manager . is database sharding only ( ) ? new sql ( sharding rule ) : new sql ( sharding rule , sharding meta data , database type , parsing result cache ) ; }	Create new instance of sharding router.
public static sql < integer > get prepared update sql ( final database type database type , final boolean is exception thrown ) { return new sql < integer > ( database type , is exception thrown ) { @ override protected integer execute sql ( final route unit route unit , final statement statement , final connection mode connection mode ) throws sql { return ( ( prepared statement ) statement ) . execute update ( ) ; } } ; }	Get update callback.
public static sql < boolean > get prepared sql ( final database type database type , final boolean is exception thrown ) { return new sql < boolean > ( database type , is exception thrown ) { @ override protected boolean execute sql ( final route unit route unit , final statement statement , final connection mode connection mode ) throws sql { return ( ( prepared statement ) statement ) . execute ( ) ; } } ; }	Get execute callback.
public boolean has logic table ( final string logic table name ) { for ( table rule each : table rules ) { if ( each . get logic table ( ) . equals ( logic table name . to lower case ( ) ) ) { return bool ; } } return bool ; }	Judge contains this logic table in this rule.
public string get binding actual table ( final string data source , final string logic table , final string other actual table ) { int index = - num ; for ( table rule each : table rules ) { index = each . find actual table index ( data source , other actual table ) ; if ( - num != index ) { break ; } } if ( - num == index ) { throw new sharding configuration exception ( str , data source , other actual table ) ; } for ( table rule each : table rules ) { if ( each . get logic table ( ) . equals ( logic table . to lower case ( ) ) ) { return each . get actual data nodes ( ) . get ( index ) . get table name ( ) . to lower case ( ) ; } } throw new sharding configuration exception ( str , data source , logic table , other actual table ) ; }	Deduce actual table name from other actual table name in same binding table rule.
public final void parse ( final select statement select statement ) { if ( ! lexer engine . skip if equal ( default keyword . group ) ) { return ; } lexer engine . accept ( default keyword . by ) ; while ( bool ) { add group by item ( basic expression parser . parse ( select statement ) , select statement ) ; if ( ! lexer engine . equal any ( symbol . comma ) ) { break ; } lexer engine . next token ( ) ; } lexer engine . skip all ( get skipped keyword after group by ( ) ) ; select statement . set group by last index ( lexer engine . get current token ( ) . get end position ( ) - lexer engine . get current token ( ) . get literals ( ) . length ( ) - num ) ; }	Parse group by.
public static abstract use parser new instance ( final database type db type , final sharding rule sharding rule , final lexer engine lexer engine ) { switch ( db type ) { case h2 : case sql : return new sql ( lexer engine ) ; default : throw new unsupported operation exception ( string . format ( str , db type ) ) ; } }	Create use parser instance.
public long read int lenenc ( ) { int first byte = read int1 ( ) ; if ( first byte < num ) { return first byte ; } if ( num == first byte ) { return num ; } if ( num == first byte ) { return byte buf . read short le ( ) ; } if ( num == first byte ) { return byte buf . read medium le ( ) ; } return byte buf . read long le ( ) ; }	Read lenenc integer from byte buffers.
public void write int lenenc ( final long value ) { if ( value < num ) { byte buf . write byte ( ( int ) value ) ; return ; } if ( value < math . pow ( num , num ) ) { byte buf . write byte ( num ) ; byte buf . write short le ( ( int ) value ) ; return ; } if ( value < math . pow ( num , num ) ) { byte buf . write byte ( num ) ; byte buf . write medium le ( ( int ) value ) ; return ; } byte buf . write byte ( num ) ; byte buf . write long le ( value ) ; }	Write lenenc integer to byte buffers.
public string read string lenenc ( ) { int length = ( int ) read int lenenc ( ) ; byte [ ] result = new byte [ length ] ; byte buf . read bytes ( result ) ; return new string ( result ) ; }	Read lenenc string from byte buffers.
public void write string lenenc ( final string value ) { if ( strings . is null or empty ( value ) ) { byte buf . write byte ( num ) ; return ; } write int lenenc ( value . get bytes ( ) . length ) ; byte buf . write bytes ( value . get bytes ( ) ) ; }	Write lenenc string to byte buffers.
public void write bytes lenenc ( final byte [ ] value ) { if ( num == value . length ) { byte buf . write byte ( num ) ; return ; } write int lenenc ( value . length ) ; byte buf . write bytes ( value ) ; }	Write lenenc bytes to byte buffers.
public string read string fix ( final int length ) { byte [ ] result = new byte [ length ] ; byte buf . read bytes ( result ) ; return new string ( result ) ; }	Read fixed length string from byte buffers.
public byte [ ] read string nul by bytes ( ) { byte [ ] result = new byte [ byte buf . bytes before ( ( byte ) num ) ] ; byte buf . read bytes ( result ) ; byte buf . skip bytes ( num ) ; return result ; }	Read null terminated string from byte buffers and return bytes.
public string read string eof ( ) { byte [ ] result = new byte [ byte buf . readable bytes ( ) ] ; byte buf . read bytes ( result ) ; return new string ( result ) ; }	Read rest of packet string from byte buffers.
public map < column , list < condition > > get conditions map ( ) { map < column , list < condition > > result = new linked hash map < > ( conditions . size ( ) , num ) ; for ( condition each : conditions ) { if ( ! result . contains key ( each . get column ( ) ) ) { result . put ( each . get column ( ) , new linked list < condition > ( ) ) ; } result . get ( each . get column ( ) ) . add ( each ) ; } return result ; }	Get conditions map.
public and condition optimize ( ) { and condition result = new and condition ( ) ; for ( condition each : conditions ) { if ( condition . class . equals ( each . get class ( ) ) ) { result . get conditions ( ) . add ( each ) ; } } if ( result . get conditions ( ) . is empty ( ) ) { result . get conditions ( ) . add ( new null condition ( ) ) ; } return result ; }	Optimize and condition.
@ suppress warnings ( str ) @ sneaky throws public void init ( final sql dialect rule definition entity , final extractor rule definition extractor rule definition ) { for ( sql each : dialect rule definition entity . get rules ( ) ) { sql sql statement rule = new sql ( each . get context ( ) , ( class < ? extends sql > ) class . for name ( each . get sql statement class ( ) ) , ( sql ) new class instance ( each . get optimizer class ( ) ) ) ; sql statement rule . get extractors ( ) . add all ( create extractors ( each . get extractor rule refs ( ) , extractor rule definition ) ) ; rules . put ( get context class name ( each . get context ( ) ) , sql statement rule ) ; } }	Initialize SQL statement rule definition.
@ sneaky throws public void start ( final int port ) { try { server bootstrap bootstrap = new server bootstrap ( ) ; boss group = create event loop group ( ) ; if ( boss group instanceof epoll event loop group ) { groups epoll ( bootstrap ) ; } else { groups nio ( bootstrap ) ; } channel future future = bootstrap . bind ( port ) . sync ( ) ; future . channel ( ) . close future ( ) . sync ( ) ; } finally { worker group . shutdown gracefully ( ) ; boss group . shutdown gracefully ( ) ; backend executor context . get instance ( ) . get execute engine ( ) . close ( ) ; } }	Start Sharding-Proxy.
public void init listeners ( ) { schema changed listener . watch ( changed type . updated , changed type . deleted ) ; properties changed listener . watch ( changed type . updated ) ; authentication changed listener . watch ( changed type . updated ) ; }	Initialize all configuration changed listeners.
@ subscribe @ sneaky throws public final synchronized void renew ( final data source changed event data source changed event ) { data source . close ( ) ; data source = new master slave data source ( data source converter . get data source map ( data source changed event . get data source configurations ( ) ) , data source . get master slave rule ( ) , data source . get sharding properties ( ) . get props ( ) ) ; }	Renew master-slave data source.
@ suppress warnings ( { str , str } ) public static int compare to ( final comparable this value , final comparable other value , final order direction order direction , final order direction null order direction ) { if ( null == this value && null == other value ) { return num ; } if ( null == this value ) { return order direction == null order direction ? - num : num ; } if ( null == other value ) { return order direction == null order direction ? num : - num ; } return order direction . asc == order direction ? this value . compare to ( other value ) : - this value . compare to ( other value ) ; }	Compare two object with order type.
public static xa create xa ( final database type database type ) { switch ( database type ) { case h2 : return new xa ( ) ; case sql : return new sqlxa ( ) ; case sql : return new sqlxa ( ) ; case oracle : return new xa ( ) ; case sql : return new sql xa ( ) ; default : throw new unsupported operation exception ( string . format ( str , database type ) ) ; } }	Create XA properties.
public void add ( final condition condition ) { if ( and conditions . is empty ( ) ) { and conditions . add ( new and condition ( ) ) ; } and conditions . get ( num ) . get conditions ( ) . add ( condition ) ; }	Add condition.
public or condition optimize ( ) { for ( and condition each : and conditions ) { if ( each . get conditions ( ) . get ( num ) instanceof null condition ) { or condition result = new or condition ( ) ; result . add ( new null condition ( ) ) ; return result ; } } return this ; }	Optimize or condition.
public list < condition > find conditions ( final column column ) { list < condition > result = new linked list < > ( ) ; for ( and condition each : and conditions ) { result . add all ( collections2 . filter ( each . get conditions ( ) , new predicate < condition > ( ) { @ override public boolean apply ( final condition input ) { return input . get column ( ) . equals ( column ) ; } } ) ) ; } return result ; }	Find conditions by column.
public void reset column label ( final string schema ) { map < string , integer > label and index map = new hash map < > ( num , num ) ; label and index map . put ( schema , num ) ; reset label and index map ( label and index map ) ; }	Reset column label.
public optional < column definition segment > find column definition ( final string column name , final sharding table meta data sharding table meta data ) { optional < column definition segment > result = find column definition from meta data ( column name , sharding table meta data ) ; return result . is present ( ) ? result : find column definition from current add clause ( column name ) ; }	Find column definition.
public optional < column definition segment > find column definition from meta data ( final string column name , final sharding table meta data sharding table meta data ) { if ( ! sharding table meta data . contains table ( get tables ( ) . get single table name ( ) ) ) { return optional . absent ( ) ; } for ( column meta data each : sharding table meta data . get ( get tables ( ) . get single table name ( ) ) . get columns ( ) . values ( ) ) { if ( column name . equals ignore case ( each . get column name ( ) ) ) { return optional . of ( new column definition segment ( column name , each . get data type ( ) , each . is primary key ( ) ) ) ; } } return optional . absent ( ) ; }	Find column definition from meta data.
public collection < sql > extract ( final sqlast ast ) { collection < sql > result = new linked list < > ( ) ; preconditions . check state ( ast . get sql ( ) . is present ( ) ) ; map < parser rule context , integer > parameter marker indexes = get parameter marker indexes ( ast . get parser rule context ( ) ) ; for ( sql each : ast . get sql ( ) . get ( ) . get extractors ( ) ) { if ( each instanceof sql ) { optional < ? extends sql > sql segment = ( ( sql ) each ) . extract ( ast . get parser rule context ( ) , parameter marker indexes ) ; if ( sql segment . is present ( ) ) { result . add ( sql segment . get ( ) ) ; } } else if ( each instanceof sql ) { result . add all ( ( ( sql ) each ) . extract ( ast . get parser rule context ( ) , parameter marker indexes ) ) ; } } return result ; }	Extract SQL segments.
public static abstract select parser new instance ( final database type db type , final sharding rule sharding rule , final lexer engine lexer engine , final sharding table meta data sharding table meta data ) { switch ( db type ) { case h2 : case sql : return new sql ( sharding rule , lexer engine , sharding table meta data ) ; case oracle : return new oracle select parser ( sharding rule , lexer engine , sharding table meta data ) ; case sql : return new sql ( sharding rule , lexer engine , sharding table meta data ) ; case sql : return new sql ( sharding rule , lexer engine , sharding table meta data ) ; default : throw new unsupported operation exception ( string . format ( str , db type ) ) ; } }	Create select parser instance.
@ override public collection < string > get slave data source names ( ) { if ( disabled data source names . is empty ( ) ) { return super . get slave data source names ( ) ; } collection < string > result = new linked list < > ( super . get slave data source names ( ) ) ; result . remove all ( disabled data source names ) ; return result ; }	Get slave data source names.
public void update disabled data source names ( final string data source name , final boolean is disabled ) { if ( is disabled ) { disabled data source names . add ( data source name ) ; } else { disabled data source names . remove ( data source name ) ; } }	Update disabled data source names.
public optional < string > get alias ( final string name ) { if ( contain star ) { return optional . absent ( ) ; } string raw name = sql . get exactly value ( name ) ; for ( select item each : items ) { if ( sql . get exactly expression ( raw name ) . equals ignore case ( sql . get exactly expression ( sql . get exactly value ( each . get expression ( ) ) ) ) ) { return each . get alias ( ) ; } if ( raw name . equals ignore case ( each . get alias ( ) . or null ( ) ) ) { return optional . of ( raw name ) ; } } return optional . absent ( ) ; }	Get alias.
public list < aggregation select item > get aggregation select items ( ) { list < aggregation select item > result = new linked list < > ( ) ; for ( select item each : items ) { if ( each instanceof aggregation select item ) { aggregation select item aggregation select item = ( aggregation select item ) each ; result . add ( aggregation select item ) ; result . add all ( aggregation select item . get derived aggregation select items ( ) ) ; } } return result ; }	Get aggregation select items.
public optional < distinct select item > get distinct select item ( ) { for ( select item each : items ) { if ( each instanceof distinct select item ) { return optional . of ( ( distinct select item ) each ) ; } } return optional . absent ( ) ; }	Get distinct select item optional.
public list < aggregation distinct select item > get aggregation distinct select items ( ) { list < aggregation distinct select item > result = new linked list < > ( ) ; for ( select item each : items ) { if ( each instanceof aggregation distinct select item ) { result . add ( ( aggregation distinct select item ) each ) ; } } return result ; }	Get aggregation distinct select items.
public boolean has unqualified star select item ( ) { for ( select item each : items ) { if ( each instanceof star select item && ! ( ( star select item ) each ) . get owner ( ) . is present ( ) ) { return bool ; } } return bool ; }	Judge has unqualified star select item.
public collection < star select item > get qualified star select items ( ) { collection < star select item > result = new linked list < > ( ) ; for ( select item each : items ) { if ( each instanceof star select item && ( ( star select item ) each ) . get owner ( ) . is present ( ) ) { result . add ( ( star select item ) each ) ; } } return result ; }	Get qualified star select items.
public optional < star select item > find star select item ( final string table name or alias ) { optional < table > table = get tables ( ) . find ( table name or alias ) ; if ( ! table . is present ( ) ) { return optional . absent ( ) ; } for ( select item each : items ) { if ( ! ( each instanceof star select item ) ) { continue ; } star select item star select item = ( star select item ) each ; if ( star select item . get owner ( ) . is present ( ) && get tables ( ) . find ( star select item . get owner ( ) . get ( ) ) . equals ( table ) ) { return optional . of ( star select item ) ; } } return optional . absent ( ) ; }	Find star select item via table name or alias.
public void set index for items ( final map < string , integer > column label index map ) { set index for aggregation item ( column label index map ) ; set index for order item ( column label index map , order by items ) ; set index for order item ( column label index map , group by items ) ; }	Set index for select items.
public select statement merge subquery statement ( ) { select statement result = process limit for subquery ( ) ; process items ( result ) ; process order by items ( result ) ; result . set parameters index ( get parameters index ( ) ) ; return result ; }	Merge subquery statement if contains.
public static void log sql ( final string logic sql , final collection < string > data source names ) { log ( str ) ; log ( str , logic sql , joiner . on ( str ) . join ( data source names ) ) ; }	Print SQL log for master slave rule.
public static void log sql ( final string logic sql , final boolean show simple , final sql sql statement , final collection < route unit > route units ) { log ( str ) ; log ( str , logic sql ) ; log ( str , sql statement ) ; if ( show simple ) { log simple mode ( route units ) ; } else { log normal mode ( route units ) ; } }	Print SQL log for sharding rule.
@ sneaky throws public static data source create data source ( final file yaml file ) { yaml root encrypt rule configuration config = yaml engine . unmarshal ( yaml file , yaml root encrypt rule configuration . class ) ; return encrypt data source factory . create data source ( config . get data source ( ) , new encrypt rule configuration yaml swapper ( ) . swap ( config . get encrypt rule ( ) ) ) ; }	Create encrypt data source.
public token get matched token ( final int token type ) throws recognition exception { token result = parser . get current token ( ) ; boolean is identifier compatible = bool ; if ( identifier token index == token type && identifier token index > result . get type ( ) ) { is identifier compatible = bool ; } if ( result . get type ( ) == token type || is identifier compatible ) { if ( token . eof != token type && is identifier compatible && result instanceof common token ) { ( ( common token ) result ) . set type ( identifier token index ) ; } parser . get error handler ( ) . report match ( parser ) ; parser . consume ( ) ; } else { result = parser . get error handler ( ) . recover inline ( parser ) ; if ( parser . get build parse tree ( ) && - num == result . get token index ( ) ) { parser . get context ( ) . add error node ( parser . create error node ( parser . get context ( ) , result ) ) ; } } return result ; }	Get matched token by token type.
@ suppress warnings ( str ) public static < t > t handle ( final environment environment , final string prefix , final class < t > target class ) { switch ( spring boot version ) { case num : return ( t ) v1 ( environment , prefix ) ; default : return ( t ) v2 ( environment , prefix , target class ) ; } }	Spring Boot 1.x is compatible with Spring Boot 2.x by Using Java Reflect.
public void add unit ( final sql [ ] column values , final object [ ] column parameters ) { if ( type == insert type . values ) { this . units . add ( new column value optimize result ( column names , column values , column parameters ) ) ; } else { this . units . add ( new set assignment optimize result ( column names , column values , column parameters ) ) ; } }	Add insert optimize result uint.
public static merge engine new instance ( final database type database type , final sharding rule sharding rule , final sql route result , final sharding table meta data sharding table meta data , final list < query result > query results ) throws sql { if ( route result . get sql statement ( ) instanceof select statement ) { return new dql ( database type , route result , query results ) ; } if ( route result . get sql statement ( ) instanceof dal ) { return new dal ( sharding rule , query results , ( dal ) route result . get sql statement ( ) , sharding table meta data ) ; } throw new unsupported operation exception ( string . format ( str , route result . get sql statement ( ) . get type ( ) ) ) ; }	Create merge engine instance.
public optional < table rule > find table rule ( final string logic table name ) { for ( table rule each : table rules ) { if ( each . get logic table ( ) . equals ignore case ( logic table name ) ) { return optional . of ( each ) ; } } return optional . absent ( ) ; }	Find table rule.
public optional < table rule > find table rule by actual table ( final string actual table name ) { for ( table rule each : table rules ) { if ( each . is existed ( actual table name ) ) { return optional . of ( each ) ; } } return optional . absent ( ) ; }	Find table rule via actual table name.
public table rule get table rule ( final string logic table name ) { optional < table rule > table rule = find table rule ( logic table name ) ; if ( table rule . is present ( ) ) { return table rule . get ( ) ; } if ( is broadcast table ( logic table name ) ) { return new table rule ( sharding data source names . get data source names ( ) , logic table name ) ; } if ( ! strings . is null or empty ( sharding data source names . get default data source name ( ) ) ) { return new table rule ( sharding data source names . get default data source name ( ) , logic table name ) ; } throw new sharding configuration exception ( str , logic table name ) ; }	Get table rule.
public sharding strategy get database sharding strategy ( final table rule table rule ) { return null == table rule . get database sharding strategy ( ) ? default database sharding strategy : table rule . get database sharding strategy ( ) ; }	Get database sharding strategy.
public sharding strategy get table sharding strategy ( final table rule table rule ) { return null == table rule . get table sharding strategy ( ) ? default table sharding strategy : table rule . get table sharding strategy ( ) ; }	Get table sharding strategy.
public boolean is all binding tables ( final collection < string > logic table names ) { if ( logic table names . is empty ( ) ) { return bool ; } optional < binding table rule > binding table rule = find binding table rule ( logic table names ) ; if ( ! binding table rule . is present ( ) ) { return bool ; } collection < string > result = new tree set < > ( string . case insensitive order ) ; result . add all ( binding table rule . get ( ) . get all logic tables ( ) ) ; return ! result . is empty ( ) && result . contains all ( logic table names ) ; }	Judge logic tables is all belong to binding encryptors.
public optional < binding table rule > find binding table rule ( final string logic table name ) { for ( binding table rule each : binding table rules ) { if ( each . has logic table ( logic table name ) ) { return optional . of ( each ) ; } } return optional . absent ( ) ; }	Find binding table rule via logic table name.
public boolean is all broadcast tables ( final collection < string > logic table names ) { if ( logic table names . is empty ( ) ) { return bool ; } for ( string each : logic table names ) { if ( ! is broadcast table ( each ) ) { return bool ; } } return bool ; }	Judge logic tables is all belong to broadcast encryptors.
public boolean is broadcast table ( final string logic table name ) { for ( string each : broadcast tables ) { if ( each . equals ignore case ( logic table name ) ) { return bool ; } } return bool ; }	Judge logic table is belong to broadcast tables.
public boolean is all in default data source ( final collection < string > logic table names ) { for ( string each : logic table names ) { if ( find table rule ( each ) . is present ( ) || is broadcast table ( each ) ) { return bool ; } } return ! logic table names . is empty ( ) ; }	Judge logic tables is all belong to default data source.
public boolean is sharding column ( final string column name , final string table name ) { for ( table rule each : table rules ) { if ( each . get logic table ( ) . equals ignore case ( table name ) && is sharding column ( each , column name ) ) { return bool ; } } return bool ; }	Judge is sharding column or not.
public optional < string > find generate key column name ( final string logic table name ) { for ( table rule each : table rules ) { if ( each . get logic table ( ) . equals ignore case ( logic table name ) && null != each . get generate key column ( ) ) { return optional . of ( each . get generate key column ( ) ) ; } } return optional . absent ( ) ; }	Find column name of generated key.
public string get logic table name ( final string logic index name ) { for ( table rule each : table rules ) { if ( logic index name . equals ( each . get logic index ( ) ) ) { return each . get logic table ( ) ; } } throw new sharding configuration exception ( str , logic index name ) ; }	Get logic table name base on logic index name.
public collection < string > get logic table names ( final string actual table name ) { collection < string > result = new linked list < > ( ) ; for ( table rule each : table rules ) { if ( each . is existed ( actual table name ) ) { result . add ( each . get logic table ( ) ) ; } } return result ; }	Get logic table names base on actual table name.
public data node get data node ( final string logic table name ) { table rule table rule = get table rule ( logic table name ) ; return table rule . get actual data nodes ( ) . get ( num ) ; }	Find data node by logic table name.
public data node get data node ( final string data source name , final string logic table name ) { table rule table rule = get table rule ( logic table name ) ; for ( data node each : table rule . get actual data nodes ( ) ) { if ( sharding data source names . get data source names ( ) . contains ( each . get data source name ( ) ) && each . get data source name ( ) . equals ( data source name ) ) { return each ; } } throw new sharding configuration exception ( str , data source name , logic table name ) ; }	Find data node by data source and logic table.
public optional < string > find actual default data source name ( ) { string default data source name = sharding data source names . get default data source name ( ) ; if ( strings . is null or empty ( default data source name ) ) { return optional . absent ( ) ; } optional < string > master default data source name = find master data source name ( default data source name ) ; return master default data source name . is present ( ) ? master default data source name : optional . of ( default data source name ) ; }	Find actual default data source name.
public optional < master slave rule > find master slave rule ( final string data source name ) { for ( master slave rule each : master slave rules ) { if ( each . contain data source name ( data source name ) ) { return optional . of ( each ) ; } } return optional . absent ( ) ; }	Find master slave rule.
public string get actual data source name ( final string actual table name ) { optional < table rule > table rule = find table rule by actual table ( actual table name ) ; if ( table rule . is present ( ) ) { return table rule . get ( ) . get actual datasource names ( ) . iterator ( ) . next ( ) ; } if ( ! strings . is null or empty ( sharding data source names . get default data source name ( ) ) ) { return sharding data source names . get default data source name ( ) ; } throw new sharding exception ( str , actual table name ) ; }	Get actual data source name.
public boolean contains ( final string logic table name ) { return find table rule ( logic table name ) . is present ( ) || find binding table rule ( logic table name ) . is present ( ) || is broadcast table ( logic table name ) ; }	Judge contains table in sharding rule.
public collection < string > get sharding logic table names ( final collection < string > logic table names ) { collection < string > result = new linked list < > ( ) ; for ( string each : logic table names ) { optional < table rule > table rule = find table rule ( each ) ; if ( table rule . is present ( ) ) { result . add ( each ) ; } } return result ; }	Get sharding logic table names.
void do await until ( ) throws interrupted exception { lock . lock ( ) ; try { condition . await ( default timeout milliseconds , time unit . milliseconds ) ; } finally { lock . unlock ( ) ; } }	Do await until default timeout milliseconds.
public static routing engine new instance ( final sharding rule sharding rule , final sharding data source meta data sharding data source meta data , final sql sql statement , final optimize result optimize result ) { collection < string > table names = sql statement . get tables ( ) . get table names ( ) ; if ( sql . tcl == sql statement . get type ( ) ) { return new database broadcast routing engine ( sharding rule ) ; } if ( sql . ddl == sql statement . get type ( ) ) { return new table broadcast routing engine ( sharding rule , sql statement ) ; } if ( sql . dal == sql statement . get type ( ) ) { return get dal ( sharding rule , sql statement , table names ) ; } if ( sql . dcl == sql statement . get type ( ) ) { return get dcl ( sharding rule , sql statement , sharding data source meta data ) ; } if ( sharding rule . is all in default data source ( table names ) ) { return new default database routing engine ( sharding rule , table names ) ; } if ( sharding rule . is all broadcast tables ( table names ) ) { return sql . dql == sql statement . get type ( ) ? new unicast routing engine ( sharding rule , table names ) : new database broadcast routing engine ( sharding rule ) ; } if ( optimize result . get sharding conditions ( ) . is always false ( ) || table names . is empty ( ) ) { return new unicast routing engine ( sharding rule , table names ) ; } collection < string > sharding table names = sharding rule . get sharding logic table names ( table names ) ; if ( num == sharding table names . size ( ) || sharding rule . is all binding tables ( sharding table names ) ) { return new standard routing engine ( sql statement , sharding rule , sharding table names . iterator ( ) . next ( ) , optimize result ) ; }	Create new instance of routing engine.
public database access configuration swap ( final data source data source ) { data source property provider provider = data source property provider loader . get provider ( data source ) ; try { string url = ( string ) find getter method ( data source , provider . get url ( ) ) . invoke ( data source ) ; string username = ( string ) find getter method ( data source , provider . get username property name ( ) ) . invoke ( data source ) ; string password = ( string ) find getter method ( data source , provider . get password property name ( ) ) . invoke ( data source ) ; return new database access configuration ( url , username , password ) ; } catch ( final reflective operation exception ex ) { throw new sharding exception ( str , data source . get class ( ) . get name ( ) , data source property provider . class . get name ( ) ) ; } }	Swap data source to database access configuration.
public static abstract describe parser new instance ( final database type db type , final sharding rule sharding rule , final lexer engine lexer engine ) { switch ( db type ) { case h2 : case sql : return new sql ( sharding rule , lexer engine ) ; default : throw new unsupported operation exception ( string . format ( str , db type ) ) ; } }	Create describe parser instance.
public string get raw master data source name ( final string data source name ) { for ( master slave rule configuration each : sharding rule config . get master slave rule configs ( ) ) { if ( each . get name ( ) . equals ( data source name ) ) { return each . get master data source name ( ) ; } } return data source name ; }	Get raw master data source name.
public string get random data source name ( final collection < string > data source names ) { random random = new random ( ) ; int index = random . next int ( data source names . size ( ) ) ; iterator < string > iterator = data source names . iterator ( ) ; for ( int i = num ; i < index ; i ++ ) { iterator . next ( ) ; } return iterator . next ( ) ; }	Get random data source name.
public sql route ( final sql sql route result ) { for ( master slave rule each : master slave rules ) { route ( each , sql route result ) ; } return sql route result ; }	Route Master slave after sharding.
public void persist configuration ( final string sharding schema name , final map < string , data source configuration > data source configs , final rule configuration rule config , final authentication authentication , final properties props , final boolean is overwrite ) { persist data source configuration ( sharding schema name , data source configs , is overwrite ) ; persist rule configuration ( sharding schema name , rule config , is overwrite ) ; persist authentication ( authentication , is overwrite ) ; persist properties ( props , is overwrite ) ; }	Persist rule configuration.
public boolean has data source configuration ( final string sharding schema name ) { return ! strings . is null or empty ( reg center . get ( config node . get data source path ( sharding schema name ) ) ) ; }	Judge whether schema has data source configuration.
public boolean has rule configuration ( final string sharding schema name ) { return ! strings . is null or empty ( reg center . get ( config node . get rule path ( sharding schema name ) ) ) ; }	Judge whether schema has rule configuration.
@ suppress warnings ( str ) public map < string , data source configuration > load data source configurations ( final string sharding schema name ) { map < string , yaml data source configuration > result = ( map ) yaml engine . unmarshal ( reg center . get directly ( config node . get data source path ( sharding schema name ) ) ) ; preconditions . check state ( null != result && ! result . is empty ( ) , str ) ; return maps . transform values ( result , new function < yaml data source configuration , data source configuration > ( ) { @ override public data source configuration apply ( final yaml data source configuration input ) { return new data source configuration yaml swapper ( ) . swap ( input ) ; } } ) ; }	Load data source configurations.
public sharding rule configuration load sharding rule configuration ( final string sharding schema name ) { return new sharding rule configuration yaml swapper ( ) . swap ( yaml engine . unmarshal ( reg center . get directly ( config node . get rule path ( sharding schema name ) ) , yaml sharding rule configuration . class ) ) ; }	Load sharding rule configuration.
public master slave rule configuration load master slave rule configuration ( final string sharding schema name ) { return new master slave rule configuration yaml swapper ( ) . swap ( yaml engine . unmarshal ( reg center . get directly ( config node . get rule path ( sharding schema name ) ) , yaml master slave rule configuration . class ) ) ; }	Load master-slave rule configuration.
public void set database sharding value ( final comparable < ? > value ) { database sharding values . clear ( ) ; database sharding values . put ( str , value ) ; database sharding only = bool ; }	Set sharding value for database sharding only. The sharding operator is {.
public void add database sharding value ( final string logic table , final comparable < ? > value ) { database sharding values . put ( logic table , value ) ; database sharding only = bool ; }	Add sharding value for database. The sharding operator is {.
public void add table sharding value ( final string logic table , final comparable < ? > value ) { table sharding values . put ( logic table , value ) ; database sharding only = bool ; }	Add sharding value for table. The sharding operator is {.
public static collection < comparable < ? > > get database sharding values ( final string logic table ) { return null == hint manager holder . get ( ) ? collections . < comparable < ? > > empty list ( ) : hint manager holder . get ( ) . database sharding values . get ( logic table ) ; }	Get database sharding values.
public static collection < comparable < ? > > get table sharding values ( final string logic table ) { return null == hint manager holder . get ( ) ? collections . < comparable < ? > > empty list ( ) : hint manager holder . get ( ) . table sharding values . get ( logic table ) ; }	Get table sharding values.
public void parse ( final dml update statement ) { lexer engine . accept ( default keyword . set ) ; do { parse set item ( update statement ) ; } while ( lexer engine . skip if equal ( symbol . comma ) ) ; }	Parse set items.
public static object get value by column type ( final result set result set , final int column index ) throws sql { result set meta data meta data = result set . get meta data ( ) ; switch ( meta data . get column type ( column index ) ) { case types . bit : case types . boolean : return result set . get boolean ( column index ) ; case types . tinyint : return result set . get byte ( column index ) ; case types . smallint : return result set . get short ( column index ) ; case types . integer : return result set . get int ( column index ) ; case types . bigint : return result set . get long ( column index ) ; case types . numeric : case types . decimal : return result set . get big decimal ( column index ) ; case types . float : case types . double : return result set . get double ( column index ) ; case types . char : case types . varchar : case types . longvarchar : return result set . get string ( column index ) ; case types . binary : case types . varbinary : case types . longvarbinary : return result set . get bytes ( column index ) ; case types . date : return result set . get date ( column index ) ; case types . time : return result set . get time ( column index ) ; case types . timestamp : return result set . get timestamp ( column index ) ; case types . clob : return result set . get clob ( column index ) ; case types . blob : return result set . get blob ( column index ) ; default : return result set . get object ( column index ) ; } }	Get value by column type.
@ subscribe public final synchronized void renew ( final data source changed event data source changed event ) throws exception { if ( ! name . equals ( data source changed event . get sharding schema name ( ) ) ) { return ; } backend data source . close ( ) ; data sources . clear ( ) ; data sources . put all ( data source converter . get data source parameter map ( data source changed event . get data source configurations ( ) ) ) ; backend data source = new jdbc ( data sources ) ; }	Renew data source configuration.
public int get length ( ) { return owner length + table name . length ( ) + quote character . get start delimiter ( ) . length ( ) + quote character . get end delimiter ( ) . length ( ) ; }	Get table token length.
public void parse ( final select statement select statement , final list < select item > items ) { do { select statement . get items ( ) . add all ( parse select items ( select statement ) ) ; } while ( lexer engine . skip if equal ( symbol . comma ) ) ; select statement . set select list stop index ( lexer engine . get current token ( ) . get end position ( ) - lexer engine . get current token ( ) . get literals ( ) . length ( ) ) ; items . add all ( select statement . get items ( ) ) ; }	Parse select list.
public string get schema name ( final string configuration node full path ) { string result = str ; pattern pattern = pattern . compile ( get schema path ( ) + str + str , pattern . case insensitive ) ; matcher matcher = pattern . matcher ( configuration node full path ) ; if ( matcher . find ( ) ) { result = matcher . group ( num ) ; } return result ; }	Get schema name.
public static sql get command packet type ( final sql payload ) { preconditions . check argument ( num == payload . read int1 ( ) , str ) ; return sql . value of ( payload . read int1 ( ) ) ; }	Get command packet type.
public static sql new instance ( final database type database type , final string sql ) { for ( sharding parse engine each : new instance service loader . new service instances ( sharding parse engine . class ) ) { if ( database type . value of ( each . get database type ( ) ) == database type ) { return each . create sql ( sql ) ; } } throw new unsupported operation exception ( string . format ( str , database type ) ) ; }	New instance of SQL parser.
public collection < string > get data source names ( ) { collection < string > result = new hash set < > ( table units . size ( ) , num ) ; for ( table unit each : table units ) { result . add ( each . get data source name ( ) ) ; } return result ; }	Get all data source names.
public map < string , set < string > > get data source logic tables map ( final collection < string > data source names ) { map < string , set < string > > result = new hash map < > ( ) ; for ( string each : data source names ) { set < string > logic table names = get logic table names ( each ) ; if ( ! logic table names . is empty ( ) ) { result . put ( each , logic table names ) ; } } return result ; }	Get map relationship between data source and logic tables via data sources' names.
public final void set column value ( final string column name , final object column value ) { sql sql expression = values [ get column index ( column name ) ] ; if ( sql expression instanceof sql ) { parameters [ get parameter index ( sql expression ) ] = column value ; } else { sql column expression = string . class == column value . get class ( ) ? new sql ( string . value of ( column value ) ) : new sql ( ( number ) column value ) ; values [ get column index ( column name ) ] = column expression ; } }	Set column value.
public final object get column value ( final string column name ) { sql sql expression = values [ get column index ( column name ) ] ; if ( sql expression instanceof sql ) { return parameters [ get parameter index ( sql expression ) ] ; } else if ( sql expression instanceof sql ) { return ( ( sql ) sql expression ) . get text ( ) ; } else { return ( ( sql ) sql expression ) . get number ( ) ; } }	Get column value.
public static command executor new instance ( final sql command packet type , final command packet command packet , final backend connection backend connection ) { log . debug ( str , command packet type , command packet ) ; switch ( command packet type ) { case com quit : return new sql ( ) ; case com init db : return new sql ( ( sql ) command packet , backend connection ) ; case com field list : return new sql ( ( sql ) command packet , backend connection ) ; case com query : return new sql ( ( sql ) command packet , backend connection ) ; case com stmt prepare : return new sql ( ( sql ) command packet , backend connection ) ; case com stmt execute : return new sql ( ( sql ) command packet , backend connection ) ; case com stmt close : return new sql ( ( sql ) command packet ) ; case com ping : return new sql ( ) ; default : return new sql ( command packet type ) ; } }	Create new instance of packet executor.
public set < string > get actual table names ( final string data source name , final string logic table name ) { set < string > result = new hash set < > ( routing tables . size ( ) , num ) ; for ( routing table each : routing tables ) { if ( data source name . equals ignore case ( this . data source name ) && each . get logic table name ( ) . equals ignore case ( logic table name ) ) { result . add ( each . get actual table name ( ) ) ; } } return result ; }	Get actual tables' names via data source name.
public set < string > get logic table names ( final string data source name ) { set < string > result = new hash set < > ( routing tables . size ( ) , num ) ; for ( routing table each : routing tables ) { if ( data source name . equals ignore case ( this . data source name ) ) { result . add ( each . get logic table name ( ) ) ; } } return result ; }	Get logic tables' names via data source name.
@ sneaky throws public final void record method invocation ( final class < ? > target class , final string method name , final class < ? > [ ] argument types , final object [ ] arguments ) { jdbc method invocations . add ( new jdbc method invocation ( target class . get method ( method name , argument types ) , arguments ) ) ; }	record method invocation.
public boolean next ( ) throws sql { boolean result = query result . next ( ) ; order values = result ? get order values ( ) : collections . < comparable < ? > > empty list ( ) ; return result ; }	iterate next data.
public byte [ ] generate random bytes ( final int length ) { byte [ ] result = new byte [ length ] ; for ( int i = num ; i < length ; i ++ ) { result [ i ] = seed [ random . next int ( seed . length ) ] ; } return result ; }	Generate random bytes.
@ sneaky throws public map < string , table meta data > load ( final sharding rule sharding rule ) { map < string , table meta data > result = new hash map < > ( ) ; result . put all ( load sharding tables ( sharding rule ) ) ; result . put all ( load default tables ( sharding rule ) ) ; return result ; }	Load all table meta data.
public static optimize engine new instance ( final sharding rule sharding rule , final sql sql statement , final list < object > parameters , final generated key generated key ) { if ( sql statement instanceof insert statement ) { return new insert optimize engine ( sharding rule , ( insert statement ) sql statement , parameters , generated key ) ; } if ( sql statement instanceof select statement || sql statement instanceof dml ) { return new query optimize engine ( sql statement . get route conditions ( ) . get or condition ( ) , parameters ) ; }	Create optimize engine instance.
public static optimize engine new instance ( final encrypt rule encrypt rule , final sql sql statement , final list < object > parameters ) { if ( sql statement instanceof insert statement ) { return new encrypt insert optimize engine ( encrypt rule , ( insert statement ) sql statement , parameters ) ; } return new encrypt default optimize engine ( ) ; }	Create encrypt optimize engine instance.
public static database protocol frontend engine new instance ( final database type database type ) { for ( database protocol frontend engine each : new instance service loader . new service instances ( database protocol frontend engine . class ) ) { if ( database type . value from ( each . get database type ( ) ) == database type ) { return each ; } } throw new unsupported operation exception ( string . format ( str , database type ) ) ; }	Create new instance of database protocol frontend engine.
public sharding configuration load ( ) throws io { collection < string > schema names = new hash set < > ( ) ; yaml proxy server configuration server config = load server configuration ( new file ( sharding configuration loader . class . get resource ( config path + server config file ) . get file ( ) ) ) ; file config path = new file ( sharding configuration loader . class . get resource ( config path ) . get file ( ) ) ; collection < yaml proxy rule configuration > rule configurations = new linked list < > ( ) ; for ( file each : find rule configuration files ( config path ) ) { optional < yaml proxy rule configuration > rule config = load rule configuration ( each , server config ) ; if ( rule config . is present ( ) ) { preconditions . check state ( schema names . add ( rule config . get ( ) . get schema name ( ) ) , str , rule config . get ( ) . get schema name ( ) ) ; rule configurations . add ( rule config . get ( ) ) ; } } preconditions . check state ( ! rule configurations . is empty ( ) || null != server config . get orchestration ( ) , str , config path . get path ( ) ) ; map < string , yaml proxy rule configuration > rule configuration map = new hash map < > ( rule configurations . size ( ) , num ) ; for ( yaml proxy rule configuration each : rule configurations ) { rule configuration map . put ( each . get schema name ( ) , each ) ; } return new sharding configuration ( server config , rule configuration map ) ; }	Load configuration of Sharding-Proxy.
@ suppress warnings ( str ) @ sneaky throws public void init ( final filler rule definition entity filler rule definition entity ) { for ( filler rule entity each : filler rule definition entity . get rules ( ) ) { rules . put ( ( class < ? extends sql > ) class . for name ( each . get sql segment class ( ) ) , ( sql ) class . for name ( each . get filler class ( ) ) . new instance ( ) ) ; } }	Initialize filler rule definition.
public static alias expression parser create alias expression parser ( final lexer engine lexer engine ) { switch ( lexer engine . get database type ( ) ) { case h2 : return new sql ( lexer engine ) ; case sql : return new sql ( lexer engine ) ; case oracle : return new oracle alias expression parser ( lexer engine ) ; case sql : return new sql ( lexer engine ) ; case sql : return new sql ( lexer engine ) ; default : throw new unsupported operation exception ( string . format ( str , lexer engine . get database type ( ) ) ) ; } }	Create alias parser instance.
private input stream trusted certificates input stream ( ) {	Returns an input stream containing one or more certificate PEM files.
private x509 trust manager default trust manager ( ) throws general security exception { trust manager factory trust manager factory = trust manager factory . get instance ( trust manager factory . get default algorithm ( ) ) ; trust manager factory . init ( ( key store ) null ) ; trust manager [ ] trust managers = trust manager factory . get trust managers ( ) ; if ( trust managers . length != num || ! ( trust managers [ num ] instanceof x509 trust manager ) ) { throw new illegal state exception ( str + arrays . to string ( trust managers ) ) ; } return ( x509 trust manager ) trust managers [ num ] ; }	Returns a trust manager that trusts the VM's default certificate authorities.
private static string inet6 address to ascii ( byte [ ] address ) {	Encodes an IPv6 address in canonical form according to RFC 5952.
@ override public synchronized void on open ( web socket web socket , response response ) { system . out . println ( str + response ) ; }	the body from slack is a 0-byte-buffer.
private void reset next proxy ( http url url , proxy proxy ) { if ( proxy != null ) {	Prepares the proxy servers to try.
private proxy next proxy ( ) throws io { if ( ! has next proxy ( ) ) { throw new socket exception ( str + address . url ( ) . host ( ) + str + proxies ) ; } proxy result = proxies . get ( next proxy index ++ ) ; reset next inet socket address ( result ) ; return result ; }	Returns the next proxy to try.
private void reset next inet socket address ( proxy proxy ) throws io {	Prepares the socket addresses to attempt for the current proxy or host.
public void await success ( ) throws exception { future task < void > future task = results . poll ( num , time unit . seconds ) ; if ( future task == null ) throw new assertion error ( str ) ; future task . get ( num , time unit . seconds ) ; }	Returns once the duplex conversation completes successfully.
private void read the list uninterruptibly ( ) { boolean interrupted = bool ; try { while ( bool ) { try { read the list ( ) ; return ; } catch ( io e ) { thread . interrupted ( ) ;	Reads the public suffix list treating the operation as uninterruptible.
void write close ( int code , byte string reason ) throws io { byte string payload = byte string . empty ; if ( code != num || reason != null ) { if ( code != num ) { validate close code ( code ) ; } buffer buffer = new buffer ( ) ; buffer . write short ( code ) ; if ( reason != null ) { buffer . write ( reason ) ; } payload = buffer . read byte string ( ) ; } try { write control frame ( opcode control close , payload ) ; } finally { writer closed = bool ; } }	Send a close frame with optional code and reason.
sink new message sink ( int format opcode , long content length ) { if ( active writer ) { throw new illegal state exception ( str ) ; } active writer = bool ;	Stream a message payload as a series of frames.
private synchronized void start ( inet socket address inet socket address ) throws io { if ( started ) throw new illegal state exception ( str ) ; started = bool ; executor = executors . new cached thread pool ( util . thread factory ( str , bool ) ) ; this . inet socket address = inet socket address ; if ( server socket factory == null ) { server socket factory = server socket factory . get default ( ) ; } server socket = server socket factory . create server socket ( ) ;	Starts the server and binds to the given socket address.
private void read message ( ) throws io { while ( bool ) { if ( closed ) throw new io ( str ) ; if ( frame length > num ) { source . read fully ( message frame buffer , frame length ) ; if ( ! is client ) { message frame buffer . read and write unsafe ( mask cursor ) ; mask cursor . seek ( message frame buffer . size ( ) - frame length ) ; toggle mask ( mask cursor , mask key ) ; mask cursor . close ( ) ; } } if ( is final frame ) break ;	Reads a message body into across one or more frames.
public synchronized headers take headers ( ) throws io { read timeout . enter ( ) ; try { while ( headers queue . is empty ( ) && error code == null ) { wait for io ( ) ; } } finally { read timeout . exit and throw if timed out ( ) ; } if ( ! headers queue . is empty ( ) ) { return headers queue . remove first ( ) ; } throw error exception != null ? error exception : new stream reset exception ( error code ) ; }	Removes and returns the stream's received response headers, blocking if necessary until headershave been received.
public synchronized headers trailers ( ) throws io { if ( error code != null ) { throw error exception != null ? error exception : new stream reset exception ( error code ) ; } if ( ! source . finished || ! source . receive buffer . exhausted ( ) || ! source . read buffer . exhausted ( ) ) { throw new illegal state exception ( str ) ; } return source . trailers != null ? source . trailers : util . empty headers ; }	Returns the trailers. It is only safe to call this once the source stream has been completelyexhausted.
public void write headers ( list < header > response headers , boolean out finished , boolean flush headers ) throws io { assert ( ! thread . holds lock ( http2 stream . this ) ) ; if ( response headers == null ) { throw new null pointer exception ( str ) ; } synchronized ( this ) { this . has response headers = bool ; if ( out finished ) { this . sink . finished = bool ; } }	Sends a reply to an incoming stream.
private real connection find healthy connection ( int connect timeout , int read timeout , int write timeout , int ping interval millis , boolean connection retry enabled , boolean do extensive health checks ) throws io { while ( bool ) { real connection candidate = find connection ( connect timeout , read timeout , write timeout , ping interval millis , connection retry enabled ) ;	Finds a connection and returns it if it is healthy.
private real connection find connection ( int connect timeout , int read timeout , int write timeout , int ping interval millis , boolean connection retry enabled ) throws io { boolean found pooled connection = bool ; real connection result = null ; route selected route = null ; real connection released connection ; socket to close ; synchronized ( connection pool ) { if ( transmitter . is canceled ( ) ) throw new io ( str ) ; has stream failure = bool ;	Returns a connection to host a new stream.
private boolean retry current route ( ) { return transmitter . connection != null && transmitter . connection . route failure count == num && util . same connection ( transmitter . connection . route ( ) . address ( ) . url ( ) , address . url ( ) ) ; }	Return true if the route used for the current connection should be retried, even if theconnection itself is unhealthy.
@ override public boolean send ( string text ) { if ( text == null ) throw new null pointer exception ( str ) ; return send ( byte string . encode utf8 ( text ) , opcode text ) ; }	Writer methods to enqueue frames.
private static headers combine ( headers cached headers , headers network headers ) { headers . builder result = new headers . builder ( ) ; for ( int i = num , size = cached headers . size ( ) ; i < size ; i ++ ) { string field name = cached headers . name ( i ) ; string value = cached headers . value ( i ) ; if ( str . equals ignore case ( field name ) && value . starts with ( str ) ) { continue ;	Combines cached headers with a network headers as defined by RFC 7234, 4.3.4.
@ override public mock response dispatch ( recorded request request ) { http url request url = mock web server . url ( request . get path ( ) ) ; string code = request url . query parameter ( str ) ; string state string = request url . query parameter ( str ) ; byte string state = state string != null ? byte string . decode base64 ( state string ) : null ; listener listener ; synchronized ( this ) { listener = listeners . get ( state ) ; } if ( code == null || listener == null ) { return new mock response ( ) . set response code ( num ) . set body ( str ) ; } try { o session = slack api . exchange code ( code , redirect url ( ) ) ; listener . session granted ( session ) ; } catch ( io e ) { return new mock response ( ) . set response code ( num ) . set body ( str + e . get message ( ) ) ; } synchronized ( this ) { listeners . remove ( state ) ; }	When the browser hits the redirect URL, use the provided code to ask Slack for a session.
public mock response with web socket upgrade ( web socket listener listener ) { set status ( str ) ; set header ( str , str ) ; set header ( str , str ) ; body = null ; web socket listener = listener ; return this ; }	Attempts to perform a web socket upgrade on the connection.
public static set < string > vary fields ( headers response headers ) { set < string > result = collections . empty set ( ) ; for ( int i = num , size = response headers . size ( ) ; i < size ; i ++ ) { if ( ! str . equals ignore case ( response headers . name ( i ) ) ) continue ; string value = response headers . value ( i ) ; if ( result . is empty ( ) ) { result = new tree set < > ( string . case insensitive order ) ; } for ( string vary field : value . split ( str ) ) { result . add ( vary field . trim ( ) ) ; } } return result ; }	Returns the names of the request headers that need to be checked for equality when caching.
public static list < challenge > parse challenges ( headers response headers , string header name ) { list < challenge > result = new array list < > ( ) ; for ( int h = num ; h < response headers . size ( ) ; h ++ ) { if ( header name . equals ignore case ( response headers . name ( h ) ) ) { buffer header = new buffer ( ) . write utf8 ( response headers . value ( h ) ) ; try { parse challenge header ( result , header ) ; } catch ( eof e ) { platform . get ( ) . log ( platform . warn , str , e ) ; } } } return result ; }	Parse RFC 7235 challenges.
private static boolean skip whitespace and commas ( buffer buffer ) throws eof { boolean comma found = bool ; while ( ! buffer . exhausted ( ) ) { byte b = buffer . get byte ( num ) ; if ( b == str ) {	Returns true if any commas were skipped.
public void request oauth session ( string scopes , string team ) throws exception { if ( session factory == null ) { session factory = new o ( slack api ) ; session factory . start ( ) ; } http url authorize url = session factory . new authorize url ( scopes , team , session -> { init oauth session ( session ) ; system . out . printf ( str , session ) ; } ) ; system . out . printf ( str , authorize url ) ; }	Shows a browser URL to authorize this app to act as this user.
public void start rtm ( ) throws io { string access token ; synchronized ( this ) { access token = session . access token ; } rtm session rtm session = new rtm session ( slack api ) ; rtm session . open ( access token ) ; }	Starts a real time messaging session.
private void connect tunnel ( int connect timeout , int read timeout , int write timeout , call call , event listener event listener ) throws io { request tunnel request = create tunnel request ( ) ; http url url = tunnel request . url ( ) ; for ( int i = num ; i < max tunnel attempts ; i ++ ) { connect socket ( connect timeout , read timeout , call , event listener ) ; tunnel request = create tunnel ( read timeout , write timeout , tunnel request , url ) ; if ( tunnel request == null ) break ;	Does all the work to build an HTTPS connection over a proxy tunnel.
private void connect socket ( int connect timeout , int read timeout , call call , event listener event listener ) throws io { proxy proxy = route . proxy ( ) ; address address = route . address ( ) ; raw socket = proxy . type ( ) == proxy . type . direct || proxy . type ( ) == proxy . type . http ? address . socket factory ( ) . create socket ( ) : new socket ( proxy ) ; event listener . connect start ( call , route . socket address ( ) , proxy ) ; raw socket . set so timeout ( read timeout ) ; try { platform . get ( ) . connect socket ( raw socket , route . socket address ( ) , connect timeout ) ; } catch ( connect exception e ) { connect exception ce = new connect exception ( str + route . socket address ( ) ) ; ce . init cause ( e ) ; throw ce ; }	Does all the work necessary to build a full HTTP or HTTPS connection on a raw socket.
private request create tunnel ( int read timeout , int write timeout , request tunnel request , http url url ) throws io {	To make an HTTPS connection over an HTTP proxy, send an unencrypted CONNECT request to createthe proxy connection.
private request create tunnel request ( ) throws io { request proxy connect request = new request . builder ( ) . url ( route . address ( ) . url ( ) ) . method ( str , null ) . header ( str , util . host header ( route . address ( ) . url ( ) , bool ) ) . header ( str , str )	Returns a request that creates a TLS tunnel via an HTTP proxy.
public boolean is healthy ( boolean do extensive checks ) { if ( socket . is closed ( ) || socket . is input shutdown ( ) || socket . is output shutdown ( ) ) { return bool ; } if ( http2 connection != null ) { return ! http2 connection . is shutdown ( ) ; } if ( do extensive checks ) { try { int read timeout = socket . get so timeout ( ) ; try { socket . set so timeout ( num ) ; if ( source . exhausted ( ) ) { return bool ;	Returns true if this connection is ready to host new streams.
@ override public void on stream ( http2 stream stream ) throws io { stream . close ( error code . refused stream , null ) ; }	Refuse incoming streams.
exchange new exchange ( interceptor . chain chain , boolean do extensive health checks ) { synchronized ( connection pool ) { if ( no more exchanges ) { throw new illegal state exception ( str ) ; } if ( exchange != null ) { throw new illegal state exception ( str + str ) ; } } exchange codec codec = exchange finder . find ( client , chain , do extensive health checks ) ; exchange result = new exchange ( this , call , event listener , exchange finder , codec ) ; synchronized ( connection pool ) { this . exchange = result ; this . exchange request done = bool ; this . exchange response done = bool ; return result ; } }	Returns a new exchange to carry a new request and response.
public void cancel ( ) { exchange exchange to cancel ; real connection connection to cancel ; synchronized ( connection pool ) { canceled = bool ; exchange to cancel = exchange ; connection to cancel = exchange finder != null && exchange finder . connecting connection ( ) != null ? exchange finder . connecting connection ( ) : connection ; } if ( exchange to cancel != null ) { exchange to cancel . cancel ( ) ; } else if ( connection to cancel != null ) { connection to cancel . cancel ( ) ; } }	Immediately closes the socket connection if it's currently held.
public http2 stream push stream ( int associated stream id , list < header > request headers , boolean out ) throws io { if ( client ) throw new illegal state exception ( str ) ; return new stream ( associated stream id , request headers , out ) ; }	Returns a new server-initiated stream.
public http2 stream new stream ( list < header > request headers , boolean out ) throws io { return new stream ( num , request headers , out ) ; }	Returns a new locally-initiated stream.
public void shutdown ( error code status code ) throws io { synchronized ( writer ) { int last good stream id ; synchronized ( this ) { if ( shutdown ) { return ; } shutdown = bool ; last good stream id = this . last good stream id ; }	Degrades this connection such that new streams can neither be created locally, nor acceptedfrom the remote peer.
@ override public void emit response ( t response ) { if ( ! is terminated ( ) ) { subject . on next ( response ) ; value set . set ( bool ) ; } else { throw new illegal state exception ( str + response ) ; } }	Emit a response that should be OnNexted to an Observer.
public exception set exception if response not received ( exception e , string exception message ) { exception exception = e ; if ( ! value set . get ( ) && ! is terminated ( ) ) { if ( e == null ) { exception = new illegal state exception ( exception message ) ; } set exception if response not received ( exception ) ; }	Set an ISE if a response is not yet received otherwise skip it.
private byte [ ] wrap class ( string class name , boolean wrap constructors , string ... method names ) throws not found exception , io , cannot compile exception { class pool cp = class pool . get default ( ) ; ct class ct clazz = cp . get ( class name ) ;	Wrap all signatures of a given method name.
protected tryable semaphore get fallback semaphore ( ) { if ( fallback semaphore override == null ) { tryable semaphore s = fallback semaphore per circuit . get ( command key . name ( ) ) ; if ( s == null ) {	Get the TryableSemaphore this HystrixCommand should use if a fallback occurs.
protected tryable semaphore get execution semaphore ( ) { if ( properties . execution isolation strategy ( ) . get ( ) == execution isolation strategy . semaphore ) { if ( execution semaphore override == null ) { tryable semaphore s = execution semaphore per circuit . get ( command key . name ( ) ) ; if ( s == null ) {	Get the TryableSemaphore this HystrixCommand should use for execution if not running in a separate thread.
private void validate completable return type ( method command method , class < ? > callback return type ) { if ( void . type == callback return type ) { throw new fallback definition exception ( create error msg ( command method , method , str + completable . class . get simple name ( ) ) ) ; } }	everything can be wrapped into completable except 'void'.
public fallback method get fallback method ( class < ? > enclosing type , method command method , boolean extended ) { if ( command method . is annotation present ( hystrix command . class ) ) { return fallback method finder . find ( enclosing type , command method , extended ) ; } return fallback method . absent ; }	Gets fallback method for command method.
public static optional < method > get method ( class < ? > type , string name , class < ? > ... parameter types ) { method [ ] methods = type . get declared methods ( ) ; for ( method method : methods ) { if ( method . get name ( ) . equals ( name ) && arrays . equals ( method . get parameter types ( ) , parameter types ) ) { return optional . of ( method ) ; } } class < ? > super class = type . get superclass ( ) ; if ( super class != null && ! super class . equals ( object . class ) ) { return get method ( super class , name , parameter types ) ; } else { return optional . absent ( ) ; } }	Gets method by name and parameters types using reflection,if the given type doesn't contain required method then continue applying this method for all super classes up to Object class.
public method unbride ( final method bridge method , class < ? > a class ) throws io , no such method exception , class not found exception { if ( bridge method . is bridge ( ) && bridge method . is synthetic ( ) ) { if ( cache . contains key ( bridge method ) ) { return cache . get ( bridge method ) ; } class reader class reader = new class reader ( a class . get name ( ) ) ; final method signature method signature = new method signature ( ) ; class reader . accept ( new class visitor ( as ) { @ override public method visitor visit method ( int access , string name , string desc , string signature , string [ ] exceptions ) { boolean bridge = ( access & acc bridge ) != num && ( access & acc synthetic ) != num ; if ( bridge && bridge method . get name ( ) . equals ( name ) && get parameter count ( desc ) == bridge method . get parameter types ( ) . length ) { return new method finder ( method signature ) ; } return super . visit method ( access , name , desc , signature , exceptions ) ; } } , num ) ; method method = a class . get declared method ( method signature . name , method signature . get parameter types ( ) ) ; cache . put ( bridge method , method ) ; return method ; } else { return bridge method ; } }	Finds generic method for the given bridge method.
public r execute ( ) { try { return queue ( ) . get ( ) ; } catch ( exception e ) { throw exceptions . sneaky throw ( decompose exception ( e ) ) ; } }	Used for synchronous execution of command.
public static class [ ] get parameter types ( join point join point ) { method signature signature = ( method signature ) join point . get signature ( ) ; method method = signature . get method ( ) ; return method . get parameter types ( ) ; }	Gets parameter types of the join point.
public static method get declared method ( class < ? > type , string method name , class < ? > ... parameter types ) { method method = null ; try { method = type . get declared method ( method name , parameter types ) ; if ( method . is bridge ( ) ) { method = method provider . get instance ( ) . unbride ( method , type ) ; } } catch ( no such method exception e ) { class < ? > superclass = type . get superclass ( ) ; if ( superclass != null ) { method = get declared method ( superclass , method name , parameter types ) ; } } catch ( class not found exception e ) { throwables . propagate ( e ) ; } catch ( io e ) { throwables . propagate ( e ) ; } return method ; }	Gets declared method from specified type by mame and parameters types.
@ override protected hystrix command < list < object > > create command ( collection < collapsed request < object , object > > collapsed requests ) { return new batch hystrix command ( hystrix command builder factory . get instance ( ) . create ( meta holder , collapsed requests ) ) ; }	Creates batch command.
public observable < response type > submit request ( final request argument type arg ) { if ( ! timer listener registered . get ( ) && timer listener registered . compare and set ( bool , bool ) ) { timer listener reference . set ( timer . add listener ( new collapsed task ( ) ) ) ; }	Submit a request to a batch.
public execution result add event ( hystrix event type event type ) { return new execution result ( event counts . plus ( event type ) , start timestamp , execution latency , user thread latency , failed execution exception , execution exception , execution occurred , is executed in thread , collapser key ) ; }	Creates a new ExecutionResult by adding the defined 'event' to the ones on the current instance.
closure create closure ( string root method name , final object closure obj ) throws exception { if ( ! is closure command ( closure obj ) ) { throw new runtime exception ( format ( error type message , root method name , get closure command type ( ) . get name ( ) ) . get message ( ) ) ; } method closure method = closure obj . get class ( ) . get method ( invoke method ) ; return new closure ( closure method , closure obj ) ; }	Creates closure.
@ override protected user account get fallback ( ) { return new user account ( user cookie . user id , user cookie . name , user cookie . account type , bool , bool , bool ) ; }	Fallback that will use data from the UserCookie and stubbed defaultsto create a UserAccount if the network call failed.
protected response handle request ( ) { response builder builder = null ; int number connections = get current connections ( ) . get ( ) ; int max number connections allowed = get max number concurrent connections allowed ( ) ;	Maintain an open connection with the client.
public static hystrix command properties . setter initialize command properties ( list < hystrix property > properties ) throws illegal argument exception { return initialize properties ( hystrix command properties . setter ( ) , properties , cmd prop map , str ) ; }	Creates and sets Hystrix command properties.
public static hystrix thread pool properties . setter initialize thread pool properties ( list < hystrix property > properties ) throws illegal argument exception { return initialize properties ( hystrix thread pool properties . setter ( ) , properties , tp prop map , str ) ; }	Creates and sets Hystrix thread pool properties.
public static hystrix collapser properties . setter initialize collapser properties ( list < hystrix property > properties ) { return initialize properties ( hystrix collapser properties . setter ( ) , properties , collapser prop map , str ) ; }	Creates and sets Hystrix collapser properties.
@ suppress warnings ( str ) public t get ( ) { if ( hystrix request context . get context for current thread ( ) == null ) { throw new illegal state exception ( hystrix request context . class . get simple name ( ) + str ) ; } concurrent hash map < hystrix request variable default < ? > , lazy initializer < ? > > variable map = hystrix request context . get context for current thread ( ) . state ;	Get the current value for this variable for the current request context.
public void clear ( string cache key ) { value cache key key = get request cache key ( cache key ) ; if ( key != null ) { concurrent hash map < value cache key , hystrix cached observable < ? > > cache instance = request variable for cache . get ( concurrency strategy ) ; if ( cache instance == null ) { throw new illegal state exception ( str ) ; } cache instance . remove ( key ) ; } }	Clear the cache for a given cacheKey.
boolean is ignorable ( throwable throwable ) { if ( ignore exceptions == null || ignore exceptions . is empty ( ) ) { return bool ; } for ( class < ? extends throwable > ignore exception : ignore exceptions ) { if ( ignore exception . is assignable from ( throwable . get class ( ) ) ) { return bool ; } } return bool ; }	Check whether triggered exception is ignorable.
private void handle request ( http servlet request request , final http servlet response response ) throws servlet exception , io { final atomic boolean more data will be sent = new atomic boolean ( bool ) ; subscription sample subscription = null ; int number connections = increment and get current concurrent connections ( ) ; try { int max number connections allowed = get max number concurrent connections allowed ( ) ;	- maintain an open connection with the client- on initial connection send latest data of each requested event type- subsequently send all changes for each requested event type.
public static void reset ( ) { singleton = new hystrix metrics publisher factory ( ) ; singleton . command publishers . clear ( ) ; singleton . thread pool publishers . clear ( ) ; singleton . collapser publishers . clear ( ) ; }	Resets the SINGLETON object.Clears all state from publishers.
public void clear cache ( cache invocation context < cache remove > context ) { hystrix cache key generator default cache key generator = hystrix cache key generator . get instance ( ) ; string cache name = context . get cache annotation ( ) . command key ( ) ; hystrix generated cache key hystrix generated cache key = default cache key generator . generate cache key ( context ) ; string key = hystrix generated cache key . get cache key ( ) ; hystrix request cache . get instance ( hystrix command key . factory . as key ( cache name ) , hystrix concurrency strategy default . get instance ( ) ) . clear ( key ) ; }	Clears the cache for a given cacheKey context.
@ override public object execute with args ( execution type execution type , object [ ] args ) throws command action execution exception { if ( execution type . asynchronous == execution type ) { closure closure = async closure factory . get instance ( ) . create closure ( meta holder , method , object , args ) ; return execute clj ( closure . get closure obj ( ) , closure . get closure method ( ) ) ; } return execute ( object , method , args ) ; }	Invokes the method. Also private method also can be invoked.
private object execute ( object o , method m , object ... args ) throws command action execution exception { object result = null ; try { m . set accessible ( bool ) ;	Invokes the method.
public void set metric registry ( object metric registry ) { if ( metrics tracker factory != null ) { throw new illegal state exception ( str ) ; } if ( metric registry != null ) { metric registry = get object or perform jndi lookup ( metric registry ) ; if ( ! safe is assignable from ( metric registry , str ) && ! ( safe is assignable from ( metric registry , str ) ) ) { throw new illegal argument exception ( str ) ; } } this . metric registry = metric registry ; }	Set a MetricRegistry instance to use for registration of metrics used by HikariCP.
@ override public void close ( ) { if ( is shutdown . get and set ( bool ) ) { return ; } hikari pool p = pool ; if ( p != null ) { try { logger . info ( str , get pool name ( ) ) ; p . shutdown ( ) ; logger . info ( str , get pool name ( ) ) ; } catch ( interrupted exception e ) { logger . warn ( str , get pool name ( ) , e ) ; thread . current thread ( ) . interrupt ( ) ; } } }	Shutdown the DataSource and its associated pool.
public static set < string > get property names ( final class < ? > target class ) { hash set < string > set = new hash set < > ( ) ; matcher matcher = getter pattern . matcher ( str ) ; for ( method method : target class . get methods ( ) ) { string name = method . get name ( ) ; if ( method . get parameter types ( ) . length == num && matcher . reset ( name ) . matches ( ) ) { name = name . replace first ( str , str ) ; try { if ( target class . get method ( str + name , method . get return type ( ) ) != null ) { name = character . to lower case ( name . char at ( num ) ) + name . substring ( num ) ; set . add ( name ) ; } } catch ( exception e ) {	Get the bean-style property names for the specified object.
void handle m ( final hikari pool hikari pool , final boolean register ) { if ( ! config . is register mbeans ( ) ) { return ; } try { final m m bean server = management factory . get platform m ( ) ; final object name bean config name = new object name ( str + pool name + str ) ; final object name bean pool name = new object name ( str + pool name + str ) ; if ( register ) { if ( ! m bean server . is registered ( bean config name ) ) { m bean server . register m ( config , bean config name ) ; m bean server . register m ( hikari pool , bean pool name ) ; } else { logger . error ( str , pool name , pool name ) ; } } else if ( m bean server . is registered ( bean config name ) ) { m bean server . unregister m ( bean config name ) ; m bean server . unregister m ( bean pool name ) ; } } catch ( exception e ) { logger . warn ( str , pool name , ( register ? str : str ) , e ) ; } }	Register MBeans for HikariConfig and HikariPool.
private connection new connection ( ) throws exception { final long start = current time ( ) ; connection connection = null ; try { string username = config . get username ( ) ; string password = config . get password ( ) ; connection = ( username == null ) ? data source . get connection ( ) : data source . get connection ( username , password ) ; setup connection ( connection ) ; last connection failure . set ( null ) ; return connection ; } catch ( exception e ) { if ( connection != null ) { quietly close connection ( connection , str ) ; } else if ( get last connection failure ( ) == null ) { logger . debug ( str , pool name , e . get message ( ) ) ; } last connection failure . set ( e ) ; throw e ; } finally {	Obtain connection from data source.
private void setup connection ( final connection connection ) throws connection setup exception { try { if ( network timeout == uninitialized ) { network timeout = get and set network timeout ( connection , validation timeout ) ; } else { set network timeout ( connection , validation timeout ) ; } if ( connection . is read only ( ) != is read only ) { connection . set read only ( is read only ) ; } if ( connection . get auto commit ( ) != is auto commit ) { connection . set auto commit ( is auto commit ) ; } check driver support ( connection ) ; if ( transaction isolation != default transaction isolation ) { connection . set transaction isolation ( transaction isolation ) ; } if ( catalog != null ) { connection . set catalog ( catalog ) ; } if ( schema != null ) { connection . set schema ( schema ) ; } execute sql ( connection , config . get connection init sql ( ) , bool ) ; set network timeout ( connection , network timeout ) ; } catch ( sql e ) { throw new connection setup exception ( e ) ; } }	Setup a connection initial state.
private void check default isolation ( final connection connection ) throws sql { try { default transaction isolation = connection . get transaction isolation ( ) ; if ( transaction isolation == - num ) { transaction isolation = default transaction isolation ; } } catch ( sql e ) { logger . warn ( str , pool name , e . get message ( ) ) ; if ( e . get sql ( ) != null && ! e . get sql ( ) . starts with ( str ) ) { throw e ; } } }	Check the default transaction isolation of the Connection.
private void set query timeout ( final statement statement , final int timeout sec ) { if ( is query timeout supported != false ) { try { statement . set query timeout ( timeout sec ) ; is query timeout supported = true ; } catch ( exception e ) { if ( is query timeout supported == uninitialized ) { is query timeout supported = false ; logger . info ( str , pool name , e . get message ( ) ) ; } } } }	Set the query timeout, if it is supported by the driver.
private void execute sql ( final connection connection , final string sql , final boolean is commit ) throws sql { if ( sql != null ) { try ( statement statement = connection . create statement ( ) ) {	Execute the user-specified init SQL.
private void set login timeout ( final data source data source ) { if ( connection timeout != integer . max value ) { try { data source . set login timeout ( math . max ( num , ( int ) milliseconds . to seconds ( num + connection timeout ) ) ) ; } catch ( exception e ) { logger . info ( str , pool name , e . get message ( ) ) ; } } }	Set the loginTimeout on the specified DataSource.
private static < t > void generate proxy class ( class < t > primary interface , string super class name , string method body ) throws exception { string new class name = super class name . replace all ( str , str ) ; ct class super ct = class pool . get ct class ( super class name ) ; ct class target ct = class pool . make class ( new class name , super ct ) ; target ct . set modifiers ( modifier . final ) ; system . out . println ( str + new class name ) ; target ct . set modifiers ( modifier . public ) ;	Generate Javassist Proxy Classes.
@ override public boolean add ( t element ) { if ( size < element data . length ) { element data [ size ++ ] = element ; } else {	Add an element to the tail of the FastList.
public t remove last ( ) { t element = element data [ -- size ] ; element data [ size ] = null ; return element ; }	Remove the last element from the list.
@ override public void clear ( ) { for ( int i = num ; i < size ; i ++ ) { element data [ i ] = null ; } size = num ; }	Clear the FastList.
public connection get connection ( final long hard timeout ) throws sql { suspend resume lock . acquire ( ) ; final long start time = current time ( ) ; try { long timeout = hard timeout ; do { pool entry pool entry = connection bag . borrow ( timeout , milliseconds ) ; if ( pool entry == null ) { break ;	Get a connection from the pool, or timeout after the specified number of milliseconds.
public synchronized void shutdown ( ) throws interrupted exception { try { pool state = pool shutdown ; if ( add connection executor == null ) {	Shutdown the pool, closing all idle connections and aborting or closingactive connections.
public void evict connection ( connection connection ) { proxy connection proxy connection = ( proxy connection ) connection ; proxy connection . cancel leak task ( ) ; try { soft evict connection ( proxy connection . get pool entry ( ) , str , ! connection . is closed ( ) ) ; } catch ( sql e ) {	Evict a Connection from the pool.
public void set metric registry ( object metric registry ) { if ( metric registry != null && safe is assignable from ( metric registry , str ) ) { set metrics tracker factory ( new codahale metrics tracker factory ( ( metric registry ) metric registry ) ) ; } else if ( metric registry != null && safe is assignable from ( metric registry , str ) ) { set metrics tracker factory ( new micrometer metrics tracker factory ( ( meter registry ) metric registry ) ) ; } else { set metrics tracker factory ( null ) ; } }	Set a metrics registry to be used when registering metrics collectors.
public void set metrics tracker factory ( metrics tracker factory metrics tracker factory ) { if ( metrics tracker factory != null ) { this . metrics tracker = new metrics tracker delegate ( metrics tracker factory . create ( config . get pool name ( ) , get pool stats ( ) ) ) ; } else { this . metrics tracker = new nop metrics tracker delegate ( ) ; } }	Set the MetricsTrackerFactory to be used to create the IMetricsTracker instance used by the pool.
public void set health check registry ( object health check registry ) { if ( health check registry != null ) { codahale health checker . register health checks ( this , config , ( health check registry ) health check registry ) ; } }	Set the health check registry to be used when registering health checks.
void log pool state ( string ... prefix ) { if ( logger . is debug enabled ( ) ) { logger . debug ( str , pool name , ( prefix . length > num ? prefix [ num ] : str ) , get total connections ( ) , get active connections ( ) , get idle connections ( ) , get threads awaiting connection ( ) ) ; } }	Log the current pool state at debug level.
private pool entry create pool entry ( ) { try { final pool entry pool entry = new pool entry ( ) ; final long max lifetime = config . get max lifetime ( ) ; if ( max lifetime > num ) {	Creating new poolEntry. If maxLifetime is configured, create a future End-of-life task with 2.5% variance fromthe maxLifetime time to ensure there is no massive die-off of Connections in the pool.
private void abort active connections ( final executor service assassin executor ) { for ( pool entry pool entry : connection bag . values ( state in use ) ) { connection connection = pool entry . close ( ) ; try { connection . abort ( assassin executor ) ; } catch ( throwable e ) { quietly close connection ( connection , str ) ; } finally { connection bag . remove ( pool entry ) ; } } }	Attempt to abort or close active connections.
private void check fail fast ( ) { final long initialization timeout = config . get initialization fail timeout ( ) ; if ( initialization timeout < num ) { return ; } final long start time = current time ( ) ; do { final pool entry pool entry = create pool entry ( ) ; if ( pool entry != null ) { if ( config . get minimum idle ( ) > num ) { connection bag . add ( pool entry ) ; logger . debug ( str , pool name , pool entry . connection ) ; } else { quietly close connection ( pool entry . close ( ) , str ) ; } return ; } if ( get last connection failure ( ) instanceof connection setup exception ) { throw pool initialization exception ( get last connection failure ( ) . get cause ( ) ) ; } quietly sleep ( seconds . to millis ( num ) ) ; } while ( elapsed millis ( start time ) < initialization timeout ) ; if ( initialization timeout > num ) { throw pool initialization exception ( get last connection failure ( ) ) ; } }	If initializationFailFast is configured, check that we have DB connectivity.
private void throw pool initialization exception ( throwable t ) { logger . error ( str , pool name , t ) ; destroy house keeping executor service ( ) ; throw new pool initialization exception ( t ) ; }	Log the Throwable that caused pool initialization to fail, and then throw a PoolInitializationException withthat cause attached.
private pool stats get pool stats ( ) { return new pool stats ( seconds . to millis ( num ) ) { @ override protected void update ( ) { this . pending threads = hikari pool . this . get threads awaiting connection ( ) ; this . idle connections = hikari pool . this . get idle connections ( ) ; this . total connections = hikari pool . this . get total connections ( ) ; this . active connections = hikari pool . this . get active connections ( ) ; this . max connections = config . get maximum pool size ( ) ; this . min connections = config . get minimum idle ( ) ; } } ; }	Create a PoolStats instance that will be used by metrics tracking, with a pollable resolution of 1 second.
public t borrow ( long timeout , final time unit time unit ) throws interrupted exception {	The method will borrow a BagEntry from the bag, blocking for thespecified timeout if none are available.
public void requite ( final t bag entry ) { bag entry . set state ( state not in use ) ; for ( int i = num ; waiters . get ( ) > num ; i ++ ) { if ( bag entry . get state ( ) != state not in use || handoff queue . offer ( bag entry ) ) { return ; } else if ( ( i & num ) == num ) { park nanos ( microseconds . to nanos ( num ) ) ; } else { yield ( ) ; } } final list < object > thread local list = thread list . get ( ) ; if ( thread local list . size ( ) < num ) { thread local list . add ( weak thread locals ? new weak reference < > ( bag entry ) : bag entry ) ; } }	This method will return a borrowed object to the bag.
public void add ( final t bag entry ) { if ( closed ) { logger . info ( str ) ; throw new illegal state exception ( str ) ; } shared list . add ( bag entry ) ;	Add a new object to the bag for others to borrow.
public int get count ( final int state ) { int count = num ; for ( i e : shared list ) { if ( e . get state ( ) == state ) { count ++ ; } } return count ; }	Get a count of the number of items in the specified state at the time of this call.
public static void register health checks ( final hikari pool pool , final hikari config hikari config , final health check registry registry ) { final properties health check properties = hikari config . get health check properties ( ) ; final metric registry metric registry = ( metric registry ) hikari config . get metric registry ( ) ; final long check timeout ms = long . parse long ( health check properties . get property ( str , string . value of ( hikari config . get connection timeout ( ) ) ) ) ; registry . register ( metric registry . name ( hikari config . get pool name ( ) , str , str ) , new connectivity health check ( pool , check timeout ms ) ) ; final long expected99th percentile = long . parse long ( health check properties . get property ( str , str ) ) ; if ( metric registry != null && expected99th percentile > num ) { sorted map < string , timer > timers = metric registry . get timers ( ( name , metric ) -> name . equals ( metric registry . name ( hikari config . get pool name ( ) , str , str ) ) ) ; if ( ! timers . is empty ( ) ) { final timer timer = timers . entry set ( ) . iterator ( ) . next ( ) . get value ( ) ; registry . register ( metric registry . name ( hikari config . get pool name ( ) , str , str ) , new connection99 percent ( timer , expected99th percentile ) ) ; } } }	Register Dropwizard health checks.
public static boolean safe is assignable from ( object obj , string class name ) { try { class < ? > clazz = class . for name ( class name ) ; return clazz . is assignable from ( obj . get class ( ) ) ; } catch ( class not found exception ignored ) { return bool ; } }	Checks whether an object is an instance of given type without throwing exception when the class is not loaded.
public static < t > t create instance ( final string class name , final class < t > clazz , final object ... args ) { if ( class name == null ) { return null ; } try { class < ? > loaded = utility elf . class . get class loader ( ) . load class ( class name ) ; if ( args . length == num ) { return clazz . cast ( loaded . new instance ( ) ) ; } class < ? > [ ] arg classes = new class < ? > [ args . length ] ; for ( int i = num ; i < args . length ; i ++ ) { arg classes [ i ] = args [ i ] . get class ( ) ; } constructor < ? > constructor = loaded . get constructor ( arg classes ) ; return clazz . cast ( constructor . new instance ( args ) ) ; } catch ( exception e ) { throw new runtime exception ( e ) ; } }	Create and instance of the specified class using the constructor matching the specifiedarguments.
public static thread pool executor create thread pool executor ( final int queue size , final string thread name , thread factory thread factory , final rejected execution handler policy ) { if ( thread factory == null ) { thread factory = new default thread factory ( thread name , bool ) ; } linked blocking queue < runnable > queue = new linked blocking queue < > ( queue size ) ; thread pool executor executor = new thread pool executor ( num , num , num , seconds , queue , thread factory , policy ) ; executor . allow core thread time out ( bool ) ; return executor ; }	Create a ThreadPoolExecutor.
public static int get transaction isolation ( final string transaction isolation name ) { if ( transaction isolation name != null ) { try {	Get the int value of a transaction isolation level by name.
@ override public < t > event poller < t > new poller ( data provider < t > data provider , sequence ... gating sequences ) { return event poller . new instance ( data provider , this , new sequence ( ) , cursor , gating sequences ) ; }	Creates an event poller for this sequence that will use the supplied data provider andgating sequences.
public event handler group < t > after ( final event processor ... processors ) { for ( final event processor processor : processors ) { consumer repository . add ( processor ) ; } return new event handler group < > ( this , consumer repository , util . get sequences for ( processors ) ) ; }	Create a group of event processors to be used as a dependency.
public < a > void publish events ( final event translator one arg < t , a > event translator , final a [ ] arg ) { ring buffer . publish events ( event translator , arg ) ; }	Publish a batch of events to the ring buffer.
private boolean has backlog ( ) { final long cursor = ring buffer . get cursor ( ) ; for ( final sequence consumer : consumer repository . get last sequence in chain ( bool ) ) { if ( cursor > consumer . get ( ) ) { return bool ; } } return bool ; }	Confirms if all messages have been consumed by all event processors.
public ring buffer < t > start ( final executor executor ) { if ( ! started . compare and set ( bool , bool ) ) { throw new illegal state exception ( str ) ; } final long cursor = ring buffer . get cursor ( ) ; work sequence . set ( cursor ) ; for ( work processor < ? > processor : work processors ) { processor . get sequence ( ) . set ( cursor ) ; executor . execute ( processor ) ; } return ring buffer ; }	Start the worker pool processing events in sequence.
private void notify start ( ) { if ( event handler instanceof lifecycle aware ) { try { ( ( lifecycle aware ) event handler ) . on start ( ) ; } catch ( final throwable ex ) { exception handler . handle on start exception ( ex ) ; } } }	Notifies the EventHandler when this processor is starting up.
private void notify shutdown ( ) { if ( event handler instanceof lifecycle aware ) { try { ( ( lifecycle aware ) event handler ) . on shutdown ( ) ; } catch ( final throwable ex ) { exception handler . handle on shutdown exception ( ex ) ; } } }	Notifies the EventHandler immediately prior to this processor shutting down.
public long add and get ( final long increment ) { long current value ; long new value ; do { current value = get ( ) ; new value = current value + increment ; } while ( ! compare and set ( current value , new value ) ) ; return new value ; }	Atomically add the supplied value.
public static < e > ring buffer < e > create multi producer ( event factory < e > factory , int buffer size , wait strategy wait strategy ) { multi producer sequencer sequencer = new multi producer sequencer ( buffer size , wait strategy ) ; return new ring buffer < e > ( factory , sequencer ) ; }	Create a new multiple producer RingBuffer with the specified wait strategy.
public static < e > ring buffer < e > create single producer ( event factory < e > factory , int buffer size , wait strategy wait strategy ) { single producer sequencer sequencer = new single producer sequencer ( buffer size , wait strategy ) ; return new ring buffer < e > ( factory , sequencer ) ; }	Create a new single producer RingBuffer with the specified wait strategy.
public static decoder result decode ( bit matrix image , result point image top left , result point image bottom left , result point image top right , result point image bottom right , int min codeword width , int max codeword width ) throws not found exception , format exception , checksum exception { bounding box bounding box = new bounding box ( image , image top left , image bottom left , image top right , image bottom right ) ; detection result row indicator column left row indicator column = null ; detection result row indicator column right row indicator column = null ; detection result detection result ; for ( boolean first pass = bool ; ; first pass = bool ) { if ( image top left != null ) { left row indicator column = get row indicator column ( image , bounding box , image top left , bool , min codeword width , max codeword width ) ; } if ( image top right != null ) { right row indicator column = get row indicator column ( image , bounding box , image top right , bool , min codeword width , max codeword width ) ; } detection result = merge ( left row indicator column , right row indicator column ) ; if ( detection result == null ) { throw not found exception . get not found instance ( ) ; } bounding box result box = detection result . get bounding box ( ) ; if ( first pass && result box != null && ( result box . get min y ( ) < bounding box . get min y ( ) || result box . get max y ( ) > bounding box . get max y ( ) ) ) { bounding box = result box ; } else { break ; } } detection result . set bounding box ( bounding box ) ; int max barcode column = detection result . get barcode column count ( ) + num ; detection result . set detection result column ( num , left row indicator column ) ; detection result . set detection result column ( max barcode column , right row indicator column ) ; boolean left to right = left row indicator column != null ; for ( int barcode column count = num ; barcode column count <= max barcode column ; barcode column count ++ ) { int barcode column = left to right ? barcode column count : max barcode column - barcode column count ; if ( detection result . get detection result column ( barcode column ) != null ) {	than it should be.
private static void verify codeword count ( int [ ] codewords , int num ec ) throws format exception { if ( codewords . length < num ) {	Verify that all is OK with the codeword array.
void remask ( ) { if ( parsed format info == null ) { return ;	Revert the mask removal done while reading the code words.
void mirror ( ) { for ( int x = num ; x < bit matrix . get width ( ) ; x ++ ) { for ( int y = x + num ; y < bit matrix . get height ( ) ; y ++ ) { if ( bit matrix . get ( x , y ) != bit matrix . get ( y , x ) ) { bit matrix . flip ( y , x ) ; bit matrix . flip ( x , y ) ; } } } }	Mirror the bit matrix in order to attempt a second reading.
public static char sequence download via http ( string uri , content type type ) throws io { return download via http ( uri , type , integer . max value ) ; }	Downloads the entire resource instead of part.
private boolean cross check diagonal ( int center i , int center j ) { int [ ] state count = get cross check state count ( ) ;	After a vertical and horizontal scan finds a potential finder pattern, this method"cross-cross-cross-checks" by scanning down diagonally through the center of the possiblefinder pattern to see if the same proportion is detected.
private static double squared distance ( finder pattern a , finder pattern b ) { double x = a . get x ( ) - b . get x ( ) ; double y = a . get y ( ) - b . get y ( ) ; return x * x + y * y ; }	Get square of distance between a and b.
private static bit matrix bit matrix from bit array ( byte [ ] [ ] input , int margin ) {	This takes an array holding the values of the PDF 417.
private static byte [ ] [ ] rotate array ( byte [ ] [ ] bitarray ) { byte [ ] [ ] temp = new byte [ bitarray [ num ] . length ] [ bitarray . length ] ; for ( int ii = num ; ii < bitarray . length ; ii ++ ) {	Takes and rotates the it 90 degrees.
private static int to narrow wide pattern ( int [ ] counters ) { int num counters = counters . length ; int max narrow counter = num ; int wide counters ; do { int min counter = integer . max value ; for ( int counter : counters ) { if ( counter < min counter && counter > max narrow counter ) { min counter = counter ; } } max narrow counter = min counter ; wide counters = num ; int total wide counters width = num ; int pattern = num ; for ( int i = num ; i < num counters ; i ++ ) { int counter = counters [ i ] ; if ( counter > max narrow counter ) { pattern |= num << ( num counters - num - i ) ; wide counters ++ ; total wide counters width += counter ; } } if ( wide counters == num ) {	per image when using some of our blackbox images.
private static list < result point [ ] > detect ( boolean multiple , bit matrix bit matrix ) { list < result point [ ] > barcode coordinates = new array list < > ( ) ; int row = num ; int column = num ; boolean found barcode in row = bool ; while ( row < bit matrix . get height ( ) ) { result point [ ] vertices = find vertices ( bit matrix , row , column ) ; if ( vertices [ num ] == null && vertices [ num ] == null ) { if ( ! found barcode in row ) {	Detects PDF417 codes in an image. Only checks 0 degree rotation.
private static result point [ ] find vertices ( bit matrix matrix , int start row , int start column ) { int height = matrix . get height ( ) ; int width = matrix . get width ( ) ; result point [ ] result = new result point [ num ] ; copy to result ( result , find rows with pattern ( matrix , height , width , start row , start column , start pattern ) , indexes start pattern ) ; if ( result [ num ] != null ) { start column = ( int ) result [ num ] . get x ( ) ; start row = ( int ) result [ num ] . get y ( ) ; } copy to result ( result , find rows with pattern ( matrix , height , width , start row , start column , stop pattern ) , indexes stop pattern ) ; return result ; }	Locate the vertices and the codewords area of a black blob using the Startand Stop patterns as locators.
public synchronized void open driver ( surface holder holder ) throws io { open camera the camera = camera ; if ( the camera == null ) { the camera = open camera interface . open ( requested camera id ) ; if ( the camera == null ) { throw new io ( str ) ; } camera = the camera ; } if ( ! initialized ) { initialized = bool ; config manager . init from camera parameters ( the camera ) ; if ( requested framing rect width > num && requested framing rect height > num ) { set manual framing rect ( requested framing rect width , requested framing rect height ) ; requested framing rect width = num ; requested framing rect height = num ; } } camera camera object = the camera . get camera ( ) ; camera . parameters parameters = camera object . get parameters ( ) ; string parameters flattened = parameters == null ? null : parameters . flatten ( ) ;	Opens the camera driver and initializes the hardware parameters.
public synchronized void close driver ( ) { if ( camera != null ) { camera . get camera ( ) . release ( ) ; camera = null ;	Closes the camera driver if still in use.
public synchronized rect get framing rect ( ) { if ( framing rect == null ) { if ( camera == null ) { return null ; } point screen resolution = config manager . get screen resolution ( ) ; if ( screen resolution == null ) {	Calculates the framing rect which the UI should draw to show the user where to place thebarcode.
public synchronized void set manual framing rect ( int width , int height ) { if ( initialized ) { point screen resolution = config manager . get screen resolution ( ) ; if ( width > screen resolution . x ) { width = screen resolution . x ; } if ( height > screen resolution . y ) { height = screen resolution . y ; } int left offset = ( screen resolution . x - width ) / num ; int top offset = ( screen resolution . y - height ) / num ; framing rect = new rect ( left offset , top offset , left offset + width , top offset + height ) ; log . d ( tag , str + framing rect ) ; framing rect in preview = null ; } else { requested framing rect width = width ; requested framing rect height = height ; } }	Allows third party apps to specify the scanning rectangle dimensions, rather than determinethem automatically based on screen resolution.
public yuv build luminance source ( byte [ ] data , int width , int height ) { rect rect = get framing rect in preview ( ) ; if ( rect == null ) { return null ; }	A factory method to build the appropriate LuminanceSource object based on the formatof the preview buffers, as described by Camera.Parameters.
@ override public uri parse ( result result ) { string raw text = get massaged text ( result ) ;	query, path or nothing.
public aztec detector result detect ( boolean is mirror ) throws not found exception {	Detects an Aztec Code in an image.
private void extract parameters ( result point [ ] bulls eye corners ) throws not found exception { if ( ! is valid ( bulls eye corners [ num ] ) || ! is valid ( bulls eye corners [ num ] ) || ! is valid ( bulls eye corners [ num ] ) || ! is valid ( bulls eye corners [ num ] ) ) { throw not found exception . get not found instance ( ) ; } int length = num * nb center layers ;	Extracts the number of data layers and data blocks from the layer around the bull's eye.
private static int get corrected parameter data ( long parameter data , boolean compact ) throws not found exception { int num codewords ; int num data codewords ; if ( compact ) { num codewords = num ; num data codewords = num ; } else { num codewords = num ; num data codewords = num ; } int num ec = num codewords - num data codewords ; int [ ] parameter words = new int [ num codewords ] ; for ( int i = num codewords - num ; i >= num ; -- i ) { parameter words [ i ] = ( int ) parameter data & num ; parameter data >>= num ; } try { reed solomon decoder rs decoder = new reed solomon decoder ( gf . aztec param ) ; rs decoder . decode ( parameter words , num ec ) ; } catch ( reed solomon exception ignored ) { throw not found exception . get not found instance ( ) ; }	Corrects the parameter bits using Reed-Solomon algorithm.
private bit matrix sample grid ( bit matrix image , result point top left , result point top right , result point bottom right , result point bottom left ) throws not found exception { grid sampler sampler = grid sampler . get instance ( ) ; int dimension = get dimension ( ) ; float low = dimension / num - nb center layers ; float high = dimension / num + nb center layers ; return sampler . sample grid ( image , dimension , dimension , low , low ,	Creates a BitMatrix by sampling the provided image.topLeft, topRight, bottomRight, and bottomLeft are the centers of the squares on thediagonal just outside the bull's eye.
private int sample line ( result point p1 , result point p2 , int size ) { int result = num ; float d = distance ( p1 , p2 ) ; float module size = d / size ; float px = p1 . get x ( ) ; float py = p1 . get y ( ) ; float dx = module size * ( p2 . get x ( ) - p1 . get x ( ) ) / d ; float dy = module size * ( p2 . get y ( ) - p1 . get y ( ) ) / d ; for ( int i = num ; i < size ; i ++ ) { if ( image . get ( math utils . round ( px + i * dx ) , math utils . round ( py + i * dy ) ) ) { result |= num << ( size - i - num ) ; } } return result ; }	Samples a line.
private int get color ( point p1 , point p2 ) { float d = distance ( p1 , p2 ) ; float dx = ( p2 . get x ( ) - p1 . get x ( ) ) / d ; float dy = ( p2 . get y ( ) - p1 . get y ( ) ) / d ; int error = num ; float px = p1 . get x ( ) ; float py = p1 . get y ( ) ; boolean color model = image . get ( p1 . get x ( ) , p1 . get y ( ) ) ; int i max = ( int ) math . ceil ( d ) ; for ( int i = num ; i < i max ; i ++ ) { px += dx ; py += dy ; if ( image . get ( math utils . round ( px ) , math utils . round ( py ) ) != color model ) { error ++ ; } } float err ratio = error / d ; if ( err ratio > num && err ratio < num ) { return num ; } return ( err ratio <= num ) == color model ? num : - num ; }	Gets the color of a segment.
private point get first different ( point init , boolean color , int dx , int dy ) { int x = init . get x ( ) + dx ; int y = init . get y ( ) + dy ; while ( is valid ( x , y ) && image . get ( x , y ) == color ) { x += dx ; y += dy ; } x -= dx ; y -= dy ; while ( is valid ( x , y ) && image . get ( x , y ) == color ) { x += dx ; } x -= dx ; while ( is valid ( x , y ) && image . get ( x , y ) == color ) { y += dy ; } y -= dy ; return new point ( x , y ) ; }	Gets the coordinate of the first point with a different color in the given direction.
private static void decode hanzi segment ( bit source bits , string builder result , int count ) throws format exception {	See specification GBT 18284-2000.
private static void change network wep ( wifi manager wifi manager , wifi parsed result wifi result ) { wifi configuration config = change network common ( wifi result ) ; config . wep keys [ num ] = quote non hex ( wifi result . get password ( ) , num , num , num ) ; config . wep tx key index = num ; config . allowed auth algorithms . set ( wifi configuration . auth algorithm . shared ) ; config . allowed key management . set ( wifi configuration . key mgmt . none ) ; config . allowed group ciphers . set ( wifi configuration . group cipher . tkip ) ; config . allowed group ciphers . set ( wifi configuration . group cipher . ccmp ) ; config . allowed group ciphers . set ( wifi configuration . group cipher . we ) ; config . allowed group ciphers . set ( wifi configuration . group cipher . we ) ; update network ( wifi manager , config ) ; }	Adding a WEP network.
private static void change network wpa ( wifi manager wifi manager , wifi parsed result wifi result ) { wifi configuration config = change network common ( wifi result ) ;	Adding a WPA or WPA2 network.
private static void change network un encrypted ( wifi manager wifi manager , wifi parsed result wifi result ) { wifi configuration config = change network common ( wifi result ) ; config . allowed key management . set ( wifi configuration . key mgmt . none ) ; update network ( wifi manager , config ) ; }	Adding an open, unsecured network.
private static string convert to quoted string ( string s ) { if ( s == null || s . is empty ( ) ) { return null ; }	Encloses the incoming string inside double quotes, if it isn't already quoted.
@ override public char sequence get display contents ( ) { wifi parsed result wifi result = ( wifi parsed result ) get result ( ) ; return wifi result . get ssid ( ) + str + wifi result . get network encryption ( ) + str ; }	Display the name of the network and the network type to the user.
private static float cross product z ( result point point a , result point point b , result point point c ) { float b x = point b . x ; float b y = point b . y ; return ( ( point c . x - b x ) * ( point a . y - b y ) ) - ( ( point c . y - b y ) * ( point a . x - b x ) ) ; }	Returns the z component of the cross product between vectors BC and BA.
private void encode contents from z ( intent intent ) {	but we use platform specific code like PhoneNumberUtils, so it can't.
private void encode contents from share intent ( intent intent ) throws writer exception {	Handles send intents from multitude of Android applications.
private void encode from stream extra ( intent intent ) throws writer exception { format = barcode format . qr code ; bundle bundle = intent . get extras ( ) ; if ( bundle == null ) { throw new writer exception ( str ) ; } uri uri = bundle . get parcelable ( intent . extra stream ) ; if ( uri == null ) { throw new writer exception ( str ) ; } byte [ ] vcard ; string vcard string ; try ( input stream stream = activity . get content resolver ( ) . open input stream ( uri ) ) { if ( stream == null ) { throw new writer exception ( str + uri ) ; } byte array output stream baos = new byte array output stream ( ) ; byte [ ] buffer = new byte [ num ] ; int bytes read ; while ( ( bytes read = stream . read ( buffer ) ) > num ) { baos . write ( buffer , num , bytes read ) ; } vcard = baos . to byte array ( ) ; vcard string = new string ( vcard , num , vcard . length , standard charsets . utf 8 ) ; } catch ( io ioe ) { throw new writer exception ( ioe ) ; } log . d ( tag , str ) ; log . d ( tag , vcard string ) ; result result = new result ( vcard string , vcard , null , barcode format . qr code ) ; parsed result parsed result = result parser . parse result ( result ) ; if ( ! ( parsed result instanceof address book parsed result ) ) { throw new writer exception ( str ) ; } encode qr ( ( address book parsed result ) parsed result ) ; if ( contents == null || contents . is empty ( ) ) { throw new writer exception ( str ) ; } }	Handles send intents from the Contacts app, retrieving a contact as a VCARD.
private void set counters ( bit array row ) throws not found exception { counter length = num ;	Records the size of all runs of white and black pixels, starting with white.This is just like recordPattern, except it records all the counters, anduses our builtin "counters" member for storage.
@ override public result decode ( binary bitmap image , map < decode hint type , ? > hints ) throws not found exception , format exception { try { return do decode ( image , hints ) ; } catch ( not found exception nfe ) { boolean try harder = hints != null && hints . contains key ( decode hint type . try harder ) ; if ( try harder && image . is rotate supported ( ) ) { binary bitmap rotated image = image . rotate counter clockwise ( ) ; result result = do decode ( rotated image , hints ) ;	Note that we don't try rotation without the try harder flag, even if rotation was supported.
@ override public bit matrix get black matrix ( ) throws not found exception { if ( matrix != null ) { return matrix ; } luminance source source = get luminance source ( ) ; int width = source . get width ( ) ; int height = source . get height ( ) ; if ( width >= minimum dimension && height >= minimum dimension ) { byte [ ] luminances = source . get matrix ( ) ; int sub width = width > > block size power ; if ( ( width & block size mask ) != num ) { sub width ++ ; } int sub height = height > > block size power ; if ( ( height & block size mask ) != num ) { sub height ++ ; } int [ ] [ ] black points = calculate black points ( luminances , sub width , sub height , width , height ) ; bit matrix new matrix = new bit matrix ( width , height ) ; calculate threshold for block ( luminances , sub width , sub height , width , height , black points , new matrix ) ; matrix = new matrix ; } else {	Calculates the final BitMatrix once for all requests.
private static void threshold block ( byte [ ] luminances , int xoffset , int yoffset , int threshold , int stride , bit matrix matrix ) { for ( int y = num , offset = yoffset * stride + xoffset ; y < block size ; y ++ , offset += stride ) { for ( int x = num ; x < block size ; x ++ ) {	Applies a single threshold to a block of pixels.
private int map index to action ( int index ) { if ( index < button count ) { int count = - num ; for ( int x = num ; x < max button count ; x ++ ) { if ( fields [ x ] ) { count ++ ; } if ( count == index ) { return x ; } } } return - num ; }	positions, based on which fields are present in this barcode.
@ override public char sequence get display contents ( ) { address book parsed result result = ( address book parsed result ) get result ( ) ; string builder contents = new string builder ( num ) ; parsed result . maybe append ( result . get names ( ) , contents ) ; int names length = contents . length ( ) ; string pronunciation = result . get pronunciation ( ) ; if ( pronunciation != null && ! pronunciation . is empty ( ) ) { contents . append ( str ) ; contents . append ( pronunciation ) ; contents . append ( str ) ; } parsed result . maybe append ( result . get title ( ) , contents ) ; parsed result . maybe append ( result . get org ( ) , contents ) ; parsed result . maybe append ( result . get addresses ( ) , contents ) ; string [ ] numbers = result . get phone numbers ( ) ; if ( numbers != null ) { for ( string number : numbers ) { if ( number != null ) { parsed result . maybe append ( format phone ( number ) , contents ) ; } } } parsed result . maybe append ( result . get emails ( ) , contents ) ; parsed result . maybe append ( result . get ur ( ) , contents ) ; string birthday = result . get birthday ( ) ; if ( birthday != null && ! birthday . is empty ( ) ) { long date = parse date ( birthday ) ; if ( date >= num ) { parsed result . maybe append ( date format . get date instance ( date format . medium ) . format ( date ) , contents ) ; } } parsed result . maybe append ( result . get note ( ) , contents ) ; if ( names length > num ) {	Overriden so we can hyphenate phone numbers, format birthdays, and bold the name.
private static void format names ( iterable < list < string > > names ) { if ( names != null ) { for ( list < string > list : names ) { string name = list . get ( num ) ; string [ ] components = new string [ num ] ; int start = num ; int end ; int component index = num ; while ( component index < components . length - num && ( end = name . index of ( str , start ) ) >= num ) { components [ component index ] = name . substring ( start , end ) ; component index ++ ; start = end + num ; } components [ component index ] = name . substring ( start ) ; string builder new name = new string builder ( num ) ; maybe append component ( components , num , new name ) ; maybe append component ( components , num , new name ) ; maybe append component ( components , num , new name ) ; maybe append component ( components , num , new name ) ; maybe append component ( components , num , new name ) ; list . set ( num , new name . to string ( ) . trim ( ) ) ; } } }	Formats name fields of the form "Public;John;Q.;Reverend;III" into a form like"Reverend John Q.
byte [ ] get scaled row ( int scale ) { byte [ ] output = new byte [ row . length * scale ] ; for ( int i = num ; i < output . length ; i ++ ) { output [ i ] = row [ i / scale ] ; } return output ; }	This function scales the row.
static int apply mask penalty rule4 ( byte matrix matrix ) { int num dark cells = num ; byte [ ] [ ] array = matrix . get array ( ) ; int width = matrix . get width ( ) ; int height = matrix . get height ( ) ; for ( int y = num ; y < height ; y ++ ) { byte [ ] array y = array [ y ] ; for ( int x = num ; x < width ; x ++ ) { if ( array y [ x ] == num ) { num dark cells ++ ; } } } int num total cells = matrix . get height ( ) * matrix . get width ( ) ; int five percent variances = math . abs ( num dark cells * num - num total cells ) * num / num total cells ; return five percent variances * n4 ; }	Apply mask penalty rule 4 and return the penalty.
private static int apply mask penalty rule1 internal ( byte matrix matrix , boolean is horizontal ) { int penalty = num ; int i limit = is horizontal ? matrix . get height ( ) : matrix . get width ( ) ; int j limit = is horizontal ? matrix . get width ( ) : matrix . get height ( ) ; byte [ ] [ ] array = matrix . get array ( ) ; for ( int i = num ; i < i limit ; i ++ ) { int num same bit cells = num ; int prev bit = - num ; for ( int j = num ; j < j limit ; j ++ ) { int bit = is horizontal ? array [ i ] [ j ] : array [ j ] [ i ] ; if ( bit == prev bit ) { num same bit cells ++ ; } else { if ( num same bit cells >= num ) { penalty += n1 + ( num same bit cells - num ) ; } num same bit cells = num ;	Helper function for applyMaskPenaltyRule1.
@ override public address book parsed result parse ( result result ) { string raw text = get massaged text ( result ) ; if ( ! raw text . starts with ( str ) ) { return null ; } string first name = match single do co mo prefixed field ( str , raw text , bool ) ; string last name = match single do co mo prefixed field ( str , raw text , bool ) ; string full name = build name ( first name , last name ) ; string title = match single do co mo prefixed field ( str , raw text , bool ) ; string org = match single do co mo prefixed field ( str , raw text , bool ) ; string [ ] addresses = match do co mo prefixed field ( str , raw text , bool ) ; string phone number1 = match single do co mo prefixed field ( str , raw text , bool ) ; string phone number2 = match single do co mo prefixed field ( str , raw text , bool ) ; string phone number3 = match single do co mo prefixed field ( str , raw text , bool ) ; string email = match single do co mo prefixed field ( str , raw text , bool ) ; return new address book parsed result ( maybe wrap ( full name ) , null , null , build phone numbers ( phone number1 , phone number2 , phone number3 ) , null , maybe wrap ( email ) , null , null , null , addresses , null , org , null , title , null , null ) ; }	DoCoMo's proposed formats.
public void set range ( int start , int end ) { if ( end < start || start < num || end > size ) { throw new illegal argument exception ( ) ; } if ( end == start ) { return ; } end -- ;	Sets a range of bits.
public boolean is range ( int start , int end , boolean value ) { if ( end < start || start < num || end > size ) { throw new illegal argument exception ( ) ; } if ( end == start ) { return bool ;	Efficient method to check if a range of bits is set, or not set.
public void append bits ( int value , int num bits ) { if ( num bits < num || num bits > num ) { throw new illegal argument exception ( str ) ; } ensure capacity ( size + num bits ) ; for ( int num bits left = num bits ; num bits left > num ; num bits left -- ) { append bit ( ( ( value > > ( num bits left - num ) ) & num ) == num ) ; } }	Appends the least-significant bits, from value, in order from most-significant toleast-significant.
public void reverse ( ) { int [ ] new bits = new int [ bits . length ] ;	Reverses all bits in the array.
private static int determine consecutive text count ( char sequence msg , int startpos ) { int len = msg . length ( ) ; int idx = startpos ; while ( idx < len ) { char ch = msg . char at ( idx ) ; int numeric count = num ; while ( numeric count < num && is digit ( ch ) && idx < len ) { numeric count ++ ; idx ++ ; if ( idx < len ) { ch = msg . char at ( idx ) ; } } if ( numeric count >= num ) { return idx - startpos - numeric count ; } if ( numeric count > num ) {	Determines the number of consecutive characters that are encodable using text compaction.
private static int determine consecutive binary count ( string msg , int startpos , charset encoding ) throws writer exception { charset encoder encoder = encoding . new encoder ( ) ; int len = msg . length ( ) ; int idx = startpos ; while ( idx < len ) { char ch = msg . char at ( idx ) ; int numeric count = num ; while ( numeric count < num && is digit ( ch ) ) { numeric count ++ ;	Determines the number of consecutive characters that are encodable using binary compaction.
public static int determine consecutive digit count ( char sequence msg , int startpos ) { int count = num ; int len = msg . length ( ) ; int idx = startpos ; if ( idx < len ) { char ch = msg . char at ( idx ) ; while ( is digit ( ch ) && idx < len ) { count ++ ; idx ++ ; if ( idx < len ) { ch = msg . char at ( idx ) ; } } } return count ; }	Determines the number of consecutive characters that are encodable using numeric compaction.
final void search map ( string address ) { launch intent ( new intent ( intent . action view , uri . parse ( str + uri . encode ( address ) ) ) ) ; }	Do a geo search using the address as the query.
final void open product search ( string upc ) { uri uri = uri . parse ( str + locale manager . get product search country tld ( activity ) + str + upc + str ) ; launch intent ( new intent ( intent . action view , uri ) ) ; }	Uses the mobile-specific version of Product Search, which is formatted for small screens.
private void decode ( byte [ ] data , int width , int height ) { long start = system . nano time ( ) ; result raw result = null ; yuv source = activity . get camera manager ( ) . build luminance source ( data , width , height ) ; if ( source != null ) { binary bitmap bitmap = new binary bitmap ( new hybrid binarizer ( source ) ) ; try { raw result = multi format reader . decode with state ( bitmap ) ; } catch ( reader exception re ) {	Decode the data within the viewfinder rectangle, and time how long it took.
private result point [ ] detect solid1 ( result point [ ] corner points ) {	Detect a solid side which has minimum transition.
private result point [ ] detect solid2 ( result point [ ] points ) {	Detect a second solid side next to first solid side.
private result point correct top right ( result point [ ] points ) {	Calculates the corner position of the white top right module.
private result point [ ] shift to module center ( result point [ ] points ) {	Shift the edge points to the module center.
protected void start activity for result ( intent intent , int code ) { if ( fragment == null ) { activity . start activity for result ( intent , code ) ; } else { fragment . start activity for result ( intent , code ) ; } }	Start an activity. This method is defined to allow different methods of activity starting fornewer versions of Android and for compatibility library.
public final alert dialog share text ( char sequence text , char sequence type ) { intent intent = new intent ( ) ; intent . add category ( intent . category default ) ; intent . set action ( bs package + str ) ; intent . put extra ( str , type ) ; intent . put extra ( str , text ) ; string target app package = find target app package ( intent ) ; if ( target app package == null ) { return show download dialog ( ) ; } intent . set package ( target app package ) ; intent . add flags ( intent . flag activity clear top ) ; intent . add flags ( flag new doc ) ; attach more extras ( intent ) ; if ( fragment == null ) { activity . start activity ( intent ) ; } else { fragment . start activity ( intent ) ; } return null ; }	Shares the given text by encoding it as a barcode, such that another user canscan the text off the screen of the device.
@ override public bit array get black row ( int y , bit array row ) throws not found exception { luminance source source = get luminance source ( ) ; int width = source . get width ( ) ; if ( row == null || row . get size ( ) < width ) { row = new bit array ( width ) ; } else { row . clear ( ) ; } init arrays ( width ) ; byte [ ] local luminances = source . get row ( y , luminances ) ; int [ ] local buckets = buckets ; for ( int x = num ; x < width ; x ++ ) { local buckets [ ( local luminances [ x ] & num ) > > luminance shift ] ++ ; } int black point = estimate black point ( local buckets ) ; if ( width < num ) {	Applies simple sharpening to the row data to improve performance of the 1D Readers.
@ override public bit matrix get black matrix ( ) throws not found exception { luminance source source = get luminance source ( ) ; int width = source . get width ( ) ; int height = source . get height ( ) ; bit matrix matrix = new bit matrix ( width , height ) ;	Does not sharpen the data, as this call is intended to only be used by 2D Readers.
state latch and append ( int mode , int value ) {	necessary different) mode, and then a code.
state shift and append ( int mode , int value ) {	to a different mode to output a single value.
state add binary shift char ( int index ) { token token = this . token ; int mode = this . mode ; int bit count = this . bit count ; if ( this . mode == high level encoder . mode punct || this . mode == high level encoder . mode digit ) {	output in Binary Shift mode.
state end binary shift ( int index ) { if ( binary shift byte count == num ) { return this ; } token token = this . token ; token = token . add binary shift ( index - binary shift byte count , binary shift byte count ) ;	Binary Shift mode.
boolean is better than or equal to ( state other ) { int new mode bit count = this . bit count + ( high level encoder . latch table [ this . mode ] [ other . mode ] > > num ) ; if ( this . binary shift byte count < other . binary shift byte count ) {	state under all possible circumstances.
private static bit matrix encode low level ( default placement placement , symbol info symbol info , int width , int height ) { int symbol width = symbol info . get symbol data width ( ) ; int symbol height = symbol info . get symbol data height ( ) ; byte matrix matrix = new byte matrix ( symbol info . get symbol width ( ) , symbol info . get symbol height ( ) ) ; int matrix y = num ; for ( int y = num ; y < symbol height ; y ++ ) {	Encode the given symbol info to a bit matrix.
private static bit matrix convert byte matrix to bit matrix ( byte matrix matrix , int req width , int req height ) { int matrix width = matrix . get width ( ) ; int matrix height = matrix . get height ( ) ; int output width = math . max ( req width , matrix width ) ; int output height = math . max ( req height , matrix height ) ; int multiple = math . min ( output width / matrix width , output height / matrix height ) ; int left padding = ( output width - ( matrix width * multiple ) ) / num ; int top padding = ( output height - ( matrix height * multiple ) ) / num ; bit matrix output ;	Convert the ByteMatrix to BitMatrix.
private static int skip white space ( bit array row ) throws not found exception { int width = row . get size ( ) ; int end start = row . get next set ( num ) ; if ( end start == width ) { throw not found exception . get not found instance ( ) ; } return end start ; }	Skip all whitespace until we get to the first black line.
public bit array get row ( int y , bit array row ) { if ( row == null || row . get size ( ) < width ) { row = new bit array ( width ) ; } else { row . clear ( ) ; } int offset = y * row size ; for ( int x = num ; x < row size ; x ++ ) { row . set bulk ( x * num , bits [ offset + x ] ) ; } return row ; }	A fast method to retrieve one row of data from the matrix as a BitArray.
public int [ ] get enclosing rectangle ( ) { int left = width ; int top = height ; int right = - num ; int bottom = - num ; for ( int y = num ; y < height ; y ++ ) { for ( int x32 = num ; x32 < row size ; x32 ++ ) { int the bits = bits [ y * row size + x32 ] ; if ( the bits != num ) { if ( y < top ) { top = y ; } if ( y > bottom ) { bottom = y ; } if ( x32 * num < left ) { int bit = num ; while ( ( the bits << ( num - bit ) ) == num ) { bit ++ ; } if ( ( x32 * num + bit ) < left ) { left = x32 * num + bit ; } } if ( x32 * num + num > right ) { int bit = num ; while ( ( the bits > > > bit ) == num ) { bit -- ; } if ( ( x32 * num + bit ) > right ) { right = x32 * num + bit ; } } } } } if ( right < left || bottom < top ) { return null ; } return new int [ ] { left , top , right - left + num , bottom - top + num } ; }	This is useful in detecting the enclosing rectangle of a 'pure' barcode.
public int [ ] get top left on bit ( ) { int bits offset = num ; while ( bits offset < bits . length && bits [ bits offset ] == num ) { bits offset ++ ; } if ( bits offset == bits . length ) { return null ; } int y = bits offset / row size ; int x = ( bits offset % row size ) * num ; int the bits = bits [ bits offset ] ; int bit = num ; while ( ( the bits << ( num - bit ) ) == num ) { bit ++ ; } x += bit ; return new int [ ] { x , y } ; }	This is useful in detecting a corner of a 'pure' barcode.
static void build matrix ( bit array data bits , error correction level ec level , version version , int mask pattern , byte matrix matrix ) throws writer exception { clear matrix ( matrix ) ; embed basic patterns ( version , matrix ) ;	success, store the result in "matrix" and return true.
static void embed basic patterns ( version version , byte matrix matrix ) throws writer exception {	- Position adjustment patterns, if need be.
static void embed type info ( error correction level ec level , int mask pattern , byte matrix matrix ) throws writer exception { bit array type info bits = new bit array ( ) ; make type info bits ( ec level , mask pattern , type info bits ) ; for ( int i = num ; i < type info bits . get size ( ) ; ++ i ) {	Embed type information. On success, modify the matrix.
static int calculate bch ( int value , int poly ) { if ( poly == num ) { throw new illegal argument exception ( str ) ; }	operations. We don't care if coefficients are positive or negative.
private static void maybe embed position adjustment patterns ( version version , byte matrix matrix ) { if ( version . get version number ( ) < num ) {	Embed position adjustment patterns if need be.
list < expanded pair > decode row2pairs ( int row number , bit array row ) throws not found exception { boolean done = bool ; while ( ! done ) { try { this . pairs . add ( retrieve next pair ( row , this . pairs , row number ) ) ; } catch ( not found exception nfe ) { if ( this . pairs . is empty ( ) ) { throw nfe ; }	Not private for testing.
private list < expanded pair > check rows ( list < expanded row > collected rows , int current row ) throws not found exception { for ( int i = current row ; i < rows . size ( ) ; i ++ ) { expanded row row = rows . get ( i ) ; this . pairs . clear ( ) ; for ( expanded row collected row : collected rows ) { this . pairs . add all ( collected row . get pairs ( ) ) ; } this . pairs . add all ( row . get pairs ( ) ) ; if ( ! is valid sequence ( this . pairs ) ) { continue ; } if ( check checksum ( ) ) { return this . pairs ; } list < expanded row > rs = new array list < > ( collected rows ) ; rs . add ( row ) ; try {	Recursion is used to implement backtracking.
private static boolean is valid sequence ( list < expanded pair > pairs ) { for ( int [ ] sequence : finder pattern sequences ) { if ( pairs . size ( ) > sequence . length ) { continue ; } boolean stop = bool ; for ( int j = num ; j < pairs . size ( ) ; j ++ ) { if ( pairs . get ( j ) . get finder pattern ( ) . get value ( ) != sequence [ j ] ) { stop = bool ; break ; } } if ( stop ) { return bool ; } } return bool ; }	either complete or a prefix.
private static void remove partial rows ( list < expanded pair > pairs , list < expanded row > rows ) { for ( iterator < expanded row > iterator = rows . iterator ( ) ; iterator . has next ( ) ; ) { expanded row r = iterator . next ( ) ; if ( r . get pairs ( ) . size ( ) == pairs . size ( ) ) { continue ; } boolean all found = bool ; for ( expanded pair p : r . get pairs ( ) ) { boolean found = bool ; for ( expanded pair pp : pairs ) { if ( p . equals ( pp ) ) { found = bool ; break ; } } if ( ! found ) { all found = bool ; break ; } } if ( all found ) {	Remove all the rows that contains only specified pairs.
private static boolean is partial row ( iterable < expanded pair > pairs , iterable < expanded row > rows ) { for ( expanded row r : rows ) { boolean all found = bool ; for ( expanded pair p : pairs ) { boolean found = bool ; for ( expanded pair pp : r . get pairs ( ) ) { if ( p . equals ( pp ) ) { found = bool ; break ; } } if ( ! found ) { all found = bool ; break ; } } if ( all found ) {	Returns true when one of the rows already contains all the pairs.
static result construct result ( list < expanded pair > pairs ) throws not found exception , format exception { bit array binary = bit array builder . build bit array ( pairs ) ; abstract expanded decoder decoder = abstract expanded decoder . create decoder ( binary ) ; string resulting string = decoder . parse information ( ) ; result point [ ] first points = pairs . get ( num ) . get finder pattern ( ) . get result points ( ) ; result point [ ] last points = pairs . get ( pairs . size ( ) - num ) . get finder pattern ( ) . get result points ( ) ; return new result ( resulting string , null , new result point [ ] { first points [ num ] , first points [ num ] , last points [ num ] , last points [ num ] } , barcode format . rss expanded ) ; }	Not private for unit testing.
private static int calculate mask penalty ( byte matrix matrix ) { return mask util . apply mask penalty rule1 ( matrix ) + mask util . apply mask penalty rule2 ( matrix ) + mask util . apply mask penalty rule3 ( matrix ) + mask util . apply mask penalty rule4 ( matrix ) ; }	Basically it applies four rules and summate all penalties.
private static version recommend version ( error correction level ec level , mode mode , bit array header bits , bit array data bits ) throws writer exception {	Decides the smallest version of QR code that will contain all of the provided data.
static void append length info ( int num letters , version version , mode mode , bit array bits ) throws writer exception { int num bits = mode . get character count bits ( version ) ; if ( num letters >= ( num << num bits ) ) { throw new writer exception ( num letters + str + ( ( num << num bits ) - num ) ) ; } bits . append bits ( num letters , num bits ) ; }	Append length info. On success, store the result in "bits".
@ override public product parsed result parse ( result result ) { barcode format format = result . get barcode format ( ) ; if ( ! ( format == barcode format . upc a || format == barcode format . upc e || format == barcode format . ean 8 || format == barcode format . ean 13 ) ) { return null ; } string raw text = get massaged text ( result ) ; if ( ! is string of digits ( raw text , raw text . length ( ) ) ) { return null ; }	Treat all UPC and EAN variants as UPCs, in the sense that they are all product barcodes.
private int [ ] determine dimensions ( int source code words , int error correction code words ) throws writer exception { float ratio = num ; int [ ] dimension = null ; for ( int cols = min cols ; cols <= max cols ; cols ++ ) { int rows = calculate number of rows ( source code words , error correction code words , cols ) ; if ( rows < min rows ) { break ; } if ( rows > max rows ) { continue ; } float new ratio = ( ( float ) ( num * cols + num ) * default module width ) / ( rows * height ) ;	Determine optimal nr of columns and rows for the specified number ofcodewords.
public void apply mirrored correction ( result point [ ] points ) { if ( ! mirrored || points == null || points . length < num ) { return ; } result point bottom left = points [ num ] ; points [ num ] = points [ num ] ; points [ num ] = bottom left ;	Apply the result points' order correction due to mirroring.
void set value ( int value ) { integer confidence = values . get ( value ) ; if ( confidence == null ) { confidence = num ; } confidence ++ ; values . put ( value , confidence ) ; }	Add an occurrence of a value.
int [ ] get value ( ) { int max confidence = - num ; collection < integer > result = new array list < > ( ) ; for ( entry < integer , integer > entry : values . entry set ( ) ) { if ( entry . get value ( ) > max confidence ) { max confidence = entry . get value ( ) ; result . clear ( ) ; result . add ( entry . get key ( ) ) ; } else if ( entry . get value ( ) == max confidence ) { result . add ( entry . get key ( ) ) ; } } return pd . to int array ( result ) ; }	Determines the maximum occurrence of a set value and returns all values which were set with this occurrence.
private collection < state > update state list for char ( iterable < state > states , int index ) { collection < state > result = new linked list < > ( ) ; for ( state state : states ) { update state for char ( state , index , result ) ; } return simplify states ( result ) ; }	non-optimal states.
private void update state for char ( state state , int index , collection < state > result ) { char ch = ( char ) ( text [ index ] & num ) ; boolean char in current table = char map [ state . get mode ( ) ] [ ch ] > num ; state state no binary = null ; for ( int mode = num ; mode <= mode punct ; mode ++ ) { int char in mode = char map [ mode ] [ ch ] ; if ( char in mode > num ) { if ( state no binary == null ) {	the "result" list.
@ override public char sequence get display contents ( ) { string contents = get result ( ) . get display result ( ) ; contents = contents . replace ( str , str ) ; return format phone ( contents ) ; }	Overriden so we can take advantage of Android's phone number hyphenation routines.
private void draw result points ( bitmap barcode , float scale factor , result raw result ) { result point [ ] points = raw result . get result points ( ) ; if ( points != null && points . length > num ) { canvas canvas = new canvas ( barcode ) ; paint paint = new paint ( ) ; paint . set color ( get resources ( ) . get color ( r . color . result points ) ) ; if ( points . length == num ) { paint . set stroke width ( num ) ; draw line ( canvas , paint , points [ num ] , points [ num ] , scale factor ) ; } else if ( points . length == num && ( raw result . get barcode format ( ) == barcode format . upc a || raw result . get barcode format ( ) == barcode format . ean 13 ) ) {	Superimpose a line for 1D or dots for 2D to highlight the key features of the barcode.
private result point [ ] center edges ( result point y , result point z , result point x , result point t ) {	recenters the points of a constant distance towards the center.
private boolean contains black point ( int a , int b , int fixed , boolean horizontal ) { if ( horizontal ) { for ( int x = a ; x <= b ; x ++ ) { if ( image . get ( x , fixed ) ) { return bool ; } } } else { for ( int y = a ; y <= b ; y ++ ) { if ( image . get ( fixed , y ) ) { return bool ; } } } return bool ; }	Determines whether a segment contains a black point.
private void add calendar event ( string summary , long start , boolean all day , long end , string location , string description , string [ ] attendees ) { intent intent = new intent ( intent . action insert ) ; intent . set type ( str ) ; intent . put extra ( str , start ) ; if ( all day ) { intent . put extra ( str , bool ) ; } if ( end < num ) { if ( all day ) {	Sends an intent to create a new calendar event by prepopulating the Add Event UI.
private static string get encoded data ( boolean [ ] corrected bits ) { int end index = corrected bits . length ; table latch table = table . upper ;	Gets the string encoded in the aztec code bits.
private static table get table ( char t ) { switch ( t ) { case str : return table . lower ; case str : return table . punct ; case str : return table . mixed ; case str : return table . digit ; case str : return table . binary ; case str : default : return table . upper ; } }	gets the table corresponding to the char passed.
private boolean [ ] extract bits ( bit matrix matrix ) { boolean compact = ddata . is compact ( ) ; int layers = ddata . get nb layers ( ) ; int base matrix size = ( compact ? num : num ) + layers * num ;	Gets the array of bits from an Aztec Code matrix.
private static int read code ( boolean [ ] rawbits , int start index , int length ) { int res = num ; for ( int i = start index ; i < start index + length ; i ++ ) { res <<= num ; if ( rawbits [ i ] ) { res |= num ; } } return res ; }	Reads a code of given length and at given index in an array of bits.
private static byte read byte ( boolean [ ] rawbits , int start index ) { int n = rawbits . length - start index ; if ( n >= num ) { return ( byte ) read code ( rawbits , start index , num ) ; } return ( byte ) ( read code ( rawbits , start index , n ) << ( num - n ) ) ; }	Reads a code of length 8 in an array of bits, padding with zeros.
static byte [ ] convert bool array to byte array ( boolean [ ] bool arr ) { byte [ ] byte arr = new byte [ ( bool arr . length + num ) / num ] ; for ( int i = num ; i < byte arr . length ; i ++ ) { byte arr [ i ] = read byte ( bool arr , num * i ) ; } return byte arr ; }	Packs a bit array into bytes, most significant bit first.
public static string convert upc upca ( string upce ) { char [ ] upce chars = new char [ num ] ; upce . get chars ( num , num , upce chars , num ) ; string builder result = new string builder ( num ) ; result . append ( upce . char at ( num ) ) ; char last char = upce chars [ num ] ; switch ( last char ) { case str : case str : case str : result . append ( upce chars , num , num ) ; result . append ( last char ) ; result . append ( str ) ; result . append ( upce chars , num , num ) ; break ; case str : result . append ( upce chars , num , num ) ; result . append ( str ) ; result . append ( upce chars , num , num ) ; break ; case str : result . append ( upce chars , num , num ) ; result . append ( str ) ; result . append ( upce chars [ num ] ) ; break ; default : result . append ( upce chars , num , num ) ; result . append ( str ) ; result . append ( last char ) ; break ; }	Expands a UPC-E value back into its full, equivalent UPC-A code value.
@ override public result decode ( binary bitmap image , map < decode hint type , ? > hints ) throws not found exception { set hints ( hints ) ; return decode internal ( image ) ; }	Decode an image using the hints provided.
private void utah ( int row , int col , int pos ) { module ( row - num , col - num , pos , num ) ; module ( row - num , col - num , pos , num ) ; module ( row - num , col - num , pos , num ) ; module ( row - num , col - num , pos , num ) ; module ( row - num , col , pos , num ) ; module ( row , col - num , pos , num ) ; module ( row , col - num , pos , num ) ; module ( row , col , pos , num ) ; }	Places the 8 bits of a utah-shaped symbol character in ECC200.
private static string massage uri ( string uri ) { uri = uri . trim ( ) ; int protocol end = uri . index of ( str ) ; if ( protocol end < num || is colon followed by port number ( uri , protocol end ) ) {	Transforms a string that represents a URI into something more proper, by adding or canonicalizingthe protocol.
public static simple page decorator first ( ) { list < simple page decorator > decorators = all ( ) ; return decorators . is empty ( ) ? null : decorators . get ( num ) ; }	The first found LoginDecarator, there can only be one.
protected map < computer , t > monitor ( ) throws interrupted exception { map < computer , t > data = new hash map < > ( ) ; for ( computer c : jenkins . get instance ( ) . get computers ( ) ) { try { thread . current thread ( ) . set name ( str + c . get display name ( ) + str + get display name ( ) ) ; if ( c . get channel ( ) == null ) data . put ( c , null ) ; else data . put ( c , monitor ( c ) ) ; } catch ( runtime exception | io e ) { logger . log ( level . warning , str + c . get display name ( ) + str + get display name ( ) , e ) ; } catch ( interrupted exception e ) { throw ( interrupted exception ) new interrupted exception ( str + c . get display name ( ) + str + get display name ( ) + str ) . init cause ( e ) ; } } return data ; }	Performs monitoring across the board.
public t get ( computer c ) { if ( record == null || ! record . data . contains key ( c ) ) {	Obtains the monitoring result currently available, or null if no data is available.
public boolean is ignored ( ) { node monitor m = computer set . get monitors ( ) . get ( this ) ; return m == null || m . is ignored ( ) ; }	Is this monitor currently ignored?.
protected boolean mark online ( computer c ) { if ( is ignored ( ) || c . is online ( ) ) return bool ;	Utility method to mark the computer online for derived classes.
protected boolean mark offline ( computer c , offline cause oc ) { if ( is ignored ( ) || c . is temporarily offline ( ) ) return bool ;	Utility method to mark the computer offline for derived classes.
public string get crumb ( servlet request request ) { string crumb = null ; if ( request != null ) { crumb = ( string ) request . get attribute ( crumb attribute ) ; } if ( crumb == null ) { crumb = issue crumb ( request , get descriptor ( ) . get crumb salt ( ) ) ; if ( request != null ) { if ( ( crumb != null ) && crumb . length ( ) > num ) { request . set attribute ( crumb attribute , crumb ) ; } else { request . remove attribute ( crumb attribute ) ; } } } return crumb ; }	Get a crumb value based on user specific information in the request.
public boolean validate crumb ( servlet request request ) { crumb issuer descriptor < crumb issuer > desc = get descriptor ( ) ; string crumb field = desc . get crumb request field ( ) ; string crumb salt = desc . get crumb salt ( ) ; return validate crumb ( request , crumb salt , request . get parameter ( crumb field ) ) ; }	Get a crumb from a request parameter and validate it against other datain the current request.
public boolean validate crumb ( servlet request request , multipart form data parser parser ) { crumb issuer descriptor < crumb issuer > desc = get descriptor ( ) ; string crumb field = desc . get crumb request field ( ) ; string crumb salt = desc . get crumb salt ( ) ; return validate crumb ( request , crumb salt , parser . get ( crumb field ) ) ; }	Get a crumb from multipart form data and validate it against other datain the current request.
@ initializer public static void init stapler crumb issuer ( ) { web app . get ( jenkins . get instance ( ) . servlet context ) . set crumb issuer ( new org . kohsuke . stapler . crumb issuer ( ) { @ override public string issue crumb ( stapler request request ) { crumb issuer ci = jenkins . get instance ( ) . get crumb issuer ( ) ; return ci != null ? ci . get crumb ( request ) : default . issue crumb ( request ) ; } @ override public void validate crumb ( stapler request request , string submitted crumb ) { crumb issuer ci = jenkins . get instance ( ) . get crumb issuer ( ) ; if ( ci == null ) { default . validate crumb ( request , submitted crumb ) ; } else { if ( ! ci . validate crumb ( request , ci . get descriptor ( ) . get crumb salt ( ) , submitted crumb ) ) throw new security exception ( str ) ; } } } ) ; }	Sets up Stapler to use our crumb issuer.
public string get public key ( ) { rsa key = instance identity provider . rsa . get public key ( ) ; if ( key == null ) { return null ; } byte [ ] encoded = base64 . encode base64 ( key . get encoded ( ) ) ; int index = num ; string builder buf = new string builder ( encoded . length + num ) ; while ( index < encoded . length ) { int len = math . min ( num , encoded . length - index ) ; if ( index > num ) { buf . append ( str ) ; } buf . append ( new string ( encoded , index , len , charsets . utf 8 ) ) ; index += len ; } return string . format ( str , buf . to string ( ) ) ; }	Returns the PEM encoded public key.
public string get fingerprint ( ) { rsa key = instance identity provider . rsa . get public key ( ) ; if ( key == null ) { return null ; }	Returns the fingerprint of the public key.
public void do json ( stapler request req , stapler response rsp ) throws io , servlet exception { if ( req . get parameter ( str ) == null || permit ( req ) ) { set headers ( rsp ) ; rsp . serve exposed bean ( req , bean , req . get parameter ( str ) == null ? flavor . json : flavor . jsonp ) ; } else { rsp . send error ( url . http forbidden , str ) ; } }	Exposes the bean as JSON.
public void do python ( stapler request req , stapler response rsp ) throws io , servlet exception { set headers ( rsp ) ; rsp . serve exposed bean ( req , bean , flavor . python ) ; }	Exposes the bean as Python literal.
@ restricted ( no external use . class ) public static cause action get build cause ( parameterized job job , stapler request req ) { cause cause ; @ suppress warnings ( str ) hudson . model . build authorization token auth token = job . get auth token ( ) ; if ( auth token != null && auth token . get token ( ) != null && req . get parameter ( str ) != null ) {	Computes the build cause, using RemoteCause or UserCause as appropriate.
public static @ check for null < t extends trigger < ? > > t get trigger ( job < ? , ? > job , class < t > clazz ) { if ( ! ( job instanceof parameterized job ) ) { return null ; } for ( trigger < ? > t : ( ( parameterized job < ? , ? > ) job ) . get triggers ( ) . values ( ) ) { if ( clazz . is instance ( t ) ) { return clazz . cast ( t ) ; } } return null ; }	Checks for the existence of a specific trigger on a job.
public synchronized string get ( ) { confidential store cs = confidential store . get ( ) ; if ( secret == null || cs != last cs ) { last cs = cs ; try { byte [ ] payload = load ( ) ; if ( payload == null ) { payload = cs . random bytes ( length / num ) ; store ( payload ) ; } secret = util . to hex string ( payload ) . substring ( num , length ) ; } catch ( io e ) { throw new error ( str + get id ( ) , e ) ; } } return secret ; }	Returns the persisted hex string value.If the value isn't persisted, a new random value is created.
public void replace by ( map < ? extends k , ? extends v > data ) { map < k , v > d = copy ( ) ; d . clear ( ) ; d . put all ( data ) ; update ( d ) ; }	Atomically replaces the entire map by the copy of the specified map.
public @ check for null file path get workspace root ( ) { file path r = get root path ( ) ; if ( r == null ) return null ; return r . child ( workspace root ) ; }	Root directory on this agent where all the job workspaces are laid out.
@ nonnull public launcher create launcher ( task listener listener ) { slave computer c = get computer ( ) ; if ( c == null ) { listener . error ( str + name + str ) ; return new launcher . dummy launcher ( listener ) ; } else {	Creates a launcher for the agent.
public static void skip ( data input stream in ) throws io { byte [ ] preamble = new byte [ preamble . length ] ; in . read fully ( preamble ) ; if ( ! arrays . equals ( preamble , preamble ) ) return ;	Skips the encoded console note.
public static int find preamble ( byte [ ] buf , int start , int len ) { int e = start + len - preamble . length + num ; outer : for ( int i = start ; i < e ; i ++ ) { if ( buf [ i ] == preamble [ num ] ) {	Locates the preamble in the given buffer.
public static list < string > remove notes ( collection < string > log lines ) { list < string > r = new array list < > ( log lines . size ( ) ) ; for ( string l : log lines ) r . add ( remove notes ( l ) ) ; return r ; }	Removes the embedded console notes in the given log lines.
public static string remove notes ( string line ) { while ( bool ) { int idx = line . index of ( preamble str ) ; if ( idx < num ) return line ; int e = line . index of ( postamble str , idx ) ; if ( e < num ) return line ; line = line . substring ( num , idx ) + line . substring ( e + postamble str . length ( ) ) ; } }	Removes the embedded console notes in the given log line.
protected string get display name of ( method e , t i ) { class < ? > c = e . get declaring class ( ) ; string key = display name of ( i ) ; if ( key . length ( ) == num ) return c . get simple name ( ) + str + e . get name ( ) ; try { resource bundle holder rb = resource bundle holder . get ( c . get class loader ( ) . load class ( c . get package ( ) . get name ( ) + str ) ) ; return rb . format ( key ) ; } catch ( class not found exception x ) { logger . log ( warning , str + x . get message ( ) + str + e . to string ( ) , x ) ; return key ; } catch ( missing resource exception x ) { logger . log ( warning , str + key + str + c . get package ( ) . get name ( ) + str , x ) ; return key ; } }	Obtains the display name of the given initialization task.
protected void invoke ( method e ) { try { class < ? > [ ] pt = e . get parameter types ( ) ; object [ ] args = new object [ pt . length ] ; for ( int i = num ; i < args . length ; i ++ ) args [ i ] = look up ( pt [ i ] ) ; e . invoke ( modifier . is static ( e . get modifiers ( ) ) ? null : look up ( e . get declaring class ( ) ) , args ) ; } catch ( illegal access exception x ) { throw ( error ) new illegal access error ( ) . init cause ( x ) ; } catch ( invocation target exception x ) { throw new error ( x ) ; } }	Invokes the given initialization method.
private object look up ( class < ? > type ) { jenkins j = jenkins . get instance ( ) ; assert j != null : str ; if ( type == jenkins . class || type == hudson . class ) return j ; injector i = j . get injector ( ) ; if ( i != null ) return i . get instance ( type ) ; throw new illegal argument exception ( str + type ) ; }	Determines the parameter injection of the initialization method.
@ nonnull public string get description ( ) { stapler stapler = stapler . get current ( ) ; if ( stapler != null ) { try { web app webapp = web app . get current ( ) ; meta class meta = webapp . get meta class ( this ) ; script s = meta . load tear off ( jelly class tear off . class ) . find script ( str ) ; if ( s == null ) { return str ; } default script invoker dsi = new default script invoker ( ) ; string writer sw = new string writer ( ) ; xml xml = dsi . create xml ( sw , bool ) ; dsi . invoke script ( stapler . get current request ( ) , stapler . get current response ( ) , s , this , xml ) ; return sw . to string ( ) ; } catch ( exception e ) { logger . log ( level . warning , null , e ) ; return str ; } } else { return str ; } }	A description of this kind of item type.
@ check for null @ deprecated public string get icon file path ( string size ) { if ( ! string utils . is blank ( get icon file path pattern ( ) ) ) { return get icon file path pattern ( ) . replace ( str , size ) ; } return null ; }	An icon file path associated to a specific size.
public maven installation get maven ( ) { for ( maven installation i : get descriptor ( ) . get installations ( ) ) { if ( maven name != null && maven name . equals ( i . get name ( ) ) ) return i ; } return null ; }	Gets the Maven to invoke,or null to invoke the default one.
protected void build env vars ( env vars env , maven installation mi ) throws io , interrupted exception { if ( mi != null ) {	Build up the environment variables toward the Maven launch.
public static boolean check is reachable ( inet address ia , int timeout ) throws io { for ( computer pinger pinger : computer pinger . all ( ) ) { try { if ( pinger . is reachable ( ia , timeout ) ) { return bool ; } } catch ( io e ) { logger . fine ( str + pinger + str + e . get message ( ) ) ; } } return bool ; }	Is this computer reachable via the given address?.
protected plugin strategy create plugin strategy ( ) { string strategy name = system properties . get string ( plugin strategy . class . get name ( ) ) ; if ( strategy name != null ) { try { class < ? > klazz = get class ( ) . get class loader ( ) . load class ( strategy name ) ; object strategy = klazz . get constructor ( plugin manager . class ) . new instance ( this ) ; if ( strategy instanceof plugin strategy ) { logger . info ( str + strategy name ) ; return ( plugin strategy ) strategy ; } else { logger . warning ( str + strategy name + str ) ; } } catch ( class not found exception e ) { logger . warning ( str + strategy name ) ; } catch ( exception e ) { logger . log ( warning , str + strategy name + str , e ) ; } logger . info ( str ) ; }	Creates a hudson.PluginStrategy, looking at the corresponding system property.
@ check for null public plugin wrapper get plugin ( string short name ) { for ( plugin wrapper p : get plugins ( ) ) { if ( p . get short name ( ) . equals ( short name ) ) return p ; } return null ; }	Get the plugin instance with the given short name.
public void stop ( ) { for ( plugin wrapper p : active plugins ) { p . stop ( ) ; p . release class loader ( ) ; } active plugins . clear ( ) ;	Orderly terminates all the plugins.
@ restricted ( do not use . class )	Get the list of all plugins - available and installed.
@ post @ restricted ( do not use . class )	Installs a list of plugins from a JSON POST.
@ post public http response do upload plugin ( stapler request req ) throws io , servlet exception { try { jenkins . get instance ( ) . check permission ( upload plugins ) ; servlet file upload upload = new servlet file upload ( new disk file item factory ( ) ) ;	Uploads a plugin.
public map < string , version number > parse requested plugins ( input stream config xml ) throws io { final map < string , version number > requested plugins = new tree map < > ( ) ; try { sax . new instance ( ) . new sax ( ) . parse ( config xml , new default handler ( ) { @ override public void start element ( string uri , string local name , string q name , attributes attributes ) throws sax { string plugin = attributes . get value ( str ) ; if ( plugin == null ) { return ; } if ( ! plugin . matches ( str ) ) { throw new sax ( str + plugin ) ; } int at = plugin . index of ( str ) ; string short name = plugin . substring ( num , at ) ; version number existing = requested plugins . get ( short name ) ; version number requested = new version number ( plugin . substring ( at + num ) ) ; if ( existing == null || existing . compare to ( requested ) < num ) { requested plugins . put ( short name , requested ) ; } } @ override public input source resolve entity ( string public id , string system id ) throws io , sax { return restrictive entity resolver . instance . resolve entity ( public id , system id ) ; } } ) ; } catch ( sax x ) { throw new io ( str , x ) ; } catch ( parser configuration exception e ) { throw new assertion error ( e ) ;	Parses configuration XML files and picks up references to XML files.
public j create chart ( category dataset ds ) { final j chart = chart factory . create line chart ( null ,	Creates a trend chart.
protected void update counts ( load statistics snapshot current ) { defined executors . update ( current . get defined executors ( ) ) ; online executors . update ( current . get online executors ( ) ) ; connecting executors . update ( current . get connecting executors ( ) ) ; busy executors . update ( current . get busy executors ( ) ) ; idle executors . update ( current . get idle executors ( ) ) ; available executors . update ( current . get available executors ( ) ) ; queue length . update ( current . get queue length ( ) ) ; }	Updates all the series from the current snapshot.
public load statistics snapshot compute snapshot ( ) { if ( modern ) { return compute snapshot ( jenkins . get instance ( ) . get queue ( ) . get buildable items ( ) ) ; } else { int t = compute total executors ( ) ; int i = compute idle executors ( ) ; return new load statistics snapshot ( t , t , math . max ( i - t , num ) , math . max ( t - i , num ) , i , i , compute queue length ( ) ) ; } }	Computes a self-consistent snapshot of the load statistics.Note: The original method of computing load statistics would compute the total and idle counts independentlywhich could lead to counting errors while jobs started in between the different state counting operations.By returning a {.
protected load statistics snapshot compute snapshot ( iterable < queue . buildable item > queue ) { final load statistics snapshot . builder builder = load statistics snapshot . builder ( ) ; final iterable < node > nodes = get nodes ( ) ; if ( nodes != null ) { for ( node node : nodes ) { builder . with ( node ) ; } } int q = num ; if ( queue != null ) { for ( queue . buildable item item : queue ) { for ( sub task st : item . task . get sub tasks ( ) ) { if ( matches ( item , st ) ) q ++ ; } } } return builder . with queue length ( q ) . build ( ) ; }	Computes the self-consistent snapshot with the specified queue items.
protected void eol ( byte [ ] in , int sz ) throws io { int next = console note . find preamble ( in , num , sz ) ;	Called after we read the whole line of plain text.
public final void process ( ) throws io , servlet exception { if ( permission != null ) try { if ( subject == null ) throw new access denied exception ( str ) ; subject . check permission ( permission ) ; } catch ( access denied exception e ) {	Runs the validation code.
protected final file get file parameter ( string param name ) { return new file ( util . fix null ( request . get parameter ( param name ) ) ) ; }	Gets the parameter as a file.
public void respond ( string html ) throws io , servlet exception { response . set content type ( str ) ; response . get writer ( ) . print ( html ) ; }	Sends out an arbitrary HTML fragment.
public void do png ( stapler request req , stapler response rsp ) throws io { if ( req . check if modified ( timestamp , rsp ) ) return ; try { buffered image image = render ( req , null ) ; rsp . set content type ( str ) ; servlet output stream os = rsp . get output stream ( ) ; io . write ( image , str , os ) ; os . close ( ) ; } catch ( error e ) { if ( e . get message ( ) . contains ( str ) ) { rsp . send redirect2 ( req . get context path ( ) + str ) ; return ; } throw e ;	Renders a graph.
public void do map ( stapler request req , stapler response rsp ) throws io { if ( req . check if modified ( timestamp , rsp ) ) return ; chart rendering info info = new chart rendering info ( ) ; render ( req , info ) ; rsp . set content type ( str ) ; rsp . get writer ( ) . println ( chart utilities . get image map ( str , info ) ) ; }	Renders a clickable map.
public long get initial delay ( ) { long l = random . next long ( ) ;	Gets the number of milliseconds til the first execution. By default it chooses the value randomly between 0 and {.
public void record cause of interruption ( run < ? , ? > build , task listener listener ) { list < cause of interruption > r ;	report cause of interruption and record it to the build, if available.
public @ check for null queue . executable get current executable ( ) { lock . read lock ( ) . lock ( ) ; try { return executable ; } finally { lock . read lock ( ) . unlock ( ) ; } }	Returns the current build this executor is running.
public @ check for null asynchronous execution get asynchronous execution ( ) { lock . read lock ( ) . lock ( ) ; try { return asynchronous execution ; } finally { lock . read lock ( ) . unlock ( ) ; } }	If currently running in asynchronous mode, returns that handle.
@ exported public int get progress ( ) { long d = executable estimated duration ; if ( d <= num ) { return default estimated duration ; } int num = ( int ) ( get elapsed time ( ) * num / d ) ; if ( num >= num ) { num = num ; } return num ; }	Returns the progress of the current build in the number between 0-100.
@ exported public boolean is likely stuck ( ) { lock . read lock ( ) . lock ( ) ; try { if ( executable == null ) { return bool ; } } finally { lock . read lock ( ) . unlock ( ) ; } long elapsed = get elapsed time ( ) ; long d = executable estimated duration ; if ( d >= num ) {	Returns true if the current build is likely stuck.
public long get time spent in queue ( ) { lock . read lock ( ) . lock ( ) ; try { return start time - work unit . context . item . buildable start milliseconds ; } finally { lock . read lock ( ) . unlock ( ) ; } }	Returns the number of milli-seconds the currently executing job spent in the queuewaiting for an available executor.
public string get estimated remaining time ( ) { long d = executable estimated duration ; if ( d < num ) { return messages . executor not available ( ) ; } long eta = d - get elapsed time ( ) ; if ( eta <= num ) { return messages . executor not available ( ) ; } return util . get time span string ( eta ) ; }	Computes a human-readable text that shows the expected remaining timeuntil the build completes.
@ post public http response do stop ( ) { lock . write lock ( ) . lock ( ) ;	Stops the current build.
public boolean has stop permission ( ) { lock . read lock ( ) . lock ( ) ; try { return executable != null && get parent of ( executable ) . get owner task ( ) . has abort permission ( ) ; } finally { lock . read lock ( ) . unlock ( ) ; } }	Checks if the current user has a permission to stop this build.
public long get idle start milliseconds ( ) { if ( is idle ( ) ) return math . max ( creation time , owner . get connect time ( ) ) ; else { return math . max ( start time + math . max ( num , executable estimated duration ) , system . current time millis ( ) + num ) ; } }	Returns when this executor started or should start being idle.
public < t > t new impersonating proxy ( class < t > type , t core ) { return new intercepting proxy ( ) { protected object call ( object o , method m , object [ ] args ) throws throwable { final executor old = impersonation . get ( ) ; impersonation . set ( executor . this ) ; try { return m . invoke ( o , args ) ; } finally { impersonation . set ( old ) ; } } } . wrap ( type , core ) ; }	Creates a proxy object that executes the callee in the context that impersonatesthis executor.
public static @ check for null executor current executor ( ) { thread t = thread . current thread ( ) ; if ( t instanceof executor ) return ( executor ) t ; return impersonation . get ( ) ; }	Returns the executor of the current thread or null if current thread is not an executor.
@ check for null public static executor of ( executable executable ) { jenkins jenkins = jenkins . get instance or null ( ) ;	Finds the executor currently running a given process.
public search result get suggestions ( stapler request req , string query ) { set < string > paths = new hash set < > ( ) ;	Gets the list of suggestions that match the given query.
private search index make suggest index ( stapler request req ) { search index builder builder = new search index builder ( ) ; for ( ancestor a : req . get ancestors ( ) ) { if ( a . get object ( ) instanceof searchable model object ) { searchable model object smo = ( searchable model object ) a . get object ( ) ; builder . add ( smo . get search index ( ) ) ; } } return builder . make ( ) ; }	Creates merged search index for suggestion.
static suggested item find closest suggested item ( list < suggested item > r , string query ) { for ( suggested item cur item : r ) { if ( logger . is loggable ( level . fine ) ) { logger . fine ( string . format ( str , cur item . item . get search url ( ) , query ) ) ; } if ( cur item . item . get search url ( ) . contains ( util . raw encode ( query ) ) ) { return cur item ; } }	When there are multiple suggested items, this method can narrow down the resultsetto the SuggestedItem that has a url that contains the query.
public static suggested item find ( search index index , string query , searchable model object search context ) { list < suggested item > r = find ( mode . find , index , query , search context ) ; if ( r . is empty ( ) ) { return null ; } else if ( num == r . size ( ) ) { return r . get ( num ) ; } else {	Performs a search and returns the match, or null if no match was foundor more than one match was found.
private input stream transform for windows ( input stream src ) throws io { buffered reader r = new buffered reader ( new input stream reader ( src ) ) ; byte array output stream out = new byte array output stream ( ) ; try ( print stream p = new print stream ( out ) ) { string line ; while ( ( line = r . read line ( ) ) != null ) { if ( ! line . starts with ( str ) && functions . is windows ( ) ) line = line . replace ( str , str ) ; p . println ( line ) ; } } return new byte array input stream ( out . to byte array ( ) ) ; }	Transform path for Windows.
@ post public http response do approve all ( ) throws io { string builder buf = new string builder ( ) ; for ( class c : rejected . get ( ) ) { buf . append ( c . get name ( ) ) . append ( str ) ; } whitelisted . append ( buf . to string ( ) ) ; return http responses . ok ( ) ; }	Approves all the currently rejected subjects.
public void set crumb salt ( string salt ) { if ( util . fix empty and trim ( salt ) == null ) { crumb salt = str ; } else { crumb salt = salt ; } }	Set the salt value.
public void set crumb request field ( string request field ) { if ( util . fix empty and trim ( request field ) == null ) { crumb request field = crumb issuer . default crumb name ; } else { crumb request field = request field ; } }	Set the request parameter name.
public string sign ( string msg ) { try { rsa key = get private key ( ) ; signature sig = signature . get instance ( signing algorithm + str + key . get algorithm ( ) ) ; sig . init sign ( key ) ; sig . update ( msg . get bytes ( standard charsets . utf 8 ) ) ; return hudson . remoting . base64 . encode ( sig . sign ( ) ) ; } catch ( general security exception e ) { throw new security exception ( e ) ; } }	Sign a message and base64 encode the signature.
public static void drain ( input stream in ) throws io { org . apache . commons . io . io . copy ( in , new null stream ( ) ) ; in . close ( ) ; }	Drains the input stream and closes it.
public static input stream skip ( input stream in , long size ) throws io { data input stream di = new data input stream ( in ) ; while ( size > num ) { int chunk = ( int ) math . min ( skip buffer . length , size ) ; di . read fully ( skip buffer , num , chunk ) ; size -= chunk ; } return in ; }	Fully skips the specified size from the given input stream. {.
public static string read first line ( input stream is , string encoding ) throws io { try ( buffered reader reader = new buffered reader ( encoding == null ? new input stream reader ( is ) : new input stream reader ( is , encoding ) ) ) { return reader . read line ( ) ; } }	Read the first line of the given stream, close it, and return that line.
private static void transform ( source source , result out ) throws transformer exception { transformer factory factory = transformer factory . new instance ( ) ; factory . set feature ( xml . feature secure processing , bool ) ;	potentially unsafe XML transformation.
public void calc fill settings ( string field , map < string , object > attributes ) { string capitalized field name = string utils . capitalize ( field ) ; string method name = str + capitalized field name + str ; method method = reflection utils . get public method named ( get class ( ) , method name ) ; if ( method == null ) throw new illegal state exception ( string . format ( str , get class ( ) , method name ) ) ;	Computes the list of other form fields that the given field depends on, via the doFillXyzItems method,and sets that as the 'fillDependsOn' attribute.
public void calc auto complete settings ( string field , map < string , object > attributes ) { string capitalized field name = string utils . capitalize ( field ) ; string method name = str + capitalized field name ; method method = reflection utils . get public method named ( get class ( ) , method name ) ; if ( method == null ) return ;	Computes the auto-completion setting.
public property type get global property type ( string field ) { if ( global property types == null ) global property types = build property types ( get class ( ) ) ; return global property types . get ( field ) ; }	Obtains the property type of the given field of this descriptor.
protected void add help file redirect ( string field name , class < ? extends describable > owner , string field name to redirect to ) { help redirect . put ( field name , new help redirect ( owner , field name to redirect to ) ) ; }	Tells Jenkins that the help file for the field 'fieldName' is defined in the help file forthe 'fieldNameToRedirectTo' in the 'owner' class.
public static @ check for null < t extends descriptor > t find by id ( collection < ? extends t > list , string id ) { for ( t d : list ) { if ( d . get id ( ) . equals ( id ) ) return d ; } return null ; }	Finds a descriptor from a collection by its ID.
public static @ check for null < t extends descriptor > t find ( collection < ? extends t > list , string string ) { t d = find by class name ( list , string ) ; if ( d != null ) { return d ; } return find by id ( list , string ) ; }	Finds a descriptor from a collection by its class name or ID.
private url try to resolve redirects ( url base , string authorization ) { try { url con = ( url ) base . open connection ( ) ; if ( authorization != null ) { con . add request property ( str , authorization ) ; } con . get input stream ( ) . close ( ) ; base = con . get url ( ) ; } catch ( exception ex ) {	As this transport mode is using POST, it is necessary to resolve possible redirections using GET first.
protected final @ nonnull result < t > monitor detailed ( ) throws interrupted exception { map < computer , future < t > > futures = new hash map < > ( ) ; set < computer > skipped = new hash set < > ( ) ; for ( computer c : jenkins . get instance ( ) . get computers ( ) ) { try { virtual channel ch = c . get channel ( ) ; futures . put ( c , null ) ;	Perform monitoring with detailed reporting.
private static url open ( url url ) throws io { url c = ( url ) url . open connection ( ) ; c . set read timeout ( timeout ) ; c . set connect timeout ( timeout ) ; return c ; }	Connects to the given HTTP URL and configure time out, to avoid infinite hang.
public boolean should display ( ) throws io , servlet exception { if ( ! functions . has permission ( jenkins . administer ) ) { return bool ; } stapler request req = stapler . get current request ( ) ; if ( req == null ) { return bool ; } list < ancestor > ancestors = req . get ancestors ( ) ; if ( ancestors == null || ancestors . size ( ) == num ) {	Whether the administrative monitors notifier should be shown.
@ nonnull public static synchronized scheduled executor service get ( ) { if ( executor service == null ) {	Returns the scheduled executor service used by all timed tasks in Jenkins.
@ override protected void on unsuccessful authentication ( http servlet request request , http servlet response response , authentication exception failed ) throws io { super . on unsuccessful authentication ( request , response , failed ) ; logger . log ( level . fine , str , failed ) ; authentication auth = failed . get authentication ( ) ; if ( auth != null ) { security listener . fire failed to log in ( auth . get name ( ) ) ; } }	Leave the information about login failure.
public static boolean has filter ( filter filter ) { jenkins j = jenkins . get instance or null ( ) ; plugin servlet filter container = null ; if ( j != null ) { container = get instance ( j . servlet context ) ; } if ( j == null || container == null ) { return legacy . contains ( filter ) ; } else { return container . list . contains ( filter ) ; } }	Checks whether the given filter is already registered in the chain.
protected void update computer list ( final boolean automatic slave launch ) { final map < node , computer > computers = get computer map ( ) ; final set < computer > old = new hash set < computer > ( computers . size ( ) ) ; queue . with lock ( new runnable ( ) { @ override public void run ( ) { map < string , computer > by name = new hash map < string , computer > ( ) ; for ( computer c : computers . values ( ) ) { old . add ( c ) ; node node = c . get node ( ) ; if ( node == null ) continue ;	Updates Computers. This method tries to reuse existing {.
public static form validation error ( throwable e , string message ) { return error ( kind . error , e , message ) ; }	Sends out a string error message, with optional "show details" link that expands to the full stack trace.
public static form validation validate executable ( string exe , file validator exe validator ) {	Makes sure that the given string points to an executable file.
public static form validation validate non negative integer ( string value ) { try { if ( integer . parse int ( value ) < num ) return error ( hudson . model . messages . a ( ) ) ; return ok ( ) ; } catch ( number format exception e ) { return error ( hudson . model . messages . a ( ) ) ; } }	Makes sure that the given string is a non-negative integer.
public static form validation validate positive integer ( string value ) { try { if ( integer . parse int ( value ) <= num ) return error ( hudson . model . messages . a ( ) ) ; return ok ( ) ; } catch ( number format exception e ) { return error ( hudson . model . messages . a ( ) ) ; } }	Makes sure that the given string is a positive integer.
public static form validation validate required ( string value ) { if ( util . fix empty and trim ( value ) == null ) return error ( messages . form validation validate required ( ) ) ; return ok ( ) ; }	Makes sure that the given string is not null or empty.
public static form validation validate base64 ( string value , boolean allow whitespace , boolean allow empty , string error message ) { try { string v = value ; if ( ! allow whitespace ) { if ( v . index of ( str ) >= num || v . index of ( str ) >= num ) return error ( error message ) ; } v = v . trim ( ) ; if ( ! allow empty && v . length ( ) == num ) return error ( error message ) ; base64 . get decoder ( ) . decode ( v . get bytes ( standard charsets . utf 8 ) ) ; return ok ( ) ; } catch ( illegal argument exception e ) { return error ( error message ) ; } }	Makes sure that the given string is a base64 encoded text.
public synchronized byte [ ] mac ( byte [ ] message ) { confidential store cs = confidential store . get ( ) ; if ( mac == null || cs != last cs ) { last cs = cs ; mac = create mac ( ) ; } return chop ( mac . do final ( message ) ) ; }	Computes the message authentication code for the specified byte sequence.
public string mac ( string message ) { return util . to hex string ( mac ( message . get bytes ( standard charsets . utf 8 ) ) ) ; }	Computes the message authentication code and return it as a string.While redundant, often convenient.
public < u extends t > list < u > get all ( class < u > type ) { list < u > r = new array list < > ( ) ; for ( t t : data ) if ( type . is instance ( t ) ) r . add ( type . cast ( t ) ) ; return r ; }	Gets all instances that matches the given type.
public void remove ( class < ? extends t > type ) throws io { for ( t t : data ) { if ( t . get class ( ) == type ) { data . remove ( t ) ; on modified ( ) ; return ; } } }	Removes an instance by its type.
public void replace ( t from , t to ) throws io { list < t > copy = new array list < > ( data . get view ( ) ) ; for ( int i = num ; i < copy . size ( ) ; i ++ ) { if ( copy . get ( i ) . equals ( from ) ) copy . set ( i , to ) ; } data . replace by ( copy ) ; }	A convenience method to replace a single item.This method shouldn't be used when you are replacing a lot of stuffas copy-on-write semantics make this rather slow.
private run list < r > limit ( final counting predicate < r > predicate ) { size = null ; first = null ; final iterable < r > nested = base ; base = new iterable < r > ( ) { public iterator < r > iterator ( ) { return hudson . util . iterators . limit ( nested . iterator ( ) , predicate ) ; } @ override public string to string ( ) { return iterables . to string ( this ) ; } } ; return this ; }	Returns the first streak of the elements that satisfy the given predicate.For example, {.
public run list < r > by timestamp ( final long start , final long end ) { return limit ( new counting predicate < r > ( ) { public boolean apply ( int index , r r ) { return start <= r . get time in millis ( ) ; } } ) . filter ( new predicate < r > ( ) { public boolean apply ( r r ) { return r . get time in millis ( ) < end ; } } ) ; }	Filter the list by timestamp.{.
@ override public void rewrite hudson war ( file by ) throws io { file dest = get hudson war ( ) ;	On Windows, jenkins.war is locked, so we place a new version under a special name,which is picked up by the service wrapper upon restart.
public static boolean is all ready ( ) throws io , interrupted exception { for ( restart listener listener : all ( ) ) { if ( ! listener . is ready to restart ( ) ) return bool ; } return bool ; }	Returns true iff all the listeners OKed the restart.
protected boolean is ignored dir ( file dir ) {	Decides if this directory is worth visiting or not.
@ override public long skip ( long n ) throws io { byte [ ] buf = new byte [ ( int ) math . min ( n , num * num ) ] ; return read ( buf , num , buf . length ) ; }	To record the bytes we've skipped, convert the call to read.
public static list < computer panel box > all ( computer computer ) { list < computer panel box > boxs = new array list < > ( ) ; for ( computer panel box box : extension list . lookup ( computer panel box . class ) ) { box . set computer ( computer ) ; boxs . add ( box ) ; } return boxs ; }	Create boxes for the given computer in its page.
public static string [ ] intern in place ( string [ ] input ) { if ( input == null ) { return null ; } else if ( input . length == num ) { return empty string array ; } for ( int i = num ; i < input . length ; i ++ ) { input [ i ] = util . intern ( input [ i ] ) ; } return input ; }	Returns the input strings, but with all values interned.
public final polling result poll ( abstract project < ? , ? > project , launcher launcher , file path workspace , task listener listener , scm baseline ) throws io , interrupted exception { if ( is1 346 or later ( ) ) {	Convenience method for the caller to handle the backward compatibility between pre 1.345 SCMs.
public static list < scm < ? > > for ( @ check for null final job project ) { if ( project == null ) return all ( ) ; final descriptor pd = jenkins . get instance ( ) . get descriptor ( ( class ) project . get class ( ) ) ; list < scm < ? > > r = new array list < scm < ? > > ( ) ; for ( scm < ? > scm descriptor : all ( ) ) { if ( ! scm descriptor . is applicable ( project ) ) continue ; if ( pd instanceof top level item descriptor ) { top level item descriptor apd = ( top level item descriptor ) pd ; if ( ! apd . is applicable ( scm descriptor ) ) continue ; } r . add ( scm descriptor ) ; } return r ; }	Determines which kinds of SCMs are applicable to a given project.
private boolean has permission to see token ( ) {	Only for legacy token.
@ deprecated public void change api token ( ) throws io {	Only usable if the user still has the legacy API token.
public void run ( action [ ] additional actions ) { if ( job == null ) { return ; } descriptor impl d = get descriptor ( ) ; logger . fine ( str + job ) ; if ( d . synchronous polling ) { logger . fine ( str + str ) ; new runner ( additional actions ) . run ( ) ; } else {	Run the SCM trigger with additional build actions.
public static boolean execute ( abstract build build , build listener listener ) { print stream logger = listener . get logger ( ) ;	Convenience method to trigger downstream builds.
public boolean can run ( final resource list resources ) { try { return with lock ( new callable < boolean > ( ) { @ override public boolean call ( ) { return ! in use . is colliding with ( resources ) ; } } ) ; } catch ( exception e ) { throw new illegal state exception ( str ) ; } }	Checks if an activity that requires the given resource listcan run immediately. This method is really only useful as a hint, sinceanother activity might acquire resources before the callergets to call {.
public resource get missing resource ( final resource list resources ) { try { return with lock ( new callable < resource > ( ) { @ override public resource call ( ) { return resources . get conflict ( in use ) ; } } ) ; } catch ( exception e ) { throw new illegal state exception ( str ) ; } }	Of the resource in the given resource list, return the one that'scurrently in use.
@ override public void add ( top level item item ) throws io { synchronized ( this ) { job names . add ( item . get relative name from ( get owner ( ) . get item group ( ) ) ) ; } save ( ) ; }	Adds the given item to this view.
@ override public boolean remove ( top level item item ) throws io { synchronized ( this ) { string name = item . get relative name from ( get owner ( ) . get item group ( ) ) ; if ( ! job names . remove ( name ) ) return bool ; } save ( ) ; return bool ; }	Removes given item from this view.
@ restricted ( no external use . class )	Determines the initial state of the checkbox.
@ override public void on online ( computer c , task listener listener ) throws io , interrupted exception { synchronized ( this ) { future . cancel ( bool ) ; future = timer . get ( ) . schedule ( monitor updater , num , time unit . seconds ) ; } }	Triggers the update with 5 seconds quiet period, to avoid triggering data check too oftenwhen multiple agents become online at about the same time.
@ exported ( name = str ) public string get assigned label string ( ) { if ( can roam || assigned node == null ) return null ; try { label expression . parse expression ( assigned node ) ; return assigned node ; } catch ( antlr e ) {	Gets the textual representation of the assigned label as it was entered by the user.
public void set assigned label ( label l ) throws io { if ( l == null ) { can roam = bool ; assigned node = null ; } else { can roam = bool ; if ( l == jenkins . get instance ( ) . get self label ( ) ) assigned node = null ; else assigned node = l . get expression ( ) ; } save ( ) ; }	Sets the assigned label.
private abstract build get build for deprecated methods ( ) { executor e = executor . current executor ( ) ; if ( e != null ) { executable exe = e . get current executable ( ) ; if ( exe instanceof abstract build ) { abstract build b = ( abstract build ) exe ; if ( b . get project ( ) == this ) return b ; } } r lb = get last build ( ) ; if ( lb != null ) return lb ; return null ; }	Various deprecated methods in this class all need the 'current' build.
public final @ check for null file path get some workspace ( ) { r b = get some build with workspace ( ) ; if ( b != null ) return b . get workspace ( ) ; for ( workspace browser browser : extension list . lookup ( workspace browser . class ) ) { file path f = browser . get workspace ( this ) ; if ( f != null ) return f ; } return null ; }	Gets a workspace for some build of this project.
public final r get some build with workspace ( ) { int cnt = num ; for ( r b = get last build ( ) ; cnt < num && b != null ; b = b . get previous build ( ) ) { file path ws = b . get workspace ( ) ; if ( ws != null ) return b ; } return null ; }	Gets some build that has a live workspace.
public boolean schedule build ( int quiet period , cause c , action ... actions ) { return schedule build2 ( quiet period , c , actions ) != null ; }	Schedules a build.Important: the actions should be persistable without outside references (e.g.
public boolean schedule polling ( ) { if ( is disabled ( ) ) return bool ; scm scmt = get trigger ( scm . class ) ; if ( scmt == null ) return bool ; scmt . run ( ) ; return bool ; }	Schedules a polling of this project.
public resource list get resource list ( ) { final set < resource activity > resource activities = get resource activities ( ) ; final list < resource list > resource lists = new array list < resource list > ( num + resource activities . size ( ) ) ; for ( resource activity activity : resource activities ) { if ( activity != this && activity != null ) {	List of necessary resources to perform the build of this project.
private void calc polling baseline ( abstract build build , launcher launcher , task listener listener ) throws io , interrupted exception { scm baseline = build . get action ( scm . class ) ; if ( baseline == null ) { try { baseline = get scm ( ) . calc revisions from build ( build , launcher , listener ) ; } catch ( abstract method error e ) { baseline = scm . none ;	Pushes the baseline up to the newly checked out revision.
public polling result poll ( task listener listener ) { scm scm = get scm ( ) ; if ( scm == null ) { listener . get logger ( ) . println ( messages . scm ( ) ) ; return no changes ; } if ( ! is buildable ( ) ) { listener . get logger ( ) . println ( messages . abstract project disabled ( ) ) ; return no changes ; } scm veto = scm . first should poll veto ( this ) ; if ( veto != null ) { listener . get logger ( ) . println ( messages . abstract project polling vetoed ( veto ) ) ; return no changes ; } r lb = get last build ( ) ; if ( lb == null ) { listener . get logger ( ) . println ( messages . abstract project no builds ( ) ) ; return is in queue ( ) ? no changes : build now ; } if ( polling baseline == null ) { r success = get last successful build ( ) ;	Checks if there's any update in SCM, and returns true if any is found.
private boolean is all suitable nodes offline ( r build ) { label label = get assigned label ( ) ; list < node > all nodes = jenkins . get instance ( ) . get nodes ( ) ; if ( label != null ) {	Returns true if all suitable nodes for the job are offline.
public boolean has participant ( user user ) { for ( r build = get last build ( ) ; build != null ; build = build . get previous build ( ) ) if ( build . has participant ( user ) ) return bool ; return bool ; }	Returns true if this user has made a commit to this project.
public < t extends trigger > t get trigger ( class < t > clazz ) { for ( trigger p : triggers ( ) ) { if ( clazz . is instance ( p ) ) return clazz . cast ( p ) ; } return null ; }	Gets the specific trigger, or null if the property is not configured for this job.
private void check and record ( abstract project that , tree map < integer , range set > r , collection < r > builds ) { for ( r build : builds ) { range set rs = build . get downstream relationship ( that ) ; if ( rs == null || rs . is empty ( ) ) continue ; int n = build . get number ( ) ; range set value = r . get ( n ) ; if ( value == null ) r . put ( n , rs ) ; else value . add ( rs ) ; } }	Helper method for getDownstreamRelationship.For each given build, find the build number range of the given project and put that into the map.
@ deprecated public int get delay ( stapler request req ) throws servlet exception { string delay = req . get parameter ( str ) ; if ( delay == null ) return get quiet period ( ) ; try {	Computes the delay by taking the default value and the override in the request parameter into the account.
public directory browser support do ws ( stapler request req , stapler response rsp ) throws io , servlet exception , interrupted exception { check permission ( item . workspace ) ; file path ws = get some workspace ( ) ; if ( ( ws == null ) || ( ! ws . exists ( ) ) ) {	Serves the workspace files.
@ post public http response do do wipe out workspace ( ) throws io , servlet exception , interrupted exception { check permission ( functions . is wipe out permission enabled ( ) ? wipeout : build ) ; r b = get some build with workspace ( ) ; file path ws = b != null ? b . get workspace ( ) : null ; if ( ws != null && get scm ( ) . process workspace before deletion ( this , ws , b . get built on ( ) ) ) { ws . delete recursive ( ) ; for ( workspace listener wl : workspace listener . all ( ) ) { wl . after delete ( this ) ; } return new http redirect ( str ) ; } else {	Wipes out the workspace.
static string paren ( label operator precedence op , label l ) { if ( op . compare to ( l . precedence ( ) ) < num ) return str + l . get expression ( ) + str ; return l . get expression ( ) ; }	Puts the label name into a parenthesis if the given operator will have a higher precedence.
public static list < integer > sequence ( final int start , int end , final int step ) { final int size = ( end - start ) / step ; if ( size < num ) throw new illegal argument exception ( str ) ; return new abstract list < integer > ( ) { public integer get ( int index ) { if ( index < num || index >= size ) throw new index out of bounds exception ( ) ; return start + index * step ; } public int size ( ) { return size ; } } ; }	Returns a list that represents [start,end).For example sequence(1,5,1)={1,2,3,4}, and sequence(7,1,-2)={7.5,3}.
public static < t > iterator < t > remove null ( final iterator < t > itr ) { return com . google . common . collect . iterators . filter ( itr , predicates . not null ( ) ) ; }	Wraps another iterator and throws away nulls.
public static < t > iterator < t > limit ( final iterator < ? extends t > base , final counting predicate < ? super t > filter ) { return new iterator < t > ( ) { private t next ; private boolean end ; private int index = num ; public boolean has next ( ) { fetch ( ) ; return next != null ; } public t next ( ) { fetch ( ) ; t r = next ; next = null ; return r ; } private void fetch ( ) { if ( next == null && ! end ) { if ( base . has next ( ) ) { next = base . next ( ) ; if ( ! filter . apply ( index ++ , next ) ) { next = null ; end = bool ; } } else { end = bool ; } } } public void remove ( ) { throw new unsupported operation exception ( ) ; } } ; }	Returns the elements in the base iterator until it hits any element that doesn't satisfy the filter.Then the rest of the elements in the base iterator gets ignored.
public static init strategy get ( class loader cl ) throws io { iterator < init strategy > it = service loader . load ( init strategy . class , cl ) . iterator ( ) ; if ( ! it . has next ( ) ) { return new init strategy ( ) ;	Obtains the instance to be used.
public void override ( string key , string value ) { if ( value == null || value . length ( ) == num ) { remove ( key ) ; return ; } int idx = key . index of ( str ) ; if ( idx > num ) { string real key = key . substring ( num , idx ) ; string v = get ( real key ) ; if ( v == null ) v = value ; else {	Overrides the current entry by the given entry. Handles {.
public static void resolve ( map < string , string > env ) { for ( map . entry < string , string > entry : env . entry set ( ) ) { entry . set value ( util . replace macro ( entry . get value ( ) , env ) ) ; } }	Resolves environment variables against each other.
public void add line ( string line ) { int sep = line . index of ( str ) ; if ( sep > num ) { put ( line . substring ( num , sep ) , line . substring ( sep + num ) ) ; } }	Takes a string that looks like "a=b" and adds that to this map.
public static env vars get remote ( virtual channel channel ) throws io , interrupted exception { if ( channel == null ) return new env vars ( str , str ) ; return channel . call ( new get env vars ( ) ) ; }	Obtains the environment variables of a remote peer.
public @ nonnull acl get acl ( final @ nonnull view item ) { return acl . lambda ( ( a , permission ) -> { acl base = item . get owner ( ) . get acl ( ) ; boolean has permission = base . has permission ( a , permission ) ; if ( ! has permission && permission == view . read ) { return base . has permission ( a , view . configure ) || ! item . get items ( ) . is empty ( ) ; } return has permission ; } ) ; }	Implementation can choose to provide different ACL for different views.This can be used as a basis for more fine-grained access control.
@ override @ post public http response do do delete ( ) throws io { check permission ( delete ) ; try { t node = get node ( ) ; if ( node != null ) {	When the agent is deleted, free the node right away.
public synchronized boolean remove ( e e ) { list < e > n = new array list < > ( core ) ; boolean r = n . remove ( e ) ; core = n ; return r ; }	Removes an item from the list.
public iterator < e > iterator ( ) { final iterator < ? extends e > itr = core . iterator ( ) ; return new iterator < e > ( ) { private e last ; public boolean has next ( ) { return itr . has next ( ) ; } public e next ( ) { return last = itr . next ( ) ; } public void remove ( ) { copy on write list . this . remove ( last ) ; } } ; }	Returns an iterator.
public static file get logs root ( ) { string tags logs path = system properties . get string ( logs root path property ) ; if ( tags logs path == null ) { return new file ( jenkins . get ( ) . get root dir ( ) , str ) ; } else { level log level = level . info ; if ( already logged ) { log level = level . fine ; } logger . log ( log level , str , logs root path property ) ; already logged = bool ; return new file ( tags logs path ) ; } }	The root path that should be used to put logs related to the tasks running in Jenkins.
private < r > collection < r > copy ( iterable < r > src ) { return lists . new array list ( src ) ; }	Creates a copy since we'll be deleting some entries from them.
public void close entry ( ) throws io { if ( this . assem len > num ) { for ( int i = this . assem len ; i < this . assem buf . length ; ++ i ) { this . assem buf [ i ] = num ; } this . buffer . write record ( this . assem buf ) ; this . curr bytes += this . assem len ; this . assem len = num ; } if ( this . curr bytes < this . curr size ) { throw new io ( str + curr name + str + this . curr bytes + str + this . curr size + str ) ; } }	Close an entry. This method MUST be called for all fileentries that contain data. The reason is that we mustbuffer data written to the stream in order to satisfythe buffer's record based writes. Thus, there may bedata fragments still being assembled that must be writtento the output stream before this entry is closed and thenext entry written.
public void set full name ( string name ) { if ( util . fix empty and trim ( name ) == null ) name = id ; this . full name = name ; }	Sets the human readable name of the user.If the input parameter is empty, the user's ID will be set.
public < t extends user property > t get property ( class < t > clazz ) { for ( user property p : properties ) { if ( clazz . is instance ( p ) ) return clazz . cast ( p ) ; } return null ; }	Gets the specific property, or null.
public static @ nonnull collection < user > get all ( ) { final id strategy strategy = id strategy ( ) ; array list < user > users = new array list < > ( all users . values ( ) ) ; users . sort ( ( o1 , o2 ) -> strategy . compare ( o1 . get id ( ) , o2 . get id ( ) ) ) ; return users ; }	Gets all the users.
@ deprecated public static void clear ( ) { if ( extension list . lookup ( all users . class ) . is empty ( ) ) { return ; } user id mapper . get instance ( ) . clear ( ) ; all users . clear ( ) ; }	Called by tests in the JTH.
public synchronized void save ( ) throws io { if ( ! is id or fullname allowed ( id ) ) { throw form validation . error ( messages . user illegal username ( id ) ) ; } if ( ! is id or fullname allowed ( full name ) ) { throw form validation . error ( messages . user illegal fullname ( full name ) ) ; } if ( bulk change . contains ( this ) ) { return ; } xml file xml file = new xml file ( xstream , construct user config file ( ) ) ; xml file . write ( this ) ; saveable listener . fire on change ( this , xml file ) ; }	Save the user configuration.
public void delete ( ) throws io { string id key = id strategy ( ) . key for ( id ) ; file existing user folder = get existing user folder ( ) ; user id mapper . get instance ( ) . remove ( id ) ; all users . remove ( id ) ; delete existing user folder ( existing user folder ) ; user details cache . get ( ) . invalidate ( id key ) ; }	Deletes the data directory and removes this user from Hudson.
@ post public void do do delete ( stapler request req , stapler response rsp ) throws io { check permission ( jenkins . administer ) ; if ( id strategy ( ) . equals ( id , jenkins . get authentication ( ) . get name ( ) ) ) { rsp . send error ( http servlet response . sc bad request , str ) ; return ; } delete ( ) ; rsp . send redirect2 ( str ) ; }	Deletes this user from Hudson.
public boolean can delete ( ) { final id strategy strategy = id strategy ( ) ; return has permission ( jenkins . administer ) && ! strategy . equals ( id , jenkins . get authentication ( ) . get name ( ) ) && user id mapper . get instance ( ) . is mapped ( id ) ; }	With ADMINISTER permission, can delete users with persisted data but can't delete self.
public list < action > get property actions ( ) { list < action > actions = new array list < > ( ) ; for ( user property user prop : get properties ( ) . values ( ) ) { if ( user prop instanceof action ) { actions . add ( ( action ) user prop ) ; } } return collections . unmodifiable list ( actions ) ; }	Return all properties that are also actions.
public list < action > get transient actions ( ) { list < action > actions = new array list < > ( ) ; for ( transient user action factory factory : transient user action factory . all ( ) ) { actions . add all ( factory . create for ( this ) ) ; } return collections . unmodifiable list ( actions ) ; }	Return all transient actions associated with this user.
public boolean is colliding with ( resource that , int count ) { assert that != null ; for ( resource r = that ; r != null ; r = r . parent ) if ( this . equals ( r ) && r . num concurrent write < count ) return bool ; for ( resource r = this ; r != null ; r = r . parent ) if ( that . equals ( r ) && r . num concurrent write < count ) return bool ; return bool ; }	Checks the resource collision.
private void copy ( stapler request req , stapler response rsp , file dir , url src , string name ) throws servlet exception , io { try { file utils . copy url ( src , new file ( dir , name ) ) ; } catch ( io e ) { logger . log ( level . severe , str + name , e ) ; send error ( str + name + str + e . get message ( ) , req , rsp ) ; throw new abort exception ( ) ; } }	Copies a single resource into the target folder, by the given name, and handle errors gracefully.
protected final void send error ( exception e , stapler request req , stapler response rsp ) throws servlet exception , io { send error ( e . get message ( ) , req , rsp ) ; }	Displays the error in a page.
static int run elevated ( file jenkins exe , string command , task listener out , file pwd ) throws io , interrupted exception { try { return new local launcher ( out ) . launch ( ) . cmds ( jenkins exe , command ) . stdout ( out ) . pwd ( pwd ) . join ( ) ; } catch ( io e ) { if ( e . get message ( ) . contains ( str ) && e . get message ( ) . contains ( str ) ) {	Invokes jenkins.exe with a SCM management command.
private void keep last updated unique ( ) { map < string , single token stats > temp = new hash map < > ( ) ; this . token stats . for each ( candidate -> { single token stats current = temp . get ( candidate . token uuid ) ; if ( current == null ) { temp . put ( candidate . token uuid , candidate ) ; } else { int comparison = single token stats . comp by last use then counter . compare ( current , candidate ) ; if ( comparison < num ) {	In case of duplicate entries, we keep only the last updated element.
@ exported ( visibility = num ) public list < cause > get causes ( ) { list < cause > r = new array list < > ( ) ; for ( map . entry < cause , integer > entry : cause bag . entry set ( ) ) { r . add all ( collections . n copies ( entry . get value ( ) , entry . get key ( ) ) ) ; } return collections . unmodifiable list ( r ) ; }	Lists all causes of this build.Note that the current implementation does not preserve insertion order of duplicates.
public < t extends cause > t find cause ( class < t > type ) { for ( cause c : cause bag . key set ( ) ) if ( type . is instance ( c ) ) return type . cast ( c ) ; return null ; }	Finds the cause of the specific type.
public static @ check for null string get valid timezone ( string timezone ) { string [ ] valid i = time zone . get available i ( ) ; for ( string str : valid i ) { if ( str != null && str . equals ( timezone ) ) { return timezone ; } } return null ; }	Checks if given timezone string is supported by TimeZone and returnsthe same string if valid, null otherwise.
public static iterable < descriptor > list legacy instances ( ) { return new iterable < descriptor > ( ) { public iterator < descriptor > iterator ( ) { return new adapted iterator < extension component < descriptor > , descriptor > ( new flatten iterator < extension component < descriptor > , copy on write array list < extension component < descriptor > > > ( legacy descriptors . values ( ) ) { protected iterator < extension component < descriptor > > expand ( copy on write array list < extension component < descriptor > > v ) { return v . iterator ( ) ; } } ) { protected descriptor adapt ( extension component < descriptor > item ) { return item . get instance ( ) ; } } ; } } ; }	List up all the legacy instances currently in use.
public void set value ( string name , string value ) { byte [ ] bytes = value . get bytes ( standard charsets . utf 16 le ) ; int new length = bytes . length + num ;	Writes a String value.
public void set value ( string name , int value ) { byte [ ] data = new byte [ num ] ; data [ num ] = ( byte ) ( value & num ) ; data [ num ] = ( byte ) ( ( value > > num ) & num ) ; data [ num ] = ( byte ) ( ( value > > num ) & num ) ; data [ num ] = ( byte ) ( ( value > > num ) & num ) ; check ( advapi32 . instance . reg set value ex ( handle , name , num , winnt . reg dword , data , data . length ) ) ; }	Writes a DWORD value.
public boolean value exists ( string name ) { int by reference p type , lpcb data ; byte [ ] lp data = new byte [ num ] ; p type = new int by reference ( ) ; lpcb data = new int by reference ( ) ; outer : while ( bool ) { int r = advapi32 . instance . reg query value ex ( handle , name , null , p type , lp data , lpcb data ) ; switch ( r ) { case winerror . error more data : lp data = new byte [ lpcb data . get value ( ) ] ; continue outer ; case winerror . error file not found : return bool ; case winerror . error success : return bool ; default : throw new jna exception ( r ) ; } } }	Does a specified value exist?.
public collection < string > get sub keys ( ) { winbase . filetime lpft last write time ; tree set < string > sub keys = new tree set < > ( ) ; char [ ] lp name = new char [ num ] ; int by reference lpc name = new int by reference ( num ) ; lpft last write time = new winbase . filetime ( ) ; int dw index = num ; while ( advapi32 . instance . reg enum key ex ( handle , dw index , lp name , lpc name , null , null , null , lpft last write time ) == winerror . error success ) { sub keys . add ( new string ( lp name , num , lpc name . get value ( ) ) ) ; lpc name . set value ( num ) ; dw index ++ ; } return sub keys ; }	Get all sub keys of a key.
public tree map < string , object > get values ( ) { int dw index , result ; char [ ] lp value name ; byte [ ] lp data ; int by reference lpcch value name , lp type , lpcb data ; string name ; tree map < string , object > values = new tree map < > ( string . case insensitive order ) ; lp value name = new char [ num ] ; lpcch value name = new int by reference ( num ) ; lp type = new int by reference ( ) ; lp data = new byte [ num ] ; lpcb data = new int by reference ( ) ; lpcb data . set value ( num ) ; dw index = num ; outer : while ( bool ) { result = advapi32 . instance . reg enum value ( handle , dw index , lp value name , lpcch value name , null , lp type , lp data , lpcb data ) ; switch ( result ) { case winerror . error no more items : return values ; case winerror . error more data : lp data = new byte [ lpcb data . get value ( ) ] ; lpcch value name = new int by reference ( num ) ; continue outer ; case winerror . error success : name = new string ( lp value name , num , lpcch value name . get value ( ) ) ; switch ( lp type . get value ( ) ) { case winnt . reg sz : values . put ( name , convert buffer to string ( lp data ) ) ; break ; case winnt . reg dword : values . put ( name , convert buffer to int ( lp data ) ) ; break ; default : break ;	Get all values under a key.
public void pre checkout ( abstract build < ? , ? > build , launcher launcher , build listener listener ) throws io , interrupted exception { abstract project < ? , ? > project = build . get project ( ) ; if ( project instanceof buildable item with build wrappers ) { buildable item with build wrappers biwbw = ( buildable item with build wrappers ) project ; for ( build wrapper bw : biwbw . get build wrappers list ( ) ) bw . pre checkout ( build , launcher , listener ) ; } }	Performs the pre checkout step.This method is called by the {.
public solution get solution ( string id ) { for ( solution s : solution . all ( ) ) if ( s . id . equals ( id ) ) return s ; return null ; }	Binds a solution to the URL.
public url get index page ( ) {	Returns the URL of the index page jelly script.
@ exported public string get url ( ) {	Gets the URL that shows more information about this plugin.
@ exported public string get long name ( ) { string name = manifest . get main attributes ( ) . get value ( str ) ; if ( name != null ) return name ; return short name ; }	Returns a one-line descriptive name of this plugin.
@ exported public yes no maybe supports dynamic load ( ) { string v = manifest . get main attributes ( ) . get value ( str ) ; if ( v == null ) return yes no maybe . maybe ; return boolean . parse boolean ( v ) ? yes no maybe . yes : yes no maybe . no ; }	Does this plugin supports dynamic loading?.
@ exported public @ check for null string get required core version ( ) { string v = manifest . get main attributes ( ) . get value ( str ) ; if ( v != null ) return v ; v = manifest . get main attributes ( ) . get value ( str ) ; if ( v != null ) return v ; return null ; }	Returns the required Jenkins core version of this plugin.
public void stop ( ) { plugin plugin = get plugin ( ) ; if ( plugin != null ) { try { logger . log ( level . fine , str , short name ) ; plugin . stop ( ) ; } catch ( throwable t ) { logger . log ( warning , str + short name , t ) ; } } else { logger . log ( level . fine , str , short name ) ; }	Terminates the plugin.
public void enable ( ) throws io { if ( ! disable file . exists ( ) ) { logger . log ( level . finest , str , get short name ( ) ) ; return ; } if ( ! disable file . delete ( ) ) throw new io ( str + disable file ) ; }	Enables this plugin next time Jenkins runs.
private void disable without check ( ) throws io {	Disable a plugin wihout checking any dependency.
@ exported public string get backup version ( ) { file backup = get backup file ( ) ; if ( backup . exists ( ) ) { try { try ( jar file backup plugin = new jar file ( backup ) ) { return backup plugin . get manifest ( ) . get main attributes ( ) . get value ( str ) ; } } catch ( io e ) { logger . log ( warning , str + backup , e ) ; return null ; } } else { return null ; } }	returns the version of the backed up plugin,or null if there's no back up.
protected synchronized thread start ( boolean force restart ) { if ( ! force restart && is fixing active ( ) ) { fix thread . interrupt ( ) ; } if ( force restart || ! is fixing active ( ) ) { fix thread = new fix thread ( ) ; fix thread . start ( ) ; } return fix thread ; }	Starts the background fixing activity.
public static void do trackback ( object it , stapler request req , stapler response rsp ) throws io , servlet exception { string url = req . get parameter ( str ) ; rsp . set status ( http servlet response . sc ok ) ; rsp . set content type ( str ) ; try ( print writer pw = rsp . get writer ( ) ) { pw . println ( str ) ; pw . println ( str + ( url != null ? num : num ) + str ) ; if ( url == null ) { pw . println ( str ) ; } pw . println ( str ) ; } }	Parses trackback ping.
public static < e > void forward to rss ( string title , string url , collection < ? extends e > entries , feed adapter < e > adapter , stapler request req , http servlet response rsp ) throws io , servlet exception { req . set attribute ( str , adapter ) ; req . set attribute ( str , title ) ; req . set attribute ( str , url ) ; req . set attribute ( str , entries ) ; req . set attribute ( str , jenkins . get instance ( ) . get root url ( ) ) ; string flavor = req . get parameter ( str ) ; if ( flavor == null ) flavor = str ; flavor = flavor . replace ( str , str ) ;	Sends the RSS feed to the client.
public string get name ( ) { string name = get class ( ) . get name ( ) ; name = name . substring ( name . last index of ( str ) + num ) ;	Gets the command name. For example, if the CLI is invoked as {.
@ restricted ( no external use . class ) public final string get single line summary ( ) { byte array output stream out = new byte array output stream ( ) ; get cmd line parser ( ) . print single line usage ( out ) ; return out . to string ( ) ; }	Get single line summary as a string.
@ restricted ( no external use . class ) public final string get usage ( ) { byte array output stream out = new byte array output stream ( ) ; get cmd line parser ( ) . print usage ( out ) ; return out . to string ( ) ; }	Get usage as a string.
@ restricted ( no external use . class ) public final string get long description ( ) { byte array output stream out = new byte array output stream ( ) ; print stream ps = new print stream ( out ) ; print usage summary ( ps ) ; ps . close ( ) ; return out . to string ( ) ; }	Get long description as a string.
protected final file path preferred location ( tool installation tool , node node ) { if ( node == null ) { throw new illegal argument exception ( str ) ; } string home = util . fix empty and trim ( tool . get home ( ) ) ; if ( home == null ) { home = sanitize ( tool . get descriptor ( ) . get id ( ) ) + file . separator char + sanitize ( tool . get name ( ) ) ; } file path root = node . get root path ( ) ; if ( root == null ) { throw new illegal argument exception ( str + node . get display name ( ) + str ) ; } return root . child ( str ) . child ( home ) ; }	Convenience method to find a location to install a tool.
private string get file name ( string possibly path name ) { possibly path name = possibly path name . substring ( possibly path name . last index of ( str ) + num ) ; possibly path name = possibly path name . substring ( possibly path name . last index of ( str ) + num ) ; return possibly path name ; }	Strip off the path portion if the given path contains it.
@ fb ( value = str , justification = str ) protected object read resolve ( ) { if ( allow empty archive == null ) { this . allow empty archive = system properties . get boolean ( artifact archiver . class . get name ( ) + str ) ; } if ( default excludes == null ) { default excludes = bool ; } if ( case sensitive == null ) { case sensitive = bool ; } return this ; }	Backwards compatibility for older builds.
private void apply forced changes ( ) {	Put here the different changes that are enforced after an update.
public boolean is due ( ) { if ( is up to date ) return bool ;	Do we need to show the upgrade wizard prompt?.
public boolean is show upgrade wizard ( ) { http session session = stapler . get current request ( ) . get session ( bool ) ; if ( session != null ) { return boolean . true . equals ( session . get attribute ( show upgrade wizard flag ) ) ; } return bool ; }	Whether to show the upgrade wizard.
public http response do show upgrade wizard ( ) throws exception { jenkins . get instance ( ) . check permission ( jenkins . administer ) ; http session session = stapler . get current request ( ) . get session ( bool ) ; session . set attribute ( show upgrade wizard flag , bool ) ; return http responses . redirect to context root ( ) ; }	Call this to show the upgrade wizard.
public http response do hide upgrade wizard ( ) { jenkins . get instance ( ) . check permission ( jenkins . administer ) ; http session session = stapler . get current request ( ) . get session ( bool ) ; if ( session != null ) { session . remove attribute ( show upgrade wizard flag ) ; } return http responses . redirect to context root ( ) ; }	Call this to hide the upgrade wizard.
@ post public http response do snooze ( ) throws io { jenkins . get instance ( ) . check permission ( jenkins . administer ) ; file f = setup wizard . get update state file ( ) ; file utils . touch ( f ) ; f . set last modified ( system . current time millis ( ) + time unit . days . to millis ( num ) ) ; logger . log ( fine , str ) ; return http responses . redirect to context root ( ) ; }	Snooze the upgrade wizard notice.
private void delete if empty ( file dir ) { string [ ] r = dir . list ( ) ; if ( r == null ) return ;	Deletes a directory if it's empty.
private boolean check ( file fingerprint file , task listener listener ) { try { fingerprint fp = load fingerprint ( fingerprint file ) ; if ( fp == null || ! fp . is alive ( ) ) { listener . get logger ( ) . println ( str + fingerprint file ) ; fingerprint file . delete ( ) ; return bool ; } else {	Examines the file and returns true if a file was deleted.
private boolean is whitelisted ( role sensitive subject , collection < role > expected ) { for ( callable whitelist w : callable whitelist . all ( ) ) { if ( w . is whitelisted ( subject , expected , context ) ) return bool ; } return bool ; }	Is this subject class name whitelisted?.
public @ check for null t get ( string key ) throws io { return get ( key , bool , null ) ; }	Finds the data object that matches the given key if available, or nullif not found.
public string get performance stats ( ) { int total = total query . get ( ) ; int hit = cache hit . get ( ) ; int weak ref = weak ref lost . get ( ) ; int failure = load failure . get ( ) ; int miss = total - hit - weak ref ; return message format . format ( str , total , hit , weak ref , failure , miss ) ; }	Gets the short summary of performance statistics.
private static boolean has some user ( ) { for ( user u : user . get all ( ) ) if ( u . get property ( details . class ) != null ) return bool ; return bool ; }	Computes if this Hudson has some user accounts configured. This is used to check for the initial.
@ override public http response commence signup ( final federated identity identity ) {	Show the sign up page with the data from the identity.
@ post public user do create account ( stapler request req , stapler response rsp ) throws io , servlet exception { return do create account ( req , rsp , str ) ; }	Creates an user account.
@ suppress warnings ( str ) private void login and take back ( stapler request req , stapler response rsp , user u ) throws servlet exception , io { http session session = req . get session ( bool ) ; if ( session != null ) {	Lets the current user silently login as the given user and report back accordingly.
@ post public void do create account by admin ( stapler request req , stapler response rsp ) throws io , servlet exception { create account by admin ( req , rsp , str , str ) ;	Creates a user account.
@ post public void do create first account ( stapler request req , stapler response rsp ) throws io , servlet exception { if ( has some user ( ) ) { rsp . send error ( sc unauthorized , str ) ; return ; } user u = create account ( req , rsp , bool , str ) ; if ( u != null ) { try to make admin ( u ) ; login and take back ( req , rsp , u ) ; } }	Creates a first admin user account.
private void try to make admin ( user u ) { authorization strategy as = jenkins . get instance ( ) . get authorization strategy ( ) ; for ( permission adder adder : extension list . lookup ( permission adder . class ) ) { if ( adder . add ( as , u , jenkins . administer ) ) { return ; } } }	Try to make this user a super-user.
public user create account ( string user name , string password ) throws io { user user = user . get by id ( user name , bool ) ; user . add property ( details . from plain password ( password ) ) ; security listener . fire user created ( user . get id ( ) ) ; return user ; }	Creates a new user account by registering a password to the user.
public user create account with hashed password ( string user name , string hashed password ) throws io { if ( ! password encoder . is password hashed ( hashed password ) ) { throw new illegal argument exception ( str ) ; } user user = user . get by id ( user name , bool ) ; user . add property ( details . from hashed password ( hashed password ) ) ; security listener . fire user created ( user . get id ( ) ) ; return user ; }	Creates a new 